# Open-Source Model Recommendations for PMOVES by Service & Deployment Context

**Overview:** The PMOVES orchestration mesh is a local-first AI stack spanning powerful GPUs (e.g. RTX¬†3090¬†Ti, future RTX¬†5090) and edge devices (Jetson Orin Nano Super), with optional cloud augmentations (e.g. Venice.ai or Cloudflare Workers AI). Below we match each PMOVES service to suitable open-source models, considering model format (FP16, INT4/8, GGUF quantization) and best inference backend (Transformers/vLLM for high-end GPUs, llama.cpp for quantized CPUs, NVIDIA TensorRT for CV on Jetson, Unsloth for fine-tuning and extended context). Key model families ‚Äì **Qwen 1.5/2.0**, **Mistral**, **Phi-3**, etc. ‚Äì are highlighted, along with specialized models for retrieval-augmented generation (RAG), reranking, multi-modal vision/audio, and summarization. A summary compatibility table is provided at the end for quick reference.

## Agent Zero ‚Äì General Orchestration LLM (Assistant & Coding Brain)

Agent Zero is the ‚Äúcentral brain‚Äù LLM driving reasoning, planning, and tool use[\[1\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L5-L14). It needs a capable chat model for broad general tasks (coding, multi-turn dialogue, reasoning) with support for tools and long context memory. We recommend using a **larger open-source chat model** locally on high-end GPUs, with quantized or smaller alternatives for edge deployment, and possibly cloud-hosted models for heavy tasks:

* **Qwen-14B-Chat (Qwen1.5)** ‚Äì *Alibaba, 14B params.* A top-performing general model; Qwen1.5 chat models showed strong results across language, coding, and reasoning tasks[\[2\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Model%20MMLU%20C,1)[\[3\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=At%20every%20model%20size%2C%20Qwen1,language%20understanding%2C%20reasoning%2C%20and%20math). All Qwen1.5 models support up to **32K tokens** context[\[4\]](https://huggingface.co/Qwen/Qwen1.5-1.8B#:~:text=,trust_remote_code) ‚Äì ideal for Agent Zero‚Äôs long conversations and retrieval augmentations. Use the FP16 model on a 24+ GB GPU (fits in \~28GB VRAM), or leverage 4-bit quantized versions (Int4 GPTQ or AWQ)[\[5\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=With%20Qwen1.5%2C%20we%20are%20open,4.37.0%60%20without%20needing%20%60trust_remote_code) to run on 3090¬†Ti (24¬†GB) with minimal quality loss. Qwen‚Äôs architecture (Transformer with SwiGLU, grouped-query attention, etc.[\[6\]](https://huggingface.co/Qwen/Qwen1.5-1.8B#:~:text=Qwen1,of%20SWA%20and%20full%20attention)) yields strong quality; e.g. Qwen1.5-14B outperforms Llama2-13B on many benchmarks[\[7\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Mistral,5). **Recommended backend:** HuggingFace Transformers with transformers\>=4.37 (native support)[\[8\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=models,4.37.0%60%20without%20needing%20%60trust_remote_code) or vLLM for high throughput. **Hyperparams:** Temperature \~0.7 for balanced creativity, max tokens 1024 (can go higher given 32k window, but keep generation shorter per turn to maintain speed).

* **Mistral-7B-instruct** ‚Äì *Mistral AI, 7B params.* A smaller but highly efficient chat model. Mistral‚Äôs 7B architecture uses advanced attention schemes and was trained on a large corpus, achieving impressive performance for its size (e.g. \~64 MMLU, rivaling older 13B models)[\[9\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Llama2,5). It supports 8K context out-of-the-box and community variants extend up to 16‚Äì32K. Mistral-7B runs in FP16 on \~16¬†GB VRAM, or as a 4-bit GGUF (\~4¬†GB) for CPU/Jetson via llama.cpp. This is suitable for Agent Zero on an edge GPU or as a fast secondary agent. **Backend:** llama.cpp (GGUF) for Jetson/CPU, or Transformers on GPU. **Hyperparams:** Temperature 0.7‚Äì0.9 (Mistral is well-tuned, can handle a bit more randomness for creativity).

* **Phi-3 Medium (14B)** ‚Äì *Microsoft, 14B params.* Phi-3 is a new family of *small language models (SLMs)* that punch above their weight. Phi-3 models ‚Äúoutperform models of the same size and next size up‚Äù[\[10\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=The%20company%20announced%20today%20the,innovations%20developed%20by%20Microsoft%20researchers) ‚Äì for example, the 3.8B Phi-3-mini outperforms some 7B models[\[11\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20is%20now%20making%20the,its%20size%2C%20the%20company%20said). The 14B ‚Äúmedium‚Äù model (expected late 2025\) aims to rival 20B+ model quality[\[12\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20also%20announced%20additional%20models,and%20other%20model%20gardens%20shortly). Once available on Hugging Face and NVIDIA‚Äôs NIM service[\[13\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=billion%20parameters%2C%20which%20performs%20better,its%20size%2C%20the%20company%20said), Phi-3 Medium will be a strong candidate for Agent Zero‚Äôs brain on a 48¬†GB GPU (RTX¬†5090) or via cloud. Its training emphasizes efficiency (it learned from a 4-year-old‚Äôs vocabulary, focusing on core concepts[\[14\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Last%20year%2C%20after%20spending%20his,how%20to%20connect%20these%20words%3F%E2%80%9D)), leading to **fast inference** and low memory use. **Backend:** Transformers or NIM runtime. **Hyperparams:** likely temperature \~0.6 (Phi models are optimized for correctness), max tokens \~1024.

* **Cloud Endpoint (GPT-OSS-20B)** ‚Äì *OpenAI/Cloudflare Workers AI.* For cloud-first deployments (e.g. on Cloudflare‚Äôs Workers AI free tier), one option is OpenAI‚Äôs new 20B open model (‚Äúgpt-oss-20b‚Äù)[\[15\]](https://developers.cloudflare.com/workers-ai/models/#:~:text=OpenAI%27s%20open,latency%2C%20and%20local%20or). This model is designed for ‚Äúpowerful reasoning \[and\] agentic tasks‚Äù with low latency on Workers AI[\[16\]](https://developers.cloudflare.com/workers-ai/models/#:~:text=OpenAI%27s%20open,latency%2C%20and%20local%20or). It can serve as Agent Zero‚Äôs brain when local GPUs are unavailable. **Backend:** Cloudflare Workers AI or Venice.ai (which hosts Llama2-70B and other large models[\[17\]](https://mastra.ai/models/providers/venice#:~:text=Access%2013%20Venice%20AI%20models,Learn%20more%20in)). **Hyperparams:** fixed by provider (OpenAI likely sets an optimal temperature \~0.7 by default). *Note:* This cloud model sacrifices some privacy, so use it only if the ‚Äúfully local‚Äù requirement can be relaxed[\[18\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE).

**Hardware Notes:** On an RTX¬†3090¬†Ti (24¬†GB), up to \~13‚Äì14B models run in FP16 (or \~30B in 8-bit). On a future RTX¬†5090 (likely 48¬†GB+), 30B‚Äì70B models (e.g. Qwen-72B, Llama2-70B) become viable[\[19\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Qwen1.5,5). For Jetson Orin, even 7B may be too slow in full precision ‚Äì prefer 4-bit quantized 7B or smaller (Phi-3 3.8B, Qwen-1.8B) offloaded to the Jetson‚Äôs GPU. Jetson Orin Nano Super (67¬†TOPS) can run these smaller GGUF models at usable speeds[\[20\]](https://docs.ultralytics.com/guides/nvidia-jetson/#:~:text=We%20have%20updated%20this%20guide,the%20most%20popular%20AI%20models), but heavy orchestration is best delegated to more powerful nodes or streamlined using Microsoft‚Äôs ONNX Runtime or NVIDIA TensorRT acceleration if available.

### Runtime layout & MCP forms

Agent Zero‚Äôs FastAPI worker (`pmoves/services/agent-zero`)[\[73\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/main/pmoves/services/agent-zero/README.md) bridges the NATS bus, Model Context Protocol (MCP) helpers, and geometry endpoints. Personas and tool palettes are declared in `pmoves/configs/agents/forms/*.yaml`[\[74\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/main/pmoves/configs/agents/forms/POWERFULMOVES.yaml); pick models that align with these layers:

* **Core orchestration brain:** Keep a high-capacity chat model (Qwen1.5/2 14B, Phi-3 Medium, or cloud DeepSeek-R1 distills) on the primary GPU for planning and long-horizon conversations.
* **MCP command expanders:** When Agent Zero renders structured outputs (e.g., `geometry.publish_cgp`, `form.switch`, `media.transcribe`), favour schema-faithful, lower-latency models such as **Qwen2.5-Coder-7B-Instruct** or **Phi-3-mini (3.8B)** running via vLLM/Transformers. They emit crisp JSON/YAML while the larger brain stays free for dialogue. Quantized 4-bit builds cover Jetson deployments.
* **Edge fallbacks & autonomy loops:** The runtime mirrors artefacts into `runtime/knowledge` and `runtime/memory`, so shipping a small local GGUF (Phi-3-mini, Qwen1.5-4B) on Jetson lets Agent Zero continue issuing MCP actions even if the main GPU falls offline. These models pair well with the `geometry.jump`/`ingest.youtube` helpers where concise tool routing matters more than eloquent prose.

## Archon ‚Äì Knowledge Management & RAG ‚ÄúMuscle‚Äù

Archon is the backend service for retrieval-augmented generation: it crawls documents, generates embeddings, and performs hybrid search with reranking[\[21\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L16-L24)[\[22\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L18-L21). Archon‚Äôs model needs are specialized: **text embedding models** for vector search and **reranker models** for cross-modal relevance, plus possibly a smaller LLM for knowledge-base curation. All models here should be efficient for high throughput (many documents/queries) and preferably run locally on GPUs (or via local CPU if needed):

* **Embedding Model ‚Äì BGE or E5**: For generating document and query embeddings, state-of-the-art open models like **BAAI‚Äôs BGE (Base General Embeddings)** or Intel‚Äôs **E5** series are ideal. In particular, *BGE-M3-large* (based on a 3B ‚Äúgemma-2‚Äù model) offers multilingual, multi-granularity embeddings with up to 8192 token support[\[23\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=image,the%20context%20length%20of%20LLM)[\[24\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=embeddings), achieving new SOTA on retrieval benchmarks[\[25\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=retrieval%29,Technical%20Report%20and%20Code). Alternatively, **E5-large-v2** (980M params) from HuggingFace is a robust choice for English semantic search. These models are smaller than LLMs and can run on CPU or modest GPU: e.g. BGE-large (1.5B) in FP16 needs \~4‚Äì6¬†GB VRAM. **Format:** FP16 or INT8 (some have ONNX versions). **Backend:** Transformers, or use the **Supabase PGVector** extension to store embeddings and perform similarity search in-database (embedding generation still done by model). Archon will use these to populate Supabase‚Äôs vector store[\[26\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L36-L44).

* **Reranker Model ‚Äì Qwen or Gemma (Cross-Encoder)**: After embedding search, Archon employs a reranker to sort top-K results for relevance[\[22\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L18-L21). A leading option is a **Qwen-based cross-encoder**. Alibaba has released a Qwen2 or Qwen3 reranker (e.g. ‚ÄúQwen2-Reranker-1.5B‚Äù) built on Qwen-1.5B that, combined with FlagOpen‚Äôs training, excels at ranking[\[27\]](https://dataloop.ai/library/model/neofung_ldir-qwen2-reranker-15b/#:~:text=LdIR%20Qwen2%20Reranker%201,the%20FlagEmbedding%20reranker%20and)[\[28\]](https://huggingface.co/neofung/LdIR-Qwen2-reranker-1.5B/discussions/4#:~:text=neofung%2FLdIR,com%2FFlagOpen%2FFlagEmbedding%2F). Archon‚Äôs logs indicate using a \~4B reranker model[\[29\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/AGENTS.md#L2-L5) ‚Äì likely a Qwen1.5-4B or Gemma-9B cross-encoder fine-tuned for search. For efficiency, consider **BGE v2.5 Gemma2-light Reranker** (a distilled 9B cross-encoder) which ‚Äúensures good performance while saving resources‚Äù[\[30\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=which%20supports%20multiple%20languages%20and,pl). These rerankers run on GPU (8‚Äì16¬†GB for 4B model, more for 9B) and often require batch-size¬†1 (one query-doc pair at a time)[\[29\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/AGENTS.md#L2-L5). **Backend:** Transformers with FP16 (for speed) on a GPU; if capacity is limited, use only embedding model and rely on hybrid search (BM25 \+ embedding) instead of heavy neural reranking. **Hyperparams:** not applicable (no generation, but ensure sequence length covers query+doc, \~512 tokens).

* **Knowledge Distillation LLM (optional)**: Archon can also benefit from a smaller LLM to summarize or refine knowledge base entries. For example, after crawling a large document, a **summarization model** like *FLAN-T5-XXL (11B)* or *LongT5* can create concise summaries for storage. FLAN-T5 (Google‚Äôs instruction-tuned T5) is open-source and excels at summarization tasks. It requires \~16¬†GB VRAM (11B FP16) or can be run in 8-bit on 3090¬†Ti. Another light option is **Phi-3 Mini (3.8B)** ‚Äì it‚Äôs powerful enough to summarize or classify text quickly and can even run on Jetson (3.8B in 4-bit fits \~4¬†GB). Phi-3-mini ‚Äúperforms better than models twice its size‚Äù[\[31\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=outperform%20models%20of%20the%20same,innovations%20developed%20by%20Microsoft%20researchers) and is designed for fast local inference. Archon can invoke such a model to generate metadata (titles, keywords) for each document. **Hyperparams:** Use low temperature (\~0.2‚Äì0.3) for deterministic summaries or info extraction.

**Hardware:** Archon‚Äôs embedding and rerank models typically run on the primary GPU server for speed. The Jetson Orin can assist with lightweight tasks ‚Äì e.g. generating embeddings for sensor data or small text ‚Äì but heavy text indexing is best on GPUs with ample memory. If needed, multiple smaller Jetsons could distribute embedding tasks (since each handles \~10‚Äì12 FPS on YOLO-sized models[\[32\]](https://forums.developer.nvidia.com/t/anyway-to-boost-yolo-performance-on-jetson-orin/313795#:~:text=Anyway%20to%20boost%20yolo%20performance,Check%20jtop%2Ftegrastats) for vision, similarly a few docs/sec for embeddings). Cloud is less useful here (embedding every document via API is costly and violates local-first privacy). Instead, ensure the local Supabase (PGVector/Qdrant) is populated with embeddings from these open models[\[26\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L36-L44) for fast semantic search.

### Archon UI & console surfaces

The Archon UI bundle (`integrations-workspace/PMOVES-Archon`)[\[79\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/main/integrations-workspace/PMOVES-Archon/README.md) layers a Vite front end on the Archon API/MCP bridge. Configuration toggles such as `ARCHON_UI_API_URL`/`NEXT_PUBLIC_ARCHON_UI_URL` in `pmoves/env.shared` point the UI back at the headless service so console operators, Agent Zero, and MCP clients share the same indexes. Recommended model roles:

* **Task + Kanban summarisation:** Phi-3-mini 3.8B or Qwen2.5-7B produce short status blurbs and acceptance criteria for the Projects view without waiting on the 14B+ primary brain. Keep temperature low (‚â§0.3) for deterministic updates.
* **Knowledge previews & highlight strips:** Qwen2.5-Omni-7B (for mixed media) or BLIP2 captioners render the thumbnail snippets that appear when a Supabase document is selected. These slots feed from the same embeddings used by Archon‚Äôs backend reranker, so latency matters more than exhaustive reasoning.
* **Inline semantic filters:** A small embedding head (e.g., BGE-small-en or E5-small) can run in-browser via WebGPU or on a nearby CPU service to power instant search-as-you-type filters while the heavier GPU pipeline refreshes the main results.

## DeepResearch & SupaSerch ‚Äì Autonomous Research Stack

The DeepResearch worker (`services/deepresearch`)[\[75\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/main/pmoves/docs/services/deepresearch/README.md) consumes `research.deepresearch.request.v1` envelopes and can execute against OpenRouter or a locally hosted Alibaba DeepResearch stack (`DEEPRESEARCH_MODE=local`). SupaSerch (`services/supaserch`)[\[76\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/main/pmoves/docs/services/supaserch/README.md) is the long-running aggregator that will fan out research jobs across DeepResearch, Archon, and MCP tools. Recommended model tiers:

* **Planner / decomposition brain:** Deploy a reasoning-focused model such as **DeepSeek-R1-Distill-Qwen-32B**[\[80\]](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) or **Qwen2.5-32B-Instruct**[\[81\]](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) behind vLLM. Both provide long context windows (32K) and reliable function calling for tool plans. When GPUs are scarce, fall back to 14B distills (DeepSeek-R1-Distill-Qwen-14B, Qwen2.5-14B) or proxy to the hosted DeepSeek API using `DEEPSEEK_API_KEY` from `env.shared`.
* **Executor / note taker:** Keep a lightweight assistant (Phi-3-mini 3.8B, Qwen2.5-7B) close to SupaSerch for quick summarisation of intermediate hops, email drafts, or Notebook mirror payloads. These run comfortably on Jetson when quantised and mirror DeepResearch outputs into Supabase / Notebook metadata.
* **Retrieval companions:** Reuse Archon‚Äôs embedding + reranker combo (BGE-large for vectors, Qwen/Gemma cross-encoder) so DeepResearch and SupaSerch share the same semantic filters. SupaSerch‚Äôs `/v1/search` endpoint can hydrate results immediately from Supabase/Qdrant while the planner loop orchestrates slower external calls.

Operational tips: surface the chosen model slugs in `DEEPRESEARCH_OPENROUTER_MODEL` and `SUPASERCH` configuration for traceability, pipe TensorZero metrics to confirm autonomous loops stay under latency budgets, and keep the Notebook mirror (`OPEN_NOTEBOOK_API_URL/TOKEN`) configured so research artifacts land beside Archon knowledge cards.

## Jellyfin Enhanced Media Ingestion & Analysis

The media ingestion trio (`media-audio`, `media-video`, `ffmpeg-whisper`) feeds the Jellyfin bridge with rich transcripts and vision tags[\[77\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/main/pmoves/services/media-audio/README.md). The ‚ÄúEnhanced Media Stack‚Äù guide[\[78\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/main/pmoves/docs/PMOVES.AI%20PLANS/Enhanced%20Media%20Stack%20with%20Advanced%20AudioVideo%20Analysis/ai-enhanced-media-stack-guide.md) details the multi-model lineup:

* **Transcription & alignment:** Run **Whisper Large v3** (or the faster Turbo variant) for base transcripts[\[84\]](https://huggingface.co/openai/whisper-large-v3), optionally delegating to **WhisperX** for word-aligned timestamps and batched diarisation[\[85\]](https://github.com/m-bain/whisperX). The `ffmpeg-whisper` sidecar can downsample, shard, and forward jobs back to `media-audio`.
* **Speaker + emotion labelling:** Pair **pyannote.audio speaker-diarization 3.1** for diarisation[\[86\]](https://huggingface.co/pyannote/speaker-diarization-3.1) with a SeaLLM audio model such as **SeaVoice-7B**[\[82\]](https://huggingface.co/SeaLLMs/SeaVoice-7B) to classify mood, genre, or conversational tone. These results are persisted to Supabase tables and broadcast on `analysis.audio.v1`.
* **Vision tagging & captioning:** For sampled frames, use **YOLOv8/v11** detectors for objects[\[87\]](https://github.com/ultralytics/ultralytics) alongside **BLIP2 FLAN-T5** captioners[\[83\]](https://huggingface.co/Salesforce/blip2-flan-t5-base) or Qwen2.5-Omni for cross-modal reasoning. Outputs (captions, moods, temporal segments) feed LangExtract + Archon RAG so Agent Zero can reference scenes, thumbnails, and moods when drafting summaries.

Most audio jobs run well on a single GPU with CUDA (or on Jetson via INT4 quantisation); video tagging benefits from TensorRT-optimised YOLO and batching BLIP2 requests through vLLM. Keep MinIO/S3 credentials aligned, and schedule LangExtract or Archon ingest after each media analysis event so Jellyfin assets immediately appear in Supabase-driven searches.

## Hi-RAG ‚Äì Hierarchical RAG & Multi-hop Reasoning

Hi-RAG (Hierarchical Retrieval-Augmented Generation) builds multi-layer indexes and facilitates deeper, multi-hop Q\&A[\[33\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L29-L37)[\[34\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L31-L39). Its model needs revolve around handling **long contexts** and performing **iterative reasoning** over retrieved data. Key model considerations:

* **Extended-Context LLM**: To effectively utilize Hi-RAG‚Äôs hierarchical retrieval (local entities, global bridges, etc.), an LLM that can handle *very large prompt contexts* (tens of thousands of tokens) is invaluable. **Qwen1.5/Qwen2** models are ideal here since even the 7B and 14B versions reliably support 32K token inputs[\[4\]](https://huggingface.co/Qwen/Qwen1.5-1.8B#:~:text=,trust_remote_code). This means Hi-RAG can feed multiple layers of retrieved text chunks into one prompt for a comprehensive answer. For example, *Qwen-7B-Chat (Qwen1.5)* quantized to 8-bit could be deployed as Hi-RAG‚Äôs answer generator on a Jetson Orin or secondary GPU, taking in e.g. \~20K tokens of structured notes. If more context is needed, consider community-extended LLaMA variants (some Llama2-based models support 100K+ via rope scaling) or emerging models like **Llama-3** (flagged by projects like Unsloth) that achieve 80K context via fine-tuning[\[35\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=%2A%204%2F30%2F2024%3A%20Release%20Llama,BGE%2C%20equipping%20BGE%20with%20visual). **Backend:** vLLM is recommended for long-context generation, as it optimizes KV cache handling and streaming on GPU.

* **Multi-hop QA / Reasoning**: For questions requiring sequential reasoning across documents, a *smaller chain-of-thought model* can be employed. One approach is to use **DeepSeek** models ‚Äì DeepSeek¬†V3.1 (open-source) is a ‚Äúhybrid reasoning‚Äù model aimed at tool use and multi-step tasks[\[36\]](https://simonwillison.net/2025/Aug/22/deepseek-31/#:~:text=DeepSeek%203.1%20,it%27s%20a%20hybrid%20reasoning%20model). While DeepSeek-3.1 is extremely large (reportedly \~685B)[\[37\]](https://simonwillison.net/2025/Aug/22/deepseek-31/#:~:text=DeepSeek%203.1%20,it%27s%20a%20hybrid%20reasoning%20model) and not feasible to run locally, the techniques can be distilled. Instead, a medium-sized instruct model (e.g. Mistral-7B or Phi-3-7B when released[\[12\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20also%20announced%20additional%20models,and%20other%20model%20gardens%20shortly)) with a reasoning-oriented prompt can emulate multi-hop reasoning. Hi-RAG can orchestrate this by first retrieving intermediate contexts, then asking the LLM to draw connections. Using *Unsloth‚Äôs inference optimizations* here can help ‚Äì Unsloth‚Äôs framework can yield \~2√ó faster inference for LLMs without accuracy loss[\[38\]](https://docs.unsloth.ai/basics/running-and-saving-models/unsloth-inference#:~:text=Learn%20how%20to%20run%20your,model%20with%20Unsloth%27s%20faster%20inference)[\[39\]](https://docs.unsloth.ai/basics/running-and-saving-models/unsloth-inference#:~:text=model_name%20%3D%20%22lora_model%22%2C%20,text_streamer%2C%20max_new_tokens%20%3D%2064), benefiting long multi-hop runs.

* **Result Re-ranking & Filtering**: Hi-RAG integrates graph-based and semantic re-rankers[\[40\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L166-L175)[\[41\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L196-L203). It can reuse Archon‚Äôs reranker model (e.g. the Qwen/Gemma cross-encoder) to prioritize which retrieved nodes to feed into the final answer prompt. Additionally, Hi-RAG‚Äôs Neo4j graph context means an **embedding-to-graph link model** (to predict if two text chunks are related) could be useful. If needed, a model like **OpenAI‚Äôs LLM-Embedder** (released by BAAI) unifies dense and graph retrieval[\[42\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=,script%20to%20mine%20hard%20negatives), though simpler heuristic blending (GraphBoost, already in use[\[43\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L164-L172)) often suffices without an extra model.

**Hardware:** Running long-context models demands significant memory. On a 24¬†GB GPU, a 7B model can handle \~32K tokens with optimized kernels; a 14B model might handle \~16K. The RTX¬†5090 class (with more VRAM and faster memory) is better for Hi-RAG‚Äôs largest context needs, possibly enabling 70B models with 32K context (e.g. Llama2-70B 32K). On Jetson, long contexts are impractical ‚Äì offload heavy reasoning to the main GPU servers. Hi-RAG can still *operate* on Jetson in a limited fashion (e.g. retrieving a few facts from a small local index), but any complex query should hit the bigger models via the mesh network. Cloud options (like AI21‚Äôs 100K context Jurassic, or Anthropic‚Äôs 100K Claude if open-access) are proprietary, so PMOVES sticks to open local models for Hi-RAG to maintain privacy[\[18\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE).

## LangExtract ‚Äì Structured Information Extraction

LangExtract converts raw texts (docs, transcripts) into structured, grounded data[\[44\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L26-L34). It uses LLMs to extract entities, facts, and relationships with source citations. The ideal model here should be **precise, deterministic, and lightweight**, since it may run on batches of documents or streaming text:

* **Gemini (7B) or Qwen-7B-Chat** ‚Äì LangExtract in the Jellyfin media pipeline has used a model referred to as ‚ÄúGemini‚Äù[\[45\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L28-L34). In lieu of Google‚Äôs proprietary Gemini, we suggest using a 6‚Äì7B open model fine-tuned for extraction. **Qwen-7B-Chat** is a strong candidate: it can be prompted with a custom instruction to output JSON or YAML with extracted entities (thanks to Qwen‚Äôs alignment improvements[\[46\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=This%20release%20brings%20substantial%20improvements,creating%20a%20truly%20%E2%80%9Cgood%E2%80%9D%20model)). It‚Äôs small enough to run on Jetson Orin (especially in 4-bit mode) and has multilingual ability for diverse content. Another choice is **MiniGPT-4 7B** or **LLaMA2 7B fine-tuned** on structured outputs, but Qwen‚Äôs overall performance is higher in 2025\. **Usage:** Prompt the model with a few examples of the desired schema and use a low temperature (‚âà0 to 0.2) to enforce consistency. Qwen‚Äôs 32K context lets LangExtract handle long transcripts in chunks with overlap for grounding.

* **Phi-2.5B / Phi-3-mini (3.8B)** ‚Äì For faster parallel processing, a smaller model like Microsoft‚Äôs Phi series can be used. *Phi-3-mini (3.8B)* is open, optimized for instruction following, and ‚Äúperforms better than models twice its size‚Äù[\[31\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=outperform%20models%20of%20the%20same,innovations%20developed%20by%20Microsoft%20researchers) ‚Äì which includes many 7B models ‚Äì making it an excellent trade-off between speed and accuracy. It can extract straightforward facts (dates, names, etc.) with high precision. Running multiple instances of a 3.8B model on a single GPU (or one per Jetson device) allows concurrent extraction jobs. **Format:** Use INT8 or FP16 on GPU, or quantize to GGUF for CPU execution if needed. *Recommended:* fine-tune or instruct it on the specific schemas (e.g. ‚ÄúTechnical Talk Transcript‚Äù schema as per PMOVES PRD[\[47\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE)) for best results.

* **Tool-augmented Extraction:** In cases where absolute accuracy is needed (e.g. no hallucination tolerance), LangExtract can incorporate non-LLM tools. For example, use **regular expressions or spaCy** for preliminary entity tagging, then have the LLM validate and fill gaps. This reduces the burden on the model. If needed, **Fabric** (the open-source text augmentation framework) can be invoked ‚Äì it has patterns like extract\_wisdom and analyze\_claims which essentially guide an LLM to extract key points or claims from text[\[48\]\[49\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE). Under the hood, Fabric will call whichever LLM is configured (likely Agent Zero‚Äôs main model or a smaller one) and produce structured output. This keeps LangExtract‚Äôs outputs consistent and grounded (Fabric patterns emphasize citing source positions[\[50\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE)).

**Hardware:** LangExtract is often run on background jobs, so it can be assigned to less powerful hardware if needed. For instance, Jetson Orin can handle a 3‚Äì7B model extracting from a single document (with quantization) ‚Äì since extraction can be done sequentially, even \~1 token/second generation is acceptable for a few pages of text. The Jetson Orin Nano Super‚Äôs 6-core CPU and 1024-core GPU can support models like Qwen2-Audio-7B in 4-bit for transcript tasks[\[51\]\[52\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi). Indeed, **Qwen2-Audio-7B (Instruct)** is used for generating summaries and tags from audio transcripts in PMOVES[\[53\]\[52\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi), showing that a 7B LLM in 4-bit can run on the Jetson (likely at \~1 token/sec). For higher throughput (e.g. batch processing many PDFs), offload to a PC GPU where dozens of extraction jobs can run in parallel across multiple model instances or use model concurrency features of vLLM.

## Publisher & Multi-Modal Services ‚Äì Vision, Audio, TTS, and Output Generation

The Publisher service in PMOVES handles final content assembly and distribution (e.g. posting answers to Discord, with embeds)[\[54\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L236-L244)[\[55\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L220-L228). It also enriches content with visual and audio analysis metadata[\[56\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi). The models here are diverse, focusing on computer vision and audio, often running on specialized hardware (Jetson for real-time video, etc.):

* **YOLOv8 (Ultralytics)** ‚Äì *Object detection CV model.* For analyzing camera feeds or video frames (e.g. identifying objects in Jellyfin media), YOLO models are a great fit. The latest YOLOv8 offers state-of-the-art accuracy with speed, and it‚Äôs optimized for NVIDIA TensorRT on Jetson devices. The Jetson Orin Nano Super (67¬†TOPS) can ‚Äúseamlessly run the most popular AI models‚Äù like YOLO in real time[\[20\]](https://docs.ultralytics.com/guides/nvidia-jetson/#:~:text=We%20have%20updated%20this%20guide,the%20most%20popular%20AI%20models), achieving \~25‚Äì35 FPS for small/medium YOLO models at 640p with TensorRT acceleration[\[57\]](https://forums.developer.nvidia.com/t/expected-fps-for-yolox-s-tensorrt-engine-on-jetson-orin-nano-jetpack-6-2/329875#:~:text=Expected%20FPS%20for%20YOLOX,No%20official%20benchmarks). Deploy YOLOv8s (small) or YOLOv8n (nano) models in INT8 or FP16 TensorRT format on the Orin for best performance. These models are tiny (‚â§50¬†MB) and load fast. **Use cases:** Publisher can use YOLO to tag images or video thumbnails with detected objects (e.g. ‚Äúcar‚Äù, ‚Äúperson‚Äù) and store that in Supabase metadata[\[58\]\[59\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi). **Alternate:** If license permits, the open-source YOLO-NAS models (by Deci) have even higher mAP at similar speeds; they can also be converted to TensorRT.

* **OpenAI CLIP (ViT-B/32)** ‚Äì *Image/text similarity model.* CLIP embeddings are integrated into Hi-RAG Gateway v2 for multimodal reranking[\[53\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi). We recommend using the open CLIP ViT-B/32 model (or a variant like LAION‚Äôs CLIP) to embed images and find related text or vice versa. It‚Äôs lightweight (‚âà87M params) and can run on CPU or GPU. Publisher can use CLIP to automatically pick a relevant thumbnail image for a summary (by comparing image embeddings with the text of the summary) ‚Äì indeed the integration plan is to attach ‚ÄúCLIP best match‚Äù image metadata to published content[\[60\]\[61\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi). CLIP ViT-B/32 in float16 runs in \<2¬†GB VRAM and processes \~10 images/sec on a GPU. **Backend:** HuggingFace Transformers or ONNXRuntime on Jetson (accelerated via NVIDIA‚Äôs DeepStream pipelines if needed). No tunable params (non-generative).

* **Qwen2-Audio-7B** ‚Äì *Audio understanding LLM.* This is a specialized multimodal model from Alibaba that accepts audio input. Qwen2-Audio can generate **transcriptions, summaries, and tags** for audio streams[\[53\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi). In PMOVES, it‚Äôs used to enrich Jellyfin media: the audio processor calls an *Ollama-hosted Qwen2-Audio* model to get a summary of a music track and its mood/genre tags[\[58\]\[62\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi). Qwen2-Audio-7B (instruction-tuned for voice) can run quantized on local hardware (Ollama and LocalAI communities have achieved running it on consumer GPUs and even CPU). Use Qwen2-Audio Q4 quant for Jetson (the plan defaults to ‚ÄúQwen2-Audio-7B Q4‚Äù for Jetson deployment[\[63\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi)). It outputs text (so normal decoding params apply: temperature \~0.2 for stable summary, since we want consistent tags). **Note:** Qwen2.5-Omni-7B, an even newer model, is a multimodal LLM that handles **text, image, audio, video** in one[\[64\]](https://huggingface.co/Qwen/Qwen2.5-Omni-7B#:~:text=Qwen%2FQwen2.5,text%2C%20images%2C%20audio%2C%20and%20video)[\[65\]](https://levelup.gitconnected.com/qwen2-5-omni-7b-a-multimodal-ai-that-thinks-like-a-human-across-text-image-audio-and-video-a-8e4b37b4c1c4#:~:text=Qwen2.5,text%2C%20images%2C%20audio%2C%20and%20video). If available, Qwen-Omni could replace separate CLIP and Qwen-audio, by providing an all-in-one captioning and analysis model (e.g. describe an image or audio directly). However, running such a model may be heavier; splitting tasks (CLIP for image, Qwen-Audio for audio) is a proven approach.

* **Stable Diffusion / FLUX (Images)** and **WAN 2.2 (Video)** ‚Äì For content generation tasks orchestrated via ComfyUI/n8n, PMOVES uses diffusion models. ‚ÄúFLUX‚Äù refers to a text-to-image model (possibly Stable Diffusion or a fine-tuned variant), and *WAN 2.2* is a community video generation model[\[66\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/README.md#L49-L54). These are not core to Q\&A, but for completeness: **Stable Diffusion 1.5 or SDXL 1.0** (open-source) can run on the 3090¬†Ti (for high-res outputs) or even Jetson (low-res, quantized). Use TensorRT or ONNX acceleration on Jetson for \~2‚Äì3 FPS generation at 256√ó256. **WAN 2.2** (likely the Weeb Anime model for animation) might be used on a GPU with at least 16¬†GB. For integration, these models are invoked via ComfyUI workflows, so they run in containers specialized for them (outside our direct control in Agent/Archon code). **Recommendation:** Ensure the **TensorZero** observability is capturing these generation calls ‚Äì e.g. measure latency and resource usage via TensorZero‚Äôs /metrics on the publisher service[\[55\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L220-L228). This will highlight if a model is too slow and needs a smaller alternative (for instance, using a faster stable diffusion variant like **SD 2.1 small** on Jetson if SDXL is too heavy).

* **VibeVoice TTS (VITS)** ‚Äì To close the loop with rich output, PMOVES includes a TTS pipeline (VibeVoice) for voice responses[\[67\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice_operator_notes.md#L9-L17). The core models here are likely based on **VITS** or **RVC** for voice cloning. Open TTS models like **Coqui-TTS** or **Bark by Suno** could also be integrated: Bark is an open model that generates natural speech (in various voices) from text, albeit slowly (requires GPU, 10‚Äì20 seconds for a short clip). For faster performance, VITS-based models fine-tuned on the user‚Äôs voice provide quick (real-time) TTS on a GPU. These TTS models are small (\<\<1B) and can run on Jetson or any machine with CUDA. The Publisher service can call them to convert Agent Zero‚Äôs text reply into an audio file for Discord or other consumer. **Formats:** use FP16 or ONNX for inference. In VibeVoice, the pipeline already exists (with WebUI and FFmpeg integration)[\[68\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice_operator_notes.md#L10-L18)[\[69\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice_operator_notes.md#L26-L30) ‚Äì just ensure the models (voice clone weights) are loaded and that n8n triggers the conversion at publish time.

## Summary Table ‚Äì Models by Service and Hardware

Below is a high-level matrix of recommended models, their intended use, and compatibility:

| Service / Component | Model &¬†Link | Params & Arch. | Quant / Format | Inference Backend | Use Case | Hardware |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **Agent¬†Zero (Orchestration)** | **Qwen-14B-Chat** (Alibaba HF¬†üè∑Ô∏è)[\[5\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=With%20Qwen1.5%2C%20we%20are%20open,4.37.0%60%20without%20needing%20%60trust_remote_code) | 14B (Qwen1.5, Transformer) | FP16 / GPTQ-4bit / GGUF int4 | HF Transformers / vLLM | General assistant (coding, reasoning) | 3090¬†Ti (24¬†GB)¬†‚úÖ, Jetson¬†‚ö†Ô∏è (quant only) |
|  | **Mistral-7B** (v0.1¬†Instruct) | 7B (Mistral, advanced attn) | FP16 / GGUF int4 | Transformers / llama.cpp | Lightweight chat & tools | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ (4-bit) |
|  | **Qwen2.5-Coder-7B-Instruct** | 7B (Qwen2.5, code-aligned) | FP16 / GPTQ-4bit | vLLM / Transformers | MCP JSON forms, tool routing | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ (4-bit) |
|  | **Phi-3 Mini** (3.8B edge brain)[\[11\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20is%20now%20making%20the,its%20size%2C%20the%20company%20said) | 3.8B (Phi-3-mini) | GGUF int4 / INT8 | llama.cpp / ONNXRuntime | Autonomy loop fallback, Jetson MCP | Jetson¬†‚úÖ (Q4), CPU¬†‚úÖ |
|  | **Phi-3 Medium** (MS¬†Phi-3 14B)[\[10\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=The%20company%20announced%20today%20the,innovations%20developed%20by%20Microsoft%20researchers)[\[12\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20also%20announced%20additional%20models,and%20other%20model%20gardens%20shortly) | 14B (Phi-3, optimizer SLM) | FP16 (open¬†2025) | HF Transformers / Azure NIM | High-quality reasoning in small size | 5090¬†(48¬†GB)¬†‚úÖ, Cloud¬†‚úÖ |
|  | **GPT-OSS-20B** (OpenAI¬†Workers AI) | 20B (OpenAI, GPT-3.5-class) | N/A (hosted) | Cloudflare Workers AI | Cloud fallback, complex tasks | Cloud API (Workers) |
| **Archon (RAG Engine)** | **BGE-Large** (BAAI¬†Embedder)[\[24\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=embeddings) | 1.5B (Gemma2, BGE¬†v1.5) | FP16 / INT8 | HF Transformers / ONNX CPU | Document & query embeddings | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ (CPU) |
|  | **E5-large-v2** (LF¬†HuggingFace) | 770M (Transformer) | FP16 / ONNX | HF Transformers / ONNX | Universal text embedding | Any (CPU ok) |
|  | **Qwen-4B Reranker** (FlagEmbedding finetune)[\[29\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/AGENTS.md#L2-L5) | \~4B (Qwen1.5 cross-encoder) | FP16 | HF Transformers (batch=1) | Rerank top-K retrievals | 3090¬†Ti¬†‚úÖ (8¬†GB+ VRAM) |
|  | **BGE¬†Reranker v2.5** (Gemma2-light)[\[30\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=which%20supports%20multiple%20languages%20and,pl) | 9B‚Üí(distilled \~5B) | FP16 | HF Transformers | Efficient reranking, multilingual | 5090¬†‚úÖ (for full 9B) |
|  | *FLAN-T5-XXL* (Google FLAN) | 11B (T5 encoder-decoder) | FP16 / BF16 | HF Transformers | Summarize documents, refine KB | 3090¬†Ti¬†‚ö†Ô∏è (11B borderline), 5090¬†‚úÖ |
|  | *Phi-3 Mini* (MS, 3.8B)[\[11\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20is%20now%20making%20the,its%20size%2C%20the%20company%20said) | 3.8B (Phi-3-mini) | FP16 / GGUF 4-bit | HF Transformers / llama.cpp | Fast metadata generation, QA | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ (4-bit) |
|  | **Qwen2.5-Omni-7B**[\[64\]](https://huggingface.co/Qwen/Qwen2.5-Omni-7B#:~:text=Qwen%2FQwen2.5,text%2C%20images%2C%20audio%2C%20and%20video) | 7B (Multimodal transformer) | FP16 / INT8 | HF Transformers (trust code) | Knowledge previews, mixed-media cards | 3090¬†Ti¬†‚úÖ, Jetson¬†‚ö†Ô∏è |
|  | **BGE-small-en / E5-small** | 110M‚Äì330M (Transformer) | FP16 / INT8 | ONNX / WebGPU | Inline semantic filters (UI search) | CPU¬†‚úÖ, Browser¬†‚úÖ |
| **DeepResearch / SupaSerch** | **DeepSeek-R1-Distill-Qwen-32B**[\[80\]](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) | 32B (Reasoning distill) | FP16 / GPTQ-4bit | vLLM (tensor parallel) | Planner, autonomous research loops | 2√ó3090¬†Ti¬†‚ö†Ô∏è, 5090¬†‚úÖ, Cloud¬†‚úÖ |
|  | **Qwen2.5-14B-Instruct**[\[81\]](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) | 14B (Long-context instruct) | FP16 / GPTQ-4bit | vLLM / Transformers | Planner fallback, tool calling | 3090¬†Ti¬†‚úÖ, Jetson¬†‚ö†Ô∏è |
|  | **Phi-3 Mini (3.8B)** | 3.8B (SLM) | GGUF int4 / INT8 | llama.cpp / ONNXRuntime | Executor notes, SupaSerch summaries | Jetson¬†‚úÖ, CPU¬†‚úÖ |
|  | **BGE-Large + Qwen Reranker** | 1.5B + 4B | FP16 / INT8 | Transformers | Shared retrieval stack | 3090¬†Ti¬†‚úÖ |
| **Hi‚ÄëRAG (Hier. RAG)** | **Qwen-7B-Chat** (32K ctx)[\[4\]](https://huggingface.co/Qwen/Qwen1.5-1.8B#:~:text=,trust_remote_code) | 7B (Qwen1.5, GQA attn) | FP16 / GPTQ-4bit | vLLM (long ctx) / HF Transf | Final answer generation with context | 3090¬†Ti¬†‚úÖ, Jetson¬†‚ö†Ô∏è (slow) |
|  | **LLaMA2-70B 4K/32K** (Meta, 2023\) | 70B (Transformer) | FP16 / GGUF-8bit (split GPUs) | Transformers / Accelerate | High-accuracy answers (if GPU memory) | 5090¬†‚ö†Ô∏è (needs 48¬†GB+), Cloud¬†‚úÖ |
|  | **Unsloth Llama-3-8B** (80K ctx)[\[35\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=%2A%204%2F30%2F2024%3A%20Release%20Llama,BGE%2C%20equipping%20BGE%20with%20visual) | 8B (QLoRA extended ctx) | FP16 (after fine-tune) | Unsloth / vLLM | Experimental long-document QA | 3090¬†Ti¬†‚úÖ (8K), 5090¬†‚úÖ (80K ctx) |
|  | *DeepSeek¬†v3.1* (hybrid)[\[36\]](https://simonwillison.net/2025/Aug/22/deepseek-31/#:~:text=DeepSeek%203.1%20,it%27s%20a%20hybrid%20reasoning%20model) | *685B (Mixture model)* | *n/a (too large local)* | *External possible (NIM?)* | *Multi-hop reasoning (reference)* | *Cloud or skip (research)* |
| **LangExtract (Extraction)** | **Qwen-7B-Chat** (structured) | 7B (Qwen1.5) | FP16 / INT4 | HF Transformers / llama.cpp | Entity & relation extraction | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ (4-bit) |
|  | **Phi-3 Mini 3.8B**[\[11\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20is%20now%20making%20the,its%20size%2C%20the%20company%20said) | 3.8B (Phi-3) | FP16 / INT4 | HF Transformers / llama.cpp | Fast fact extraction, tagging | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ |
|  | **Qwen2-Audio-7B** (Ollama)[\[70\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) | 7B (Qwen2, audio-enabled) | INT4 (Q4) | Ollama / LocalAI (GGML) | Audio transcript summarization | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ (Q4) |
|  | *Fabric (patterns)*[\[48\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE) | *Uses Agent LLM* | n/a (calls above LLMs) | Subprocess (Python) | Structured output enforcement | Any (uses existing LLM) |
| **Publisher / Multi‚ÄëModal** | **YOLOv8n/v8s** (Ultralytics)[\[20\]](https://docs.ultralytics.com/guides/nvidia-jetson/#:~:text=We%20have%20updated%20this%20guide,the%20most%20popular%20AI%20models) | \~7M‚Äì20M (ConvNet) | TensorRT FP16/INT8 | TensorRT / DeepStream | Object detection (images/video) | Jetson Orin¬†‚úÖ (real-time) |
|  | **OpenAI¬†CLIP ViT-B/32**[\[63\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) | 150M (ViT-B/32) | FP16 / FP32 | HF Transformers / ONNX | Image-text similarity, thumbnail pick | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ |
|  | **Qwen2.5-Omni-7B**[\[64\]](https://huggingface.co/Qwen/Qwen2.5-Omni-7B#:~:text=Qwen%2FQwen2.5,text%2C%20images%2C%20audio%2C%20and%20video) | 7B (Multimodal transformer) | FP16 (multi-input) | HF Transformers (trust\_code) | Unified caption (text/image/audio) | 3090¬†Ti¬†‚úÖ, Jetson¬†‚ö†Ô∏è (slow) |
|  | **Stable Diffusion XL** (StabilityAI) | 2.6B UNet, 1B VAE | FP16 | CompVis Diffusers / TensorRT | High-res image generation (comfyUI) | 3090¬†Ti¬†‚úÖ, Jetson¬†‚ö†Ô∏è (very slow) |
|  | **WAN¬†Animate¬†2.2** (Video) | *\~1B diffusion per frame* | FP16 | ComfyUI (n8n trigger) | Anime video generation (comfyUI) | 3090¬†Ti¬†‚úÖ (needs \>16GB) |
|  | **VITS Voice Clone** (VibeVoice) | \~30M (GAN TTS) | FP16 | VITS realtime engine | Text-to-Speech (personal voice) | 3090¬†Ti¬†‚úÖ, Jetson¬†‚úÖ |
|  | **Bark TTS** (Suno AI) | n/a (Transformer) | FP16 | HF Transformers | Zero-shot multi-voice TTS | 3090¬†Ti¬†‚ö†Ô∏è (slow), Jetson¬†‚ùå |
| **Jellyfin Ingestion (Media)** | **Whisper Large v3 / Turbo**[\[84\]](https://huggingface.co/openai/whisper-large-v3) | 1.55B (Encoder-decoder) | FP16 / INT8 | Transformers / CTranslate2 | Base ASR transcripts | 3090¬†Ti¬†‚úÖ, Jetson¬†‚ö†Ô∏è (Turbo) |
|  | **WhisperX + Pyannote 3.1**[\[85\]](https://github.com/m-bain/whisperX) | 1.5B + 31M | FP16 | PyTorch / ONNX | Alignment, diarisation | 3090¬†Ti¬†‚úÖ |
|  | **SeaVoice-7B**[\[82\]](https://huggingface.co/SeaLLMs/SeaVoice-7B) | 7B (SeaLLM audio) | FP16 / INT8 | Transformers | Emotion, tone tagging | 3090¬†Ti¬†‚úÖ, Jetson¬†‚ö†Ô∏è |
|  | **YOLOv8/v11 TensorRT**[\[87\]](https://github.com/ultralytics/ultralytics) | 7M‚Äì25M (ConvNet) | INT8 / FP16 | TensorRT / DeepStream | Frame object tags | Jetson Orin¬†‚úÖ |
|  | **BLIP2 FLAN-T5**[\[83\]](https://huggingface.co/Salesforce/blip2-flan-t5-base) | 1B (ViT-G + T5) | FP16 | Transformers / vLLM | Captioning, scene summaries | 3090¬†Ti¬†‚úÖ |

**Legend:** ‚úÖ ‚Äì well-supported; ‚ö†Ô∏è ‚Äì partially (with limitations); ‚ùå ‚Äì not recommended. *Param counts* and *architectures* highlight designs like Qwen2 (next-gen Alibaba), Mistral (efficient attention), Phi-3 (small model tuned to outperform larger ones)[\[10\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=The%20company%20announced%20today%20the,innovations%20developed%20by%20Microsoft%20researchers). *Quantization* notes indicate available 4-bit (Q4)/8-bit models for edge deployment. *Inference backends:* vLLM for dynamic batching on GPUs, TensorRT/DeepStream for optimized CV, Ollama/llama.cpp for local CPU/GGUF serving, etc. All these models are open-source and can be hosted locally or via services like Venice.ai (which provides many of these models through an API)[\[17\]](https://mastra.ai/models/providers/venice#:~:text=Access%2013%20Venice%20AI%20models,Learn%20more%20in). Integration with **TensorZero** is encouraged ‚Äì e.g. use TensorZero‚Äôs dashboards to monitor LLM latency, throughput, and GPU memory for each service[\[71\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/README.md#L30-L38)[\[54\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L236-L244), ensuring the chosen models meet the PMOVES performance and observability standards.

**Sources:** Key details have been drawn from PMOVES documentation and external model references, including PMOVES architecture docs[\[1\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L5-L14)[\[21\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L16-L24), integration plans[\[60\]\[51\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi), model technical reports and announcements (Alibaba‚Äôs Qwen[\[4\]](https://huggingface.co/Qwen/Qwen1.5-1.8B#:~:text=,trust_remote_code)[\[5\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=With%20Qwen1.5%2C%20we%20are%20open,4.37.0%60%20without%20needing%20%60trust_remote_code), Microsoft‚Äôs Phi-3[\[10\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=The%20company%20announced%20today%20the,innovations%20developed%20by%20Microsoft%20researchers), FlagEmbedding/BGE[\[72\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=embeddings)[\[30\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=which%20supports%20multiple%20languages%20and,pl)), and hardware guides (NVIDIA Jetson Orin Nano performance[\[20\]](https://docs.ultralytics.com/guides/nvidia-jetson/#:~:text=We%20have%20updated%20this%20guide,the%20most%20popular%20AI%20models)). These ensure the recommendations are up-to-date as of late 2025 and aligned with PMOVES‚Äôs local-first, multi-modal AI mission.

---

[\[1\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L5-L14) [\[21\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L16-L24) [\[22\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L18-L21) [\[26\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L36-L44) [\[33\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L29-L37) [\[34\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L31-L39) [\[44\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L26-L34) [\[45\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md#L28-L34) PMOVES.md

[https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES.md)

[\[2\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Model%20MMLU%20C,1) [\[3\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=At%20every%20model%20size%2C%20Qwen1,language%20understanding%2C%20reasoning%2C%20and%20math) [\[5\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=With%20Qwen1.5%2C%20we%20are%20open,4.37.0%60%20without%20needing%20%60trust_remote_code) [\[7\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Mistral,5) [\[8\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=models,4.37.0%60%20without%20needing%20%60trust_remote_code) [\[9\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Llama2,5) [\[19\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=Qwen1.5,5) [\[46\]](https://qwenlm.github.io/blog/qwen1.5/#:~:text=This%20release%20brings%20substantial%20improvements,creating%20a%20truly%20%E2%80%9Cgood%E2%80%9D%20model) Introducing Qwen1.5 | Qwen

[https://qwenlm.github.io/blog/qwen1.5/](https://qwenlm.github.io/blog/qwen1.5/)

[\[4\]](https://huggingface.co/Qwen/Qwen1.5-1.8B#:~:text=,trust_remote_code) [\[6\]](https://huggingface.co/Qwen/Qwen1.5-1.8B#:~:text=Qwen1,of%20SWA%20and%20full%20attention) Qwen/Qwen1.5-1.8B ¬∑ Hugging Face

[https://huggingface.co/Qwen/Qwen1.5-1.8B](https://huggingface.co/Qwen/Qwen1.5-1.8B)

[\[10\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=The%20company%20announced%20today%20the,innovations%20developed%20by%20Microsoft%20researchers) [\[11\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20is%20now%20making%20the,its%20size%2C%20the%20company%20said) [\[12\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Microsoft%20also%20announced%20additional%20models,and%20other%20model%20gardens%20shortly) [\[13\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=billion%20parameters%2C%20which%20performs%20better,its%20size%2C%20the%20company%20said) [\[14\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=Last%20year%2C%20after%20spending%20his,how%20to%20connect%20these%20words%3F%E2%80%9D) [\[31\]](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/#:~:text=outperform%20models%20of%20the%20same,innovations%20developed%20by%20Microsoft%20researchers) Tiny but mighty: The Phi-3 small language models with big potential \- Source

[https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/](https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/)

[\[15\]](https://developers.cloudflare.com/workers-ai/models/#:~:text=OpenAI%27s%20open,latency%2C%20and%20local%20or) [\[16\]](https://developers.cloudflare.com/workers-ai/models/#:~:text=OpenAI%27s%20open,latency%2C%20and%20local%20or) Models ¬∑ Cloudflare Workers AI docs

[https://developers.cloudflare.com/workers-ai/models/](https://developers.cloudflare.com/workers-ai/models/)

[\[17\]](https://mastra.ai/models/providers/venice#:~:text=Access%2013%20Venice%20AI%20models,Learn%20more%20in) Venice AI | Models \- Mastra

[https://mastra.ai/models/providers/venice](https://mastra.ai/models/providers/venice)

[\[18\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE) [\[47\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE) [\[48\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE) [\[49\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE) [\[50\]](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE) PMOVES\_Orchestration\_Mesh\_System Report and PRD.md

[https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE](https://drive.google.com/file/d/1K74PjvOr8DeRii1n7lzMfwe0Hele8NPE)

[\[20\]](https://docs.ultralytics.com/guides/nvidia-jetson/#:~:text=We%20have%20updated%20this%20guide,the%20most%20popular%20AI%20models) Quick Start Guide: NVIDIA Jetson with Ultralytics YOLO11

[https://docs.ultralytics.com/guides/nvidia-jetson/](https://docs.ultralytics.com/guides/nvidia-jetson/)

[\[23\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=image,the%20context%20length%20of%20LLM) [\[24\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=embeddings) [\[25\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=retrieval%29,Technical%20Report%20and%20Code) [\[30\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=which%20supports%20multiple%20languages%20and,pl) [\[35\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=%2A%204%2F30%2F2024%3A%20Release%20Llama,BGE%2C%20equipping%20BGE%20with%20visual) [\[42\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=,script%20to%20mine%20hard%20negatives) [\[72\]](https://github.com/FlagOpen/FlagEmbedding#:~:text=embeddings) GitHub \- FlagOpen/FlagEmbedding: Retrieval and Retrieval-augmented LLMs

[https://github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

[\[27\]](https://dataloop.ai/library/model/neofung_ldir-qwen2-reranker-15b/#:~:text=LdIR%20Qwen2%20Reranker%201,the%20FlagEmbedding%20reranker%20and) LdIR Qwen2 Reranker 1.5B ¬∑ Models \- Dataloop

[https://dataloop.ai/library/model/neofung\_ldir-qwen2-reranker-15b/](https://dataloop.ai/library/model/neofung_ldir-qwen2-reranker-15b/)

[\[28\]](https://huggingface.co/neofung/LdIR-Qwen2-reranker-1.5B/discussions/4#:~:text=neofung%2FLdIR,com%2FFlagOpen%2FFlagEmbedding%2F) neofung/LdIR-Qwen2-reranker-1.5B \- Hugging Face

[https://huggingface.co/neofung/LdIR-Qwen2-reranker-1.5B/discussions/4](https://huggingface.co/neofung/LdIR-Qwen2-reranker-1.5B/discussions/4)

[\[29\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/AGENTS.md#L2-L5) AGENTS.md

[https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/AGENTS.md](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/AGENTS.md)

[\[32\]](https://forums.developer.nvidia.com/t/anyway-to-boost-yolo-performance-on-jetson-orin/313795#:~:text=Anyway%20to%20boost%20yolo%20performance,Check%20jtop%2Ftegrastats) Anyway to boost yolo performance on Jetson Orin?

[https://forums.developer.nvidia.com/t/anyway-to-boost-yolo-performance-on-jetson-orin/313795](https://forums.developer.nvidia.com/t/anyway-to-boost-yolo-performance-on-jetson-orin/313795)

[\[36\]](https://simonwillison.net/2025/Aug/22/deepseek-31/#:~:text=DeepSeek%203.1%20,it%27s%20a%20hybrid%20reasoning%20model) [\[37\]](https://simonwillison.net/2025/Aug/22/deepseek-31/#:~:text=DeepSeek%203.1%20,it%27s%20a%20hybrid%20reasoning%20model) DeepSeek 3.1 \- Simon Willison's Weblog

[https://simonwillison.net/2025/Aug/22/deepseek-31/](https://simonwillison.net/2025/Aug/22/deepseek-31/)

[\[38\]](https://docs.unsloth.ai/basics/running-and-saving-models/unsloth-inference#:~:text=Learn%20how%20to%20run%20your,model%20with%20Unsloth%27s%20faster%20inference) [\[39\]](https://docs.unsloth.ai/basics/running-and-saving-models/unsloth-inference#:~:text=model_name%20%3D%20%22lora_model%22%2C%20,text_streamer%2C%20max_new_tokens%20%3D%2064) Unsloth Inference | Unsloth Documentation

[https://docs.unsloth.ai/basics/running-and-saving-models/unsloth-inference](https://docs.unsloth.ai/basics/running-and-saving-models/unsloth-inference)

[\[40\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L166-L175) [\[41\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L196-L203) [\[43\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L164-L172) [\[54\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L236-L244) [\[55\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md#L220-L228) PMOVES\_ADVANCED\_CAPABILITIES.md

[https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES\_ADVANCED\_CAPABILITIES.md](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/docs/PMOVES_ADVANCED_CAPABILITIES.md)

[\[51\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[52\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[53\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[56\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[58\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[59\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[60\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[61\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[62\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[63\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) [\[70\]](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi) CLIP\_QWEN\_INTEGRATION\_PLAN.md

[https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi](https://drive.google.com/file/d/1wlXTWX-JjLTLw-9Z4rlUM7UkA0-DvNPi)

[\[57\]](https://forums.developer.nvidia.com/t/expected-fps-for-yolox-s-tensorrt-engine-on-jetson-orin-nano-jetpack-6-2/329875#:~:text=Expected%20FPS%20for%20YOLOX,No%20official%20benchmarks) Expected FPS for YOLOX-S TensorRT Engine on Jetson Orin Nano ...

[https://forums.developer.nvidia.com/t/expected-fps-for-yolox-s-tensorrt-engine-on-jetson-orin-nano-jetpack-6-2/329875](https://forums.developer.nvidia.com/t/expected-fps-for-yolox-s-tensorrt-engine-on-jetson-orin-nano-jetpack-6-2/329875)

[\[64\]](https://huggingface.co/Qwen/Qwen2.5-Omni-7B#:~:text=Qwen%2FQwen2.5,text%2C%20images%2C%20audio%2C%20and%20video) Qwen/Qwen2.5-Omni-7B \- Hugging Face

[https://huggingface.co/Qwen/Qwen2.5-Omni-7B](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)

[\[65\]](https://levelup.gitconnected.com/qwen2-5-omni-7b-a-multimodal-ai-that-thinks-like-a-human-across-text-image-audio-and-video-a-8e4b37b4c1c4#:~:text=Qwen2.5,text%2C%20images%2C%20audio%2C%20and%20video) Qwen2.5-Omni-7B: A Multimodal AI That Thinks Like a Human ...

[https://levelup.gitconnected.com/qwen2-5-omni-7b-a-multimodal-ai-that-thinks-like-a-human-across-text-image-audio-and-video-a-8e4b37b4c1c4](https://levelup.gitconnected.com/qwen2-5-omni-7b-a-multimodal-ai-that-thinks-like-a-human-across-text-image-audio-and-video-a-8e4b37b4c1c4)

[\[66\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/README.md#L49-L54) [\[71\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/README.md#L30-L38) README.md

[https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/README.md](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/README.md)

[\[67\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice_operator_notes.md#L9-L17) [\[68\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice_operator_notes.md#L10-L18) [\[69\]](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice_operator_notes.md#L26-L30) vibevoice\_operator\_notes.md

[https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice\_operator\_notes.md](https://github.com/POWERFULMOVES/PMOVES.AI/blob/2ce99035e2ab1581868d99abc964283f942cf3f3/pmoves/creator/tutorials/vibevoice_operator_notes.md)