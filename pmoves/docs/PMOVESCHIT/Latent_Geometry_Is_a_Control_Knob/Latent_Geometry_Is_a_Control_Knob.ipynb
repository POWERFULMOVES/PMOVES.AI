{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Geometry Is a Control Knob: A Mini-Empirical Study\n",
        "\n",
        "## Abstract\n",
        "\n",
        "We ran five small, Colab-reproducible experiments to test whether a model’s latent “geometry” (measured with simple graph/topology/distance probes) (i) depends on the observer/process, (ii) tracks OOD behavior, and (iii) can be **directly optimized** to improve robustness without changing the base network. Results: (1) different observers learn measurably different geometries on the same data; (2) training *process* deforms geometry and relates to OOD; (3) path order leaves a geometric “holonomy” imprint; (4) on this dataset the “phase transition” is mild; (5) a tiny **sidecar** trained to reduce δ-hyperbolicity improves OOD while preserving ID accuracy, and beats capacity-matched and head-only controls.\n",
        "\n",
        "---\n",
        "\n",
        "## Common Setup\n",
        "\n",
        "* **Data:** Fashion-MNIST (ID). OOD variants: **+30° rotation** and **elastic transform**.\n",
        "* **Observers:** Small CNN & MLP (penultimate 64-D embedding).\n",
        "* **Probes (test-set embeddings):** CKA, kNN graph Ollivier–Ricci curvature (node/edge + entropy), four-point **δ-hyperbolicity** (median), persistent homology (H₁ peak summary), PCA→2D trustworthiness/continuity, geodesic stretch under input noise.\n",
        "* **Compute:** Colab; seeds fixed; 10-ish epochs unless noted.\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment 1 — Observer Variance (CNN vs MLP)\n",
        "\n",
        "**Claim:** Different observers learn different latent geometries from the same data with similar ID accuracy.\n",
        "\n",
        "**Key results**\n",
        "\n",
        "* **CKA(CNN,MLP)=0.710** (≪1: non-identical reps)\n",
        "* **Ricci (node mean):** CNN **−0.0405**, MLP **+0.0328**; entropy 2.557 vs 2.601\n",
        "* **H₁ peak radius:** CNN **9.80** (count 138) vs MLP **7.14** (148)\n",
        "* **Trustworthiness:** CNN **0.872** vs MLP **0.907**\n",
        "* **Geodesic stretch:** CNN **0.128** vs MLP **0.113**\n",
        "* **δ:** \\~**1.80** for both (similar here)\n",
        "\n",
        "**Takeaway:** With comparable accuracy, geometry differs on multiple axes (curvature, topology, neighborhood preservation).\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment 2 — Energy Injection Deforms Geometry (same CNN, different processes)\n",
        "\n",
        "**Processes**\n",
        "\n",
        "* **E₁ vanilla**, **E₂ entropy** (MixUp+smooth+more dropout), **E₃ curriculum** (easy→full; fixed order).\n",
        "\n",
        "**ID/OOD & geometry (selected)**\n",
        "\n",
        "* **E₁:** ID **91.7%**; OOD(rot) **27.7%**, elastic **85.0%**; δ **1.71**; Ricci node mean **−0.034**.\n",
        "* **E₂:** ID **91.7%**; OOD(rot) **30.5%**, elastic **85.8%**; δ **0.874**; Ricci **−0.050**; geodesic stretch ↑ **0.255**.\n",
        "* **E₃:** ID **90.2%**; OOD(rot) **35.5%**, elastic **81.4%**; δ **1.255**; Ricci **+0.014**.\n",
        "\n",
        "**Small-N correlations (3 points; indicative):**\n",
        "\n",
        "* **(−δ) vs OOD mean:** Pearson **0.814** (p≈0.39)\n",
        "* **Curvature entropy vs OOD mean:** Pearson **−0.842** (p≈0.36)\n",
        "\n",
        "**Takeaway:** Process choice deforms geometry; **lower δ** and the **sign/entropy of Ricci** move with OOD in sensible directions.\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment 3 — Holonomy / Path Dependence\n",
        "\n",
        "**Schedules:** same total budget, **A→B→C→A** vs **A→C→B→A**.\n",
        "\n",
        "**Results**\n",
        "\n",
        "* **ID parity:** 91.75% vs 92.00%\n",
        "* **CKA(final):** **0.951** (still close), but **Procrustes residual:** **0.246** (large)\n",
        "* **OOD(rot):** 27.67% vs **28.27%**; **Linear probe:** 91.37% vs **91.73%**\n",
        "* **δ:** 1.208 vs 1.224\n",
        "\n",
        "**Takeaway:** **Path leaves a geometric imprint** even with similar endpoint performance. Procrustes is more sensitive than CKA here.\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment 4 — Scaling “Phase Transition”\n",
        "\n",
        "**Grid:** widths {16,32,64,128} × train-fraction {0.33,0.66,1.0}.\n",
        "\n",
        "**Observation**\n",
        "\n",
        "* OOD jumps were **modest (≈2–3 pts)**; our heuristic detector found **no ≥10-pt jump**. δ varied in 1.27–1.75 range, not sharply predictive here.\n",
        "\n",
        "**Takeaway:** On Fashion-MNIST with mild OOD, transition is **soft**. (Expect sharper behavior on harder datasets/corruptions.)\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment 5 — Sidecar Bending (utility test)\n",
        "\n",
        "**Setup:** Freeze base CNN; add tiny **sidecar MLP** on embeddings + fresh head. Train with **CE + λ·δ-loss** (batch-wise differentiable). Goal: reduce δ while preserving ID.\n",
        "\n",
        "**Main result (λ=0.05, 6 epochs)**\n",
        "\n",
        "* **δ:** **1.893 → 0.420** (−1.473)\n",
        "* **ID:** **91.91% → 92.06%** (+0.15 pts)\n",
        "* **OOD(rot):** **32.06% → 32.88%** (+0.82 pts)\n",
        "* **OOD(elastic):** **85.36% → 85.62%** (+0.26 pts)\n",
        "\n",
        "**Controls**\n",
        "\n",
        "* **Capacity-matched (λ=0):** ID **92.51%**, rot **31.18%** (↓), elastic **86.55%** (↑), **δ=2.28** (↑). Geometry moved the **wrong way** and rot OOD worsened.\n",
        "* **Head-only (sidecar disabled):** ID **92.12%**, rot **33.53%** (↑), elastic **85.40%** (\\~), **δ≈1.88** (\\~baseline). Improves one shift **without** geometric change.\n",
        "* **Geometry-bent (ours):** largest targeted **δ drop** and **consistent** OOD improvements; **CKA/Procrustes vs base** show the biggest representation change.\n",
        "\n",
        "**Takeaway:** **Optimizing geometry directly is a useful control knob**: we improved OOD with a tiny add-on while keeping the base frozen.\n",
        "\n",
        "---\n",
        "\n",
        "## Interpretation & Significance\n",
        "\n",
        "1. **Observer dependence:** Geometry is not unique; it depends on the observer/architecture.\n",
        "2. **Process dependence:** Training **process** (noise level, order) systematically bends geometry and tracks OOD tendencies.\n",
        "3. **Holonomy:** The **path** through hyperparameter space matters; endpoints with similar accuracy can host different worlds.\n",
        "4. **Control:** Geometry is not just descriptive; it’s **actionable**. A simple δ-loss on a sidecar reshapes geometry and improves OOD.\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations\n",
        "\n",
        "* **Dataset simplicity:** Fashion-MNIST + mild OOD; effects likely underestimates vs CIFAR-10C/strong rotations.\n",
        "* **Proxy metrics:** δ, Ricci, Betti summaries are coarse; they correlate but aren’t full causal stories.\n",
        "* **Small-N correlations:** In Exp-2 the process count is 3; correlations are directional, not statistically strong.\n",
        "* **Compute budget:** Light training/budgets were used for fast iteration.\n",
        "\n",
        "---\n",
        "\n",
        "## What we’d do next\n",
        "\n",
        "* **Harder OOD:** Repeat E2/E4/E5 on **CIFAR-10/10C** and ±60–90° rotations; expect clearer phase-like behavior.\n",
        "* **Broader indicators:** Add **local intrinsic dimensionality (LID)**, **Laplacian spectral decay**, **sectional curvature proxies**.\n",
        "* **Holonomy v2:** Mix optimizers and inject gradient noise (we provided a variant) to push CKA < 0.9 consistently.\n",
        "* **Sidecar objectives:** Blend δ-loss with **κ-histogram matching** or LID regularization; hyper-sweep λ for the best ID-preserving OOD gains.\n",
        "* **Seed sweeps:** Quantify variance; report ΔOOD ± SE.\n",
        "\n",
        "---\n",
        "\n",
        "## Reproducibility Notes\n",
        "\n",
        "* Each experiment was provided as a **single Colab cell** (copy-paste runnable).\n",
        "* All reported numbers above come from your runs:\n",
        "\n",
        "  * **Exp-1:** CKA **0.710**, Ricci node (CNN **−0.0405**, MLP **+0.0328**), trust **0.872/0.907**, etc.\n",
        "  * **Exp-2:** E₁/E₂/E₃ deltas; **δ** tracked OOD directionally.\n",
        "  * **Exp-3:** **Procrustes 0.246** with endpoint parity.\n",
        "  * **Exp-4:** No ≥10-pt jump on this grid.\n",
        "  * **Exp-5:** δ **1.89→0.42**, OOD(rot) **+0.82 pts**, controls validate causality of the geometry loss.\n",
        "\n",
        "---\n",
        "\n",
        "## Glossary (selected)\n",
        "\n",
        "* **CKA:** Alignment of representation similarity matrices (1 = identical up to orthogonal transform and scaling).\n",
        "* **Ollivier–Ricci curvature (graph):** Curvature proxy on kNN graphs; sign/entropy reflect expansion vs contraction tendencies.\n",
        "* **δ-hyperbolicity:** Four-point metric tree-likeness (lower = more tree-like/negatively curved).\n",
        "* **Procrustes residual:** Post-orthogonal alignment discrepancy; higher = embeddings differ more after best rigid alignment.\n",
        "\n",
        "---\n",
        "\n",
        "### One-sentence conclusion\n",
        "\n",
        "Across five compact experiments, we found that latent geometry is **observer- and process-dependent**, **path-dependent**, and—crucially—**controllable**: shaping it (e.g., lowering δ) with a tiny sidecar can **improve OOD** while preserving ID performance.\n"
      ],
      "metadata": {
        "id": "yvc6Nckuy7ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ripser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYVufmMCYZak",
        "outputId": "8d05c837-6768-4b85-dbea-6726820629a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ripser\n",
            "  Downloading ripser-0.6.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (from ripser) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ripser) (2.0.2)\n",
            "Collecting persim (from ripser)\n",
            "  Downloading persim-0.3.8-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from ripser) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from ripser) (1.6.1)\n",
            "Collecting deprecated (from persim->ripser)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting hopcroftkarp (from persim->ripser)\n",
            "  Downloading hopcroftkarp-1.2.5.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from persim->ripser) (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from persim->ripser) (3.10.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->ripser) (3.6.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->persim->ripser) (1.17.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->persim->ripser) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->persim->ripser) (1.17.0)\n",
            "Downloading ripser-0.6.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (827 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.3/827.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading persim-0.3.8-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: hopcroftkarp\n",
            "  Building wheel for hopcroftkarp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopcroftkarp: filename=hopcroftkarp-1.2.5-py2.py3-none-any.whl size=18104 sha256=3eb9cbdf35ddf0aeb40b5ea14af89bb9f54aa26b02734154ff792a7762b651f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/fd/fe/f4b8fd82894e1d9e04040ef41dc5ae6eb7a8e9b0ef5a9402fe\n",
            "Successfully built hopcroftkarp\n",
            "Installing collected packages: hopcroftkarp, deprecated, persim, ripser\n",
            "Successfully installed deprecated-1.2.18 hopcroftkarp-1.2.5 persim-0.3.8 ripser-0.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [colab] Experiment 1 (patched): Observer Variance — robust Betti via subsample + finite thresh\n",
        "!pip -q install graphricci curvature_networkx ripser persim scikit-learn==1.5.2 networkx==3.2.1\n",
        "try:\n",
        "    import GraphRicciCurvature\n",
        "except:\n",
        "    !pip -q install GraphRicciCurvature\n",
        "\n",
        "import os, math, random, sys, gc, time, statistics\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.manifold import trustworthiness\n",
        "from sklearn.decomposition import PCA\n",
        "import networkx as nx\n",
        "from ripser import ripser\n",
        "\n",
        "try:\n",
        "    from GraphRicciCurvature.OllivierRicci import OllivierRicci\n",
        "except Exception:\n",
        "    from graphricci.curvature import OllivierRicci\n",
        "\n",
        "SEED = 1337\n",
        "BATCH = 256\n",
        "EPOCHS = 10\n",
        "LR = 3e-3\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "K_KNN = 15\n",
        "N_QDRUPLES = 3000         # a bit lighter\n",
        "EPS_NOISE = 0.05\n",
        "BETTI_GRID = np.linspace(0.0, 10.0, 50)\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed()\n",
        "\n",
        "# Data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = torchvision.datasets.FashionMNIST(root=\"/content/data\", train=True, download=True, transform=transform)\n",
        "test_ds  = torchvision.datasets.FashionMNIST(root=\"/content/data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Models\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, emb_dim=64, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.fc_emb = nn.Linear(128, emb_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim, n_classes)\n",
        "    def forward(self, x, return_emb=False):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        emb = self.fc_emb(x)\n",
        "        logits = self.fc_out(F.gelu(emb))\n",
        "        return (emb, logits) if return_emb else logits\n",
        "\n",
        "class SmallMLP(nn.Module):\n",
        "    def __init__(self, emb_dim=64, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc_emb = nn.Linear(128, emb_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim, n_classes)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "    def forward(self, x, return_emb=False):\n",
        "        x = self.flat(x)\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.drop(x)\n",
        "        x = F.gelu(self.fc2(x))\n",
        "        emb = self.fc_emb(x)\n",
        "        logits = self.fc_out(F.gelu(emb))\n",
        "        return (emb, logits) if return_emb else logits\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Train/Eval\n",
        "def train_one(model, opt, loader, device=DEVICE):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        loss_sum += loss.item() * xb.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one(model, loader, device=DEVICE):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb)\n",
        "        loss_sum += loss.item() * xb.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings(model, loader, device=DEVICE):\n",
        "    model.eval()\n",
        "    embs, ys = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        emb, _ = model(xb, return_emb=True)\n",
        "        embs.append(emb.detach().cpu().numpy())\n",
        "        ys.append(yb.numpy())\n",
        "    return np.concatenate(embs, 0), np.concatenate(ys, 0)\n",
        "\n",
        "# Probes\n",
        "def center_gram(K):\n",
        "    n = K.shape[0]\n",
        "    H = np.eye(n) - np.ones((n,n))/n\n",
        "    return H @ K @ H\n",
        "\n",
        "def cka(X, Y):\n",
        "    Kx = X @ X.T\n",
        "    Ky = Y @ Y.T\n",
        "    Kx_c = center_gram(Kx); Ky_c = center_gram(Ky)\n",
        "    hsic = np.sum(Kx_c * Ky_c)\n",
        "    var1 = np.sqrt(np.sum(Kx_c * Kx_c))\n",
        "    var2 = np.sqrt(np.sum(Ky_c * Ky_c))\n",
        "    return float(hsic / (var1 * var2 + 1e-12))\n",
        "\n",
        "def build_knn_graph(X, k=K_KNN):\n",
        "    D = pairwise_distances(X, metric=\"euclidean\")\n",
        "    np.fill_diagonal(D, np.inf)\n",
        "    n = X.shape[0]\n",
        "    G = nx.Graph()\n",
        "    for i in range(n):\n",
        "        G.add_node(i)\n",
        "        nbrs = np.argpartition(D[i], k)[:k]\n",
        "        for j in nbrs:\n",
        "            w = 1.0 / (D[i, j] + 1e-9)\n",
        "            G.add_edge(i, j, weight=w, dist=float(D[i,j]))\n",
        "    return G\n",
        "\n",
        "def ricci_curvature_stats(G):\n",
        "    try:\n",
        "        orc = OllivierRicci(G, alpha=0.5, verbose=\"ERROR\")\n",
        "        orc.compute_ricci_curvature()\n",
        "        edge_k = [edata[\"ricciCurvature\"] for _,_,edata in orc.G.edges(data=True)]\n",
        "        node_k = []\n",
        "        for v in orc.G.nodes():\n",
        "            inc = [orc.G[v][u][\"ricciCurvature\"] for u in orc.G.neighbors(v)]\n",
        "            if len(inc)==0: continue\n",
        "            node_k.append(float(np.mean(inc)))\n",
        "        return np.array(edge_k), np.array(node_k)\n",
        "    except Exception as e:\n",
        "        print(\"Ricci curvature failed:\", e)\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "def entropy_hist(x, bins=40):\n",
        "    if len(x)==0: return float(\"nan\")\n",
        "    hist, _ = np.histogram(x, bins=bins, density=True)\n",
        "    p = hist + 1e-12\n",
        "    p = p / p.sum()\n",
        "    return float(-np.sum(p * np.log(p)))\n",
        "\n",
        "def estimate_delta_hyperbolicity(X, n_samples=N_QDRUPLES, metric=\"euclidean\"):\n",
        "    n = X.shape[0]\n",
        "    if n < 4: return float(\"nan\")\n",
        "    idx = np.random.choice(n, size=(n_samples, 4), replace=True)\n",
        "    D = pairwise_distances(X, metric=metric)\n",
        "    deltas = []\n",
        "    for a,b,c,d in idx:\n",
        "        dab, dac, dad, dbc, dbd, dcd = D[a,b], D[a,c], D[a,d], D[b,c], D[b,d], D[c,d]\n",
        "        s = sorted([dab+dcd, dac+dbd, dad+dbc])\n",
        "        deltas.append(0.5 * (s[2] - s[1]))\n",
        "    return float(np.median(deltas))\n",
        "\n",
        "def betti_curves_from_ripser(X, maxdim=1, n_sample=2000, thresh_quantile=0.95):\n",
        "    \"\"\"\n",
        "    Robust PH summary:\n",
        "      - Subsample up to n_sample points\n",
        "      - Compute a finite radius threshold from pairwise distances' quantile\n",
        "      - Call ripser with that numeric thresh\n",
        "      - Build crude Betti curves over BETTI_GRID\n",
        "    \"\"\"\n",
        "    n = X.shape[0]\n",
        "    if n > n_sample:\n",
        "        idx = np.random.choice(n, size=n_sample, replace=False)\n",
        "        Xs = X[idx]\n",
        "    else:\n",
        "        Xs = X\n",
        "\n",
        "    # Pairwise distances for threshold (on the subsample only)\n",
        "    D = pairwise_distances(Xs, metric=\"euclidean\")\n",
        "    # Only upper triangle (exclude zeros)\n",
        "    tri = D[np.triu_indices_from(D, k=1)]\n",
        "    tri = tri[np.isfinite(tri)]\n",
        "    # Guard for degenerate case\n",
        "    if tri.size == 0:\n",
        "        thresh = 1.0\n",
        "    else:\n",
        "        thresh = float(np.quantile(tri, thresh_quantile))\n",
        "        if not np.isfinite(thresh) or thresh <= 0:\n",
        "            thresh = float(np.median(tri) if np.isfinite(np.median(tri)) and np.median(tri)>0 else 1.0)\n",
        "\n",
        "    res = ripser(Xs, maxdim=maxdim, thresh=thresh, metric='euclidean')\n",
        "    dgms = res['dgms']\n",
        "\n",
        "    curves = {}\n",
        "    for dim, dgm in enumerate(dgms):\n",
        "        if dgm.size == 0:\n",
        "            curves[f\"H{dim}\"] = np.zeros_like(BETTI_GRID, dtype=int)\n",
        "            continue\n",
        "        births = dgm[:,0]\n",
        "        deaths = dgm[:,1]\n",
        "        # Cap inf deaths to something larger than our grid max so features persist\n",
        "        cap = max(BETTI_GRID[-1], thresh) * 2.0\n",
        "        deaths = np.where(np.isinf(deaths), cap, deaths)\n",
        "        counts = []\n",
        "        for r in BETTI_GRID:\n",
        "            alive = np.logical_and(births <= r, deaths > r)\n",
        "            counts.append(int(np.sum(alive)))\n",
        "        curves[f\"H{dim}\"] = np.array(counts, dtype=int)\n",
        "    return curves\n",
        "\n",
        "def continuity(high_X, low_Y, n_neighbors=10):\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    n = high_X.shape[0]\n",
        "    nbr_h = NearestNeighbors(n_neighbors=n_neighbors+1).fit(high_X)\n",
        "    nbr_l = NearestNeighbors(n_neighbors=n_neighbors+1).fit(low_Y)\n",
        "    idx_h = nbr_h.kneighbors(return_distance=False)[:,1:]\n",
        "    idx_l = nbr_l.kneighbors(return_distance=False)[:,1:]\n",
        "    ranks_l = np.full((n, n), -1, dtype=int)\n",
        "    for i in range(n):\n",
        "        for rank, j in enumerate(idx_l[i], start=1):\n",
        "            ranks_l[i, j] = rank\n",
        "    s = 0.0\n",
        "    for i in range(n):\n",
        "        missing = [j for j in idx_h[i] if ranks_l[i, j] == -1]\n",
        "        for _ in missing:\n",
        "            s += (n_neighbors)\n",
        "    norm = n * n_neighbors * (2*n - 3*n_neighbors - 1) / 2\n",
        "    return 1.0 - s / max(norm, 1e-9)\n",
        "\n",
        "def geodesic_distortion_under_noise(model, X_images, eps=EPS_NOISE, n_samples=2000, device=DEVICE):\n",
        "    model.eval()\n",
        "    idx = np.random.choice(X_images.shape[0], size=min(n_samples, X_images.shape[0]), replace=False)\n",
        "    xb = torch.from_numpy(X_images[idx]).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb0, _ = model(xb, return_emb=True)\n",
        "        noise = torch.randn_like(xb) * eps\n",
        "        xb_noisy = torch.clamp(xb + noise, 0.0, 1.0)\n",
        "        emb1, _ = model(xb_noisy, return_emb=True)\n",
        "    E0 = emb0.detach().cpu().numpy()\n",
        "    E1 = emb1.detach().cpu().numpy()\n",
        "    D0 = pairwise_distances(E0, metric=\"euclidean\")\n",
        "    D1 = pairwise_distances(E1, metric=\"euclidean\")\n",
        "    mask = ~np.isclose(D0, 0.0)\n",
        "    stretch = np.mean(np.abs(D1[mask] - D0[mask]) / (D0[mask] + 1e-9))\n",
        "    return float(stretch)\n",
        "\n",
        "# Train observers\n",
        "set_seed()\n",
        "cnn = SmallCNN().to(DEVICE)\n",
        "mlp = SmallMLP().to(DEVICE)\n",
        "print(f\"CNN params: {count_params(cnn)/1e6:.3f}M  |  MLP params: {count_params(mlp)/1e6:.3f}M\")\n",
        "\n",
        "opt_cnn = optim.AdamW(cnn.parameters(), lr=LR, weight_decay=1e-4)\n",
        "opt_mlp = optim.AdamW(mlp.parameters(), lr=LR, weight_decay=1e-4)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    trl, tra = train_one(cnn, opt_cnn, train_loader)\n",
        "    vel, vea = eval_one(cnn, test_loader)\n",
        "    trl2, tra2 = train_one(mlp, opt_mlp, train_loader)\n",
        "    vel2, vea2 = eval_one(mlp, test_loader)\n",
        "    print(f\"[Epoch {ep:02d}] CNN   loss {vel:.3f}  acc {vea*100:5.2f}%   |   MLP   loss {vel2:.3f}  acc {vea2*100:5.2f}%\")\n",
        "\n",
        "# Embeddings on test set + raw images\n",
        "@torch.no_grad()\n",
        "def collect_all_test_images(loader):\n",
        "    xs, ys = [], []\n",
        "    for xb, yb in loader:\n",
        "        xs.append(xb.numpy()); ys.append(yb.numpy())\n",
        "    return np.concatenate(xs, 0), np.concatenate(ys, 0)\n",
        "\n",
        "X_images, y_test = collect_all_test_images(test_loader)\n",
        "E_cnn,  y1 = collect_embeddings(cnn, test_loader)\n",
        "E_mlp,  y2 = collect_embeddings(mlp, test_loader)\n",
        "assert np.allclose(y1, y2)\n",
        "\n",
        "# Probes\n",
        "cka_cnn_mlp = cka(E_cnn, E_mlp)\n",
        "\n",
        "G_cnn = build_knn_graph(E_cnn, k=K_KNN)\n",
        "G_mlp = build_knn_graph(E_mlp, k=K_KNN)\n",
        "edge_k_cnn, node_k_cnn = ricci_curvature_stats(G_cnn)\n",
        "edge_k_mlp, node_k_mlp = ricci_curvature_stats(G_mlp)\n",
        "curv_entropy_cnn = entropy_hist(node_k_cnn, bins=40) if node_k_cnn.size>0 else float(\"nan\")\n",
        "curv_entropy_mlp = entropy_hist(node_k_mlp, bins=40) if node_k_mlp.size>0 else float(\"nan\")\n",
        "\n",
        "delta_cnn = estimate_delta_hyperbolicity(E_cnn, n_samples=N_QDRUPLES)\n",
        "delta_mlp = estimate_delta_hyperbolicity(E_mlp, n_samples=N_QDRUPLES)\n",
        "\n",
        "# <-- Patched: robust Betti curves\n",
        "betti_cnn = betti_curves_from_ripser(E_cnn, maxdim=1, n_sample=2000, thresh_quantile=0.95)\n",
        "betti_mlp = betti_curves_from_ripser(E_mlp, maxdim=1, n_sample=2000, thresh_quantile=0.95)\n",
        "\n",
        "def betti_peak_location(curve):\n",
        "    arr = curve.astype(float)\n",
        "    peak_idx = int(np.argmax(arr))\n",
        "    return float(BETTI_GRID[peak_idx]), int(arr[peak_idx])\n",
        "\n",
        "H0_peak_r_cnn, H0_peak_v_cnn = betti_peak_location(betti_cnn[\"H0\"])\n",
        "H1_peak_r_cnn, H1_peak_v_cnn = betti_peak_location(betti_cnn.get(\"H1\", np.zeros_like(BETTI_GRID)))\n",
        "H0_peak_r_mlp, H0_peak_v_mlp = betti_peak_location(betti_mlp[\"H0\"])\n",
        "H1_peak_r_mlp, H1_peak_v_mlp = betti_peak_location(betti_mlp.get(\"H1\", np.zeros_like(BETTI_GRID)))\n",
        "\n",
        "pca = PCA(n_components=2, random_state=SEED)\n",
        "E2_cnn = pca.fit_transform(E_cnn)\n",
        "E2_mlp = pca.fit_transform(E_mlp)\n",
        "\n",
        "trust_cnn = trustworthiness(E_cnn, E2_cnn, n_neighbors=10)\n",
        "trust_mlp = trustworthiness(E_mlp, E2_mlp, n_neighbors=10)\n",
        "cont_cnn = continuity(E_cnn, E2_cnn, n_neighbors=10)\n",
        "cont_mlp = continuity(E_mlp, E2_mlp, n_neighbors=10)\n",
        "\n",
        "geo_stretch_cnn = geodesic_distortion_under_noise(cnn, X_images, eps=EPS_NOISE)\n",
        "geo_stretch_mlp = geodesic_distortion_under_noise(mlp, X_images, eps=EPS_NOISE)\n",
        "\n",
        "# Report\n",
        "def safemean(x): return float(np.mean(x)) if len(x)>0 else float('nan')\n",
        "def safestd(x): return float(np.std(x)) if len(x)>0 else float('nan')\n",
        "\n",
        "print(\"\\n=== Representation Similarity ===\")\n",
        "print(f\"CKA(CNN, MLP): {cka_cnn_mlp:.4f}\")\n",
        "\n",
        "print(\"\\n=== Ricci Curvature (Ollivier) ===\")\n",
        "print(f\"CNN: edge κ mean {safemean(edge_k_cnn):+.4f} ± {safestd(edge_k_cnn):.4f} | node κ mean {safemean(node_k_cnn):+.4f} ± {safestd(node_k_cnn):.4f} | entropy {curv_entropy_cnn:.4f}\")\n",
        "print(f\"MLP: edge κ mean {safemean(edge_k_mlp):+.4f} ± {safestd(edge_k_mlp):.4f} | node κ mean {safemean(node_k_mlp):+.4f} ± {safestd(node_k_mlp):.4f} | entropy {curv_entropy_mlp:.4f}\")\n",
        "\n",
        "print(\"\\n=== δ-hyperbolicity (median, four-point) ===\")\n",
        "print(f\"CNN δ ≈ {delta_cnn:.4f}   |   MLP δ ≈ {delta_mlp:.4f}\")\n",
        "\n",
        "print(\"\\n=== Persistent Homology (crude Betti summaries) ===\")\n",
        "print(f\"CNN: H0 peak at r={H0_peak_r_cnn:.3f} (count={H0_peak_v_cnn}),  H1 peak at r={H1_peak_r_cnn:.3f} (count={H1_peak_v_cnn})\")\n",
        "print(f\"MLP: H0 peak at r={H0_peak_r_mlp:.3f} (count={H0_peak_v_mlp}),  H1 peak at r={H1_peak_r_mlp:.3f} (count={H1_peak_v_mlp})\")\n",
        "\n",
        "print(\"\\n=== Manifold Preservation (PCA→2D) ===\")\n",
        "print(f\"CNN: Trustworthiness={trust_cnn:.4f}, Continuity={cont_cnn:.4f}\")\n",
        "print(f\"MLP: Trustworthiness={trust_mlp:.4f}, Continuity={cont_mlp:.4f}\")\n",
        "\n",
        "print(\"\\n=== Geodesic Distortion under Input Noise ===\")\n",
        "print(f\"CNN: mean relative geodesic stretch={geo_stretch_cnn:.4f}\")\n",
        "print(f\"MLP: mean relative geodesic stretch={geo_stretch_mlp:.4f}\")\n",
        "\n",
        "try:\n",
        "    import json\n",
        "    summary = {\n",
        "        \"cka\": cka_cnn_mlp,\n",
        "        \"ricci\": {\n",
        "            \"cnn\": {\"edge_mean\": safemean(edge_k_cnn), \"edge_std\": safestd(edge_k_cnn),\n",
        "                    \"node_mean\": safemean(node_k_cnn), \"node_std\": safestd(node_k_cnn),\n",
        "                    \"entropy\": curv_entropy_cnn},\n",
        "            \"mlp\": {\"edge_mean\": safemean(edge_k_mlp), \"edge_std\": safestd(edge_k_mlp),\n",
        "                    \"node_mean\": safemean(node_k_mlp), \"node_std\": safestd(node_k_mlp),\n",
        "                    \"entropy\": curv_entropy_mlp}\n",
        "        },\n",
        "        \"delta_hyperbolicity\": {\"cnn\": delta_cnn, \"mlp\": delta_mlp},\n",
        "        \"betti_peaks\": {\n",
        "            \"cnn\": {\"H0_r\": H0_peak_r_cnn, \"H0_val\": H0_peak_v_cnn, \"H1_r\": H1_peak_r_cnn, \"H1_val\": H1_peak_v_cnn},\n",
        "            \"mlp\": {\"H0_r\": H0_peak_r_mlp, \"H0_val\": H0_peak_v_mlp, \"H1_r\": H1_peak_r_mlp, \"H1_val\": H1_peak_v_mlp},\n",
        "        },\n",
        "        \"manifold\": {\"cnn\": {\"trust\": trust_cnn, \"continuity\": cont_cnn},\n",
        "                     \"mlp\": {\"trust\": trust_mlp, \"continuity\": cont_mlp}},\n",
        "        \"geodesic_stretch\": {\"cnn\": geo_stretch_cnn, \"mlp\": geo_stretch_mlp},\n",
        "    }\n",
        "    print(\"\\nJSON summary:\")\n",
        "    print(json.dumps(summary, indent=2))\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CaFPPL_Yema7",
        "outputId": "4342d504-e0c0-4499-824b-9009bcf3cf0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement graphricci (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for graphricci\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\t\t\t<script type=\"text/javascript\">\n",
              "\t\t\t<!--\n",
              "\t\t\t\t\n",
              "\t\t\t{\n",
              "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
              "\t\t\t\tif (element) {\n",
              "\t\t\t\t\telement.parentNode.removeChild(element);\n",
              "\t\t\t\t}\n",
              "\t\t\t\telement = document.createElement('script');\n",
              "\t\t\t\telement.type = 'text/javascript';\n",
              "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
              "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
              "\t\t\t\tdocument.head.appendChild(element);\n",
              "\t\t\t}\n",
              "\t\t\n",
              "\t\t\t\t\n",
              "\t\t\t{\n",
              "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
              "\t\t\t\tif (element) {\n",
              "\t\t\t\t\telement.parentNode.removeChild(element);\n",
              "\t\t\t\t}\n",
              "\t\t\t\telement = document.createElement('style');\n",
              "\t\t\t\telement.type = 'text/css';\n",
              "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
              "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
              "\t\t\t\tdocument.head.appendChild(element);\n",
              "\t\t\t}\n",
              "\t\t\n",
              "\t\t\t\t\n",
              "\t\t\t{\n",
              "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
              "\t\t\t\tif (element) {\n",
              "\t\t\t\t\telement.parentNode.removeChild(element);\n",
              "\t\t\t\t}\n",
              "\t\t\t\telement = document.createElement('div');\n",
              "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
              "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
              "\t\t\t\tdocument.body.appendChild(element);\n",
              "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
              "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
              "\t\t\t\t}\n",
              "\t\t\t}\n",
              "\t\t\n",
              "\t\t\t-->\n",
              "\t\t\t</script>\n",
              "\t\t"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.6MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 203kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.39MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN params: 0.429M  |  MLP params: 0.243M\n",
            "[Epoch 01] CNN   loss 0.354  acc 87.21%   |   MLP   loss 0.448  acc 83.36%\n",
            "[Epoch 02] CNN   loss 0.314  acc 88.41%   |   MLP   loss 0.398  acc 85.60%\n",
            "[Epoch 03] CNN   loss 0.251  acc 90.98%   |   MLP   loss 0.383  acc 86.57%\n",
            "[Epoch 04] CNN   loss 0.255  acc 90.67%   |   MLP   loss 0.350  acc 87.62%\n",
            "[Epoch 05] CNN   loss 0.264  acc 90.46%   |   MLP   loss 0.366  acc 86.70%\n",
            "[Epoch 06] CNN   loss 0.258  acc 90.72%   |   MLP   loss 0.339  acc 87.69%\n",
            "[Epoch 07] CNN   loss 0.248  acc 91.38%   |   MLP   loss 0.344  acc 88.26%\n",
            "[Epoch 08] CNN   loss 0.259  acc 91.28%   |   MLP   loss 0.339  acc 87.92%\n",
            "[Epoch 09] CNN   loss 0.270  acc 91.81%   |   MLP   loss 0.331  acc 88.47%\n",
            "[Epoch 10] CNN   loss 0.275  acc 91.54%   |   MLP   loss 0.326  acc 88.40%\n",
            "\n",
            "=== Representation Similarity ===\n",
            "CKA(CNN, MLP): 0.7101\n",
            "\n",
            "=== Ricci Curvature (Ollivier) ===\n",
            "CNN: edge κ mean -0.0279 ± 0.2041 | node κ mean -0.0405 ± 0.1257 | entropy 2.5572\n",
            "MLP: edge κ mean +0.0425 ± 0.2271 | node κ mean +0.0328 ± 0.1324 | entropy 2.6011\n",
            "\n",
            "=== δ-hyperbolicity (median, four-point) ===\n",
            "CNN δ ≈ 1.8026   |   MLP δ ≈ 1.8056\n",
            "\n",
            "=== Persistent Homology (crude Betti summaries) ===\n",
            "CNN: H0 peak at r=0.000 (count=2000),  H1 peak at r=9.796 (count=138)\n",
            "MLP: H0 peak at r=0.000 (count=2000),  H1 peak at r=7.143 (count=148)\n",
            "\n",
            "=== Manifold Preservation (PCA→2D) ===\n",
            "CNN: Trustworthiness=0.8722, Continuity=0.9990\n",
            "MLP: Trustworthiness=0.9069, Continuity=0.9991\n",
            "\n",
            "=== Geodesic Distortion under Input Noise ===\n",
            "CNN: mean relative geodesic stretch=0.1280\n",
            "MLP: mean relative geodesic stretch=0.1133\n",
            "\n",
            "JSON summary:\n",
            "{\n",
            "  \"cka\": 0.7101321085150837,\n",
            "  \"ricci\": {\n",
            "    \"cnn\": {\n",
            "      \"edge_mean\": -0.027909546412411636,\n",
            "      \"edge_std\": 0.2040823260640846,\n",
            "      \"node_mean\": -0.04054756013475494,\n",
            "      \"node_std\": 0.1257232343792914,\n",
            "      \"entropy\": 2.5572461983177277\n",
            "    },\n",
            "    \"mlp\": {\n",
            "      \"edge_mean\": 0.042495201673653143,\n",
            "      \"edge_std\": 0.2270950095196014,\n",
            "      \"node_mean\": 0.0328195235919744,\n",
            "      \"node_std\": 0.13240999039602797,\n",
            "      \"entropy\": 2.6010598307847888\n",
            "    }\n",
            "  },\n",
            "  \"delta_hyperbolicity\": {\n",
            "    \"cnn\": 1.8026180267333984,\n",
            "    \"mlp\": 1.8056278228759766\n",
            "  },\n",
            "  \"betti_peaks\": {\n",
            "    \"cnn\": {\n",
            "      \"H0_r\": 0.0,\n",
            "      \"H0_val\": 2000,\n",
            "      \"H1_r\": 9.795918367346939,\n",
            "      \"H1_val\": 138\n",
            "    },\n",
            "    \"mlp\": {\n",
            "      \"H0_r\": 0.0,\n",
            "      \"H0_val\": 2000,\n",
            "      \"H1_r\": 7.142857142857143,\n",
            "      \"H1_val\": 148\n",
            "    }\n",
            "  },\n",
            "  \"manifold\": {\n",
            "    \"cnn\": {\n",
            "      \"trust\": 0.8722180229355501,\n",
            "      \"continuity\": 0.9990487956332315\n",
            "    },\n",
            "    \"mlp\": {\n",
            "      \"trust\": 0.9069330822775302,\n",
            "      \"continuity\": 0.9991085983274075\n",
            "    }\n",
            "  },\n",
            "  \"geodesic_stretch\": {\n",
            "    \"cnn\": 0.12800884246826172,\n",
            "    \"mlp\": 0.11326967924833298\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [colab] Experiment 2: Energy Injection Deforms Geometry (same model, different process)\n",
        "!pip -q install graphricci curvature_networkx ripser persim scikit-learn==1.5.2 networkx==3.2.1\n",
        "try:\n",
        "    import GraphRicciCurvature\n",
        "except:\n",
        "    !pip -q install GraphRicciCurvature\n",
        "\n",
        "import os, math, random, sys, gc, time, statistics\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import trustworthiness\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "import networkx as nx\n",
        "from ripser import ripser\n",
        "\n",
        "try:\n",
        "    from GraphRicciCurvature.OllivierRicci import OllivierRicci\n",
        "except Exception:\n",
        "    from graphricci.curvature import OllivierRicci\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "SEED = 1337\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH = 256\n",
        "EPOCHS_BASE = 10          # bump to 15–20 for tighter ID matching\n",
        "LR = 3e-3\n",
        "WD = 1e-4\n",
        "K_KNN = 15\n",
        "N_QDRUPLES = 3000\n",
        "EPS_NOISE = 0.05\n",
        "BETTI_GRID = np.linspace(0.0, 10.0, 50)\n",
        "MIXUP_ALPHA = 0.4\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed()\n",
        "\n",
        "# ---------------- Data ----------------\n",
        "to_tensor = transforms.ToTensor()\n",
        "train_ds_full = FashionMNIST(root=\"/content/data\", train=True, download=True, transform=to_tensor)\n",
        "test_ds       = FashionMNIST(root=\"/content/data\", train=False, download=True, transform=to_tensor)\n",
        "\n",
        "def build_loader(ds, shuffle, batch=BATCH):\n",
        "    return DataLoader(ds, batch_size=batch, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
        "\n",
        "test_loader = build_loader(test_ds, shuffle=False)\n",
        "\n",
        "# OOD testsets: rotation (+30 deg) and elastic distortions\n",
        "rotate30 = transforms.Compose([transforms.RandomRotation(degrees=(30,30)), transforms.ToTensor()])\n",
        "elastic = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ElasticTransform(alpha=50.0, sigma=6.0),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class TransformedCopy(Dataset):\n",
        "    def __init__(self, base, transform):\n",
        "        self.base = base\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x, y = self.base[i]\n",
        "        x = transforms.ToPILImage()(x)\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "test_rot = TransformedCopy(test_ds, transforms.Compose([transforms.RandomRotation((30,30)), transforms.ToTensor()]))\n",
        "test_elastic = TransformedCopy(test_ds, transforms.Compose([transforms.ElasticTransform(alpha=50.0, sigma=6.0), transforms.ToTensor()]))\n",
        "test_rot_loader = build_loader(test_rot, shuffle=False)\n",
        "test_elastic_loader = build_loader(test_elastic, shuffle=False)\n",
        "\n",
        "# For curriculum: identify \"easy\" samples (heuristic)\n",
        "# We'll use a shallow probe to estimate per-sample difficulty: a tiny 1-epoch MLP; top-confidence samples = \"easy\".\n",
        "class TinyProbe(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "    def forward(self, x): return self.fc2(F.gelu(self.fc1(self.flat(x))))\n",
        "\n",
        "probe = TinyProbe().to(DEVICE)\n",
        "opt_p = optim.AdamW(probe.parameters(), lr=5e-3)\n",
        "probe_loader = build_loader(train_ds_full, shuffle=True)\n",
        "probe.train()\n",
        "for xb, yb in probe_loader:\n",
        "    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "    opt_p.zero_grad(); logits = probe(xb); loss = F.cross_entropy(logits, yb)\n",
        "    loss.backward(); opt_p.step()\n",
        "    break  # one quick pass over a single batch\n",
        "\n",
        "probe.eval()\n",
        "with torch.no_grad():\n",
        "    confs = []\n",
        "    for i in range(len(train_ds_full)):\n",
        "        x,y = train_ds_full[i]\n",
        "        p = torch.softmax(probe(x.unsqueeze(0).to(DEVICE)), dim=-1)[0, y].item()\n",
        "        confs.append(p)\n",
        "confs = np.array(confs)\n",
        "easy_idx = np.argsort(-confs)[: len(train_ds_full)//2]   # top 50% confidence as \"easy\"\n",
        "hard_idx = np.setdiff1d(np.arange(len(train_ds_full)), easy_idx)\n",
        "easy_ds = Subset(train_ds_full, easy_idx)\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, emb_dim=64, p_drop=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "        self.fc_emb = nn.Linear(128, 64)\n",
        "        self.fc_out = nn.Linear(64, 10)\n",
        "    def forward(self, x, return_emb=False):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop(F.gelu(self.fc1(x)))\n",
        "        emb = self.fc_emb(x)\n",
        "        logits = self.fc_out(F.gelu(emb))\n",
        "        return (emb, logits) if return_emb else logits\n",
        "\n",
        "def count_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "# ---------------- Training utils ----------------\n",
        "def train_epoch_vanilla(model, opt, loader, label_smoothing=0.0):\n",
        "    model.train(); n=0; correct=0; loss_sum=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb, label_smoothing=label_smoothing)\n",
        "        loss.backward(); opt.step()\n",
        "        loss_sum += loss.item()*xb.size(0)\n",
        "        correct += (logits.argmax(1)==yb).sum().item(); n+=xb.size(0)\n",
        "    return loss_sum/n, correct/n\n",
        "\n",
        "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
        "    if alpha <= 0: return x, y, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size, device=x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, (y_a, y_b), lam\n",
        "\n",
        "def mixup_criterion(logits, y_a, y_b, lam, label_smoothing=0.1):\n",
        "    loss_a = F.cross_entropy(logits, y_a, label_smoothing=label_smoothing)\n",
        "    loss_b = F.cross_entropy(logits, y_b, label_smoothing=label_smoothing)\n",
        "    return lam * loss_a + (1 - lam) * loss_b\n",
        "\n",
        "def train_epoch_mixup(model, opt, loader, label_smoothing=0.1):\n",
        "    model.train(); n=0; correct=0; loss_sum=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        xb, (ya,yb2), lam = mixup_data(xb, yb, alpha=MIXUP_ALPHA)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = mixup_criterion(logits, ya, yb2, lam, label_smoothing)\n",
        "        loss.backward(); opt.step()\n",
        "        # for accuracy (approx), use argmax on logits vs original y\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item(); n+= xb.size(0)\n",
        "        loss_sum += loss.item()*xb.size(0)\n",
        "    return loss_sum/n, correct/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader):\n",
        "    model.eval(); n=0; correct=0; loss_sum=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb)\n",
        "        loss_sum += loss.item()*xb.size(0)\n",
        "        correct += (logits.argmax(1)==yb).sum().item(); n+=xb.size(0)\n",
        "    return loss_sum/n, correct/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings(model, loader):\n",
        "    model.eval(); embs=[]; ys=[]\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        emb, _ = model(xb, return_emb=True)\n",
        "        embs.append(emb.detach().cpu().numpy()); ys.append(yb.numpy())\n",
        "    return np.concatenate(embs,0), np.concatenate(ys,0)\n",
        "\n",
        "# ---------------- Geometry probes ----------------\n",
        "def center_gram(K):\n",
        "    n = K.shape[0]\n",
        "    H = np.eye(n) - np.ones((n,n))/n\n",
        "    return H @ K @ H\n",
        "\n",
        "def cka(X, Y):\n",
        "    Kx = X @ X.T; Ky = Y @ Y.T\n",
        "    Kx_c = center_gram(Kx); Ky_c = center_gram(Ky)\n",
        "    hsic = np.sum(Kx_c * Ky_c)\n",
        "    var1 = np.sqrt(np.sum(Kx_c*Kx_c)); var2 = np.sqrt(np.sum(Ky_c*Ky_c))\n",
        "    return float(hsic / (var1*var2 + 1e-12))\n",
        "\n",
        "def build_knn_graph(X, k=K_KNN):\n",
        "    D = pairwise_distances(X, metric=\"euclidean\")\n",
        "    np.fill_diagonal(D, np.inf)\n",
        "    n = X.shape[0]; G = nx.Graph()\n",
        "    for i in range(n):\n",
        "        G.add_node(i)\n",
        "        nbrs = np.argpartition(D[i], k)[:k]\n",
        "        for j in nbrs:\n",
        "            w = 1.0/(D[i,j]+1e-9); G.add_edge(i,j,weight=w,dist=float(D[i,j]))\n",
        "    return G\n",
        "\n",
        "def ricci_curvature_stats(G):\n",
        "    try:\n",
        "        orc = OllivierRicci(G, alpha=0.5, verbose=\"ERROR\")\n",
        "        orc.compute_ricci_curvature()\n",
        "        edge_k = [edata[\"ricciCurvature\"] for _,_,edata in orc.G.edges(data=True)]\n",
        "        node_k = []\n",
        "        for v in orc.G.nodes():\n",
        "            inc = [orc.G[v][u][\"ricciCurvature\"] for u in orc.G.neighbors(v)]\n",
        "            if len(inc)==0: continue\n",
        "            node_k.append(float(np.mean(inc)))\n",
        "        return np.array(edge_k), np.array(node_k)\n",
        "    except Exception as e:\n",
        "        print(\"Ricci curvature failed:\", e); return np.array([]), np.array([])\n",
        "\n",
        "def entropy_hist(x, bins=40):\n",
        "    if len(x)==0: return float(\"nan\")\n",
        "    hist,_ = np.histogram(x, bins=bins, density=True)\n",
        "    p = hist + 1e-12; p = p/p.sum()\n",
        "    return float(-np.sum(p*np.log(p)))\n",
        "\n",
        "def estimate_delta_hyperbolicity(X, n_samples=N_QDRUPLES):\n",
        "    n = X.shape[0];\n",
        "    if n < 4: return float(\"nan\")\n",
        "    idx = np.random.choice(n, size=(n_samples,4), replace=True)\n",
        "    D = pairwise_distances(X, metric=\"euclidean\")\n",
        "    deltas=[]\n",
        "    for a,b,c,d in idx:\n",
        "        s = sorted([D[a,b]+D[c,d], D[a,c]+D[b,d], D[a,d]+D[b,c]])\n",
        "        deltas.append(0.5*(s[2]-s[1]))\n",
        "    return float(np.median(deltas))\n",
        "\n",
        "def betti_curves_from_ripser(X, maxdim=1, n_sample=2000, thresh_quantile=0.95):\n",
        "    n = X.shape[0]\n",
        "    if n>n_sample:\n",
        "        Xs = X[np.random.choice(n, size=n_sample, replace=False)]\n",
        "    else:\n",
        "        Xs = X\n",
        "    D = pairwise_distances(Xs, metric=\"euclidean\")\n",
        "    tri = D[np.triu_indices_from(D,1)]\n",
        "    tri = tri[np.isfinite(tri)]\n",
        "    thresh = float(np.quantile(tri, thresh_quantile)) if tri.size>0 else 1.0\n",
        "    if not np.isfinite(thresh) or thresh<=0:\n",
        "        thresh = float(np.median(tri) if tri.size>0 else 1.0)\n",
        "    res = ripser(Xs, maxdim=maxdim, thresh=thresh, metric='euclidean')\n",
        "    dgms = res['dgms']\n",
        "    curves={}\n",
        "    for dim,dgm in enumerate(dgms):\n",
        "        if dgm.size==0:\n",
        "            curves[f\"H{dim}\"]=np.zeros_like(BETTI_GRID,dtype=int); continue\n",
        "        births, deaths = dgm[:,0], dgm[:,1]\n",
        "        cap = max(BETTI_GRID[-1], thresh)*2.0\n",
        "        deaths = np.where(np.isinf(deaths), cap, deaths)\n",
        "        counts=[]\n",
        "        for r in BETTI_GRID:\n",
        "            alive = (births<=r) & (deaths>r)\n",
        "            counts.append(int(np.sum(alive)))\n",
        "        curves[f\"H{dim}\"] = np.array(counts, dtype=int)\n",
        "    return curves\n",
        "\n",
        "def continuity(high_X, low_Y, n_neighbors=10):\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    n = high_X.shape[0]\n",
        "    nbr_h = NearestNeighbors(n_neighbors=n_neighbors+1).fit(high_X)\n",
        "    nbr_l = NearestNeighbors(n_neighbors=n_neighbors+1).fit(low_Y)\n",
        "    idx_h = nbr_h.kneighbors(return_distance=False)[:,1:]\n",
        "    idx_l = nbr_l.kneighbors(return_distance=False)[:,1:]\n",
        "    ranks_l = np.full((n,n), -1, dtype=int)\n",
        "    for i in range(n):\n",
        "        for rank,j in enumerate(idx_l[i], start=1):\n",
        "            ranks_l[i,j] = rank\n",
        "    s=0.0\n",
        "    for i in range(n):\n",
        "        missing = [j for j in idx_h[i] if ranks_l[i,j]==-1]\n",
        "        for _ in missing:\n",
        "            s += n_neighbors\n",
        "    norm = n*n_neighbors*(2*n - 3*n_neighbors - 1)/2\n",
        "    return 1.0 - s/max(norm,1e-9)\n",
        "\n",
        "def geodesic_distortion_under_noise(model, X_images, eps=EPS_NOISE, n_samples=2000):\n",
        "    model.eval()\n",
        "    idx = np.random.choice(X_images.shape[0], size=min(n_samples, X_images.shape[0]), replace=False)\n",
        "    xb = torch.from_numpy(X_images[idx]).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        e0,_ = model(xb, return_emb=True)\n",
        "        xb_noisy = torch.clamp(xb + torch.randn_like(xb)*eps, 0.0, 1.0)\n",
        "        e1,_ = model(xb_noisy, return_emb=True)\n",
        "    E0, E1 = e0.cpu().numpy(), e1.cpu().numpy()\n",
        "    D0 = pairwise_distances(E0); D1 = pairwise_distances(E1)\n",
        "    mask = ~np.isclose(D0,0.0)\n",
        "    return float(np.mean(np.abs(D1[mask]-D0[mask])/(D0[mask]+1e-9)))\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_all_images(loader):\n",
        "    xs, ys = [], []\n",
        "    for xb, yb in loader:\n",
        "        xs.append(xb.numpy()); ys.append(yb.numpy())\n",
        "    return np.concatenate(xs,0), np.concatenate(ys,0)\n",
        "\n",
        "# ---------------- Train three processes on SAME architecture ----------------\n",
        "def train_process(process_name):\n",
        "    # Build loaders per process\n",
        "    if process_name == \"E1_vanilla\":\n",
        "        train_loader = build_loader(train_ds_full, shuffle=True)\n",
        "        model = SmallCNN(p_drop=0.1).to(DEVICE)\n",
        "        opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "        for ep in range(1, EPOCHS_BASE+1):\n",
        "            trl, tra = train_epoch_vanilla(model, opt, train_loader, label_smoothing=0.0)\n",
        "        return model\n",
        "\n",
        "    elif process_name == \"E2_entropy\":\n",
        "        # Stronger dropout + MixUp + label smoothing\n",
        "        train_loader = build_loader(train_ds_full, shuffle=True)\n",
        "        model = SmallCNN(p_drop=0.3).to(DEVICE)\n",
        "        opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "        for ep in range(1, EPOCHS_BASE+1):\n",
        "            trl, tra = train_epoch_mixup(model, opt, train_loader, label_smoothing=0.1)\n",
        "        return model\n",
        "\n",
        "    elif process_name == \"E3_curriculum\":\n",
        "        # Easy-first then full; fixed order (no shuffle)\n",
        "        easy_loader = build_loader(easy_ds, shuffle=False)\n",
        "        full_loader_noshuf = build_loader(train_ds_full, shuffle=False)\n",
        "        model = SmallCNN(p_drop=0.1).to(DEVICE)\n",
        "        opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "        # half epochs on easy set, half on full set (no shuffle to encode order)\n",
        "        half = max(1, EPOCHS_BASE//2)\n",
        "        for ep in range(half):\n",
        "            trl, tra = train_epoch_vanilla(model, opt, easy_loader, label_smoothing=0.0)\n",
        "        for ep in range(EPOCHS_BASE - half):\n",
        "            trl, tra = train_epoch_vanilla(model, opt, full_loader_noshuf, label_smoothing=0.0)\n",
        "        return model\n",
        "    else:\n",
        "        raise ValueError(\"Unknown process\")\n",
        "\n",
        "processes = [\"E1_vanilla\", \"E2_entropy\", \"E3_curriculum\"]\n",
        "models = {}\n",
        "metrics = {}\n",
        "\n",
        "# Build a consistent eval suite\n",
        "test_loader_std = test_loader\n",
        "\n",
        "def evaluate_all(name, model):\n",
        "    # Accuracies\n",
        "    val_loss, val_acc = eval_model(model, test_loader_std)\n",
        "    rot_loss, rot_acc = eval_model(model, test_rot_loader)\n",
        "    el_loss, el_acc = eval_model(model, test_elastic_loader)\n",
        "\n",
        "    # Embeddings and probes\n",
        "    E, y = collect_embeddings(model, test_loader_std)\n",
        "    X_images, _ = collect_all_images(test_loader_std)\n",
        "\n",
        "    G = build_knn_graph(E, k=K_KNN)\n",
        "    edge_k, node_k = ricci_curvature_stats(G)\n",
        "    curv_entropy = entropy_hist(node_k, bins=40) if node_k.size>0 else float(\"nan\")\n",
        "    delta = estimate_delta_hyperbolicity(E, n_samples=N_QDRUPLES)\n",
        "    betti = betti_curves_from_ripser(E, maxdim=1, n_sample=2000, thresh_quantile=0.95)\n",
        "    def betti_peak_location(curve):\n",
        "        arr = curve.astype(float); peak_idx = int(np.argmax(arr))\n",
        "        return float(BETTI_GRID[peak_idx]), int(arr[peak_idx])\n",
        "    H0_r, H0_v = betti_peak_location(betti[\"H0\"])\n",
        "    H1_r, H1_v = betti_peak_location(betti.get(\"H1\", np.zeros_like(BETTI_GRID)))\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=SEED)\n",
        "    E2 = pca.fit_transform(E)\n",
        "    trust = trustworthiness(E, E2, n_neighbors=10)\n",
        "    cont  = continuity(E, E2, n_neighbors=10)\n",
        "    stretch = geodesic_distortion_under_noise(model, X_images, eps=EPS_NOISE)\n",
        "\n",
        "    return {\n",
        "        \"val_acc\": val_acc, \"ood_rot_acc\": rot_acc, \"ood_elastic_acc\": el_acc,\n",
        "        \"ricci_node_mean\": float(np.mean(node_k)) if node_k.size>0 else float(\"nan\"),\n",
        "        \"ricci_node_std\": float(np.std(node_k)) if node_k.size>0 else float(\"nan\"),\n",
        "        \"curv_entropy\": curv_entropy,\n",
        "        \"delta\": delta,\n",
        "        \"H0_peak_r\": H0_r, \"H0_peak_v\": H0_v,\n",
        "        \"H1_peak_r\": H1_r, \"H1_peak_v\": H1_v,\n",
        "        \"trust\": trust, \"continuity\": cont, \"geo_stretch\": stretch\n",
        "    }\n",
        "\n",
        "print(\"Training three processes on the SAME architecture...\")\n",
        "for name in processes:\n",
        "    set_seed(SEED)  # reset so weights init comparable\n",
        "    m = train_process(name)\n",
        "    models[name] = m\n",
        "    metrics[name] = evaluate_all(name, m)\n",
        "    print(f\"Done {name}: ID acc={metrics[name]['val_acc']*100:.2f}%, \"\n",
        "          f\"OOD(rot)={metrics[name]['ood_rot_acc']*100:.2f}%, OOD(elastic)={metrics[name]['ood_elastic_acc']*100:.2f}%\")\n",
        "\n",
        "# ---------------- Correlations ----------------\n",
        "# Small-N caveat: just 3 points, but we show directionality\n",
        "curv = np.array([metrics[p][\"curv_entropy\"] for p in processes])\n",
        "neg_delta = -np.array([metrics[p][\"delta\"] for p in processes])\n",
        "ood_mean = np.array([(metrics[p][\"ood_rot_acc\"] + metrics[p][\"ood_elastic_acc\"])/2.0 for p in processes])\n",
        "\n",
        "def safe_corr(x, y, name):\n",
        "    try:\n",
        "        sp = spearmanr(x, y, nan_policy='omit')\n",
        "        pr = pearsonr(x, y)\n",
        "        return f\"{name}: Spearman ρ={sp.correlation:.3f} (p={sp.pvalue:.3f}), Pearson r={pr[0]:.3f} (p={pr[1]:.3f})\"\n",
        "    except Exception as e:\n",
        "        return f\"{name}: n/a ({e})\"\n",
        "\n",
        "print(\"\\n=== Correlations across processes (small-N, indicative) ===\")\n",
        "print(safe_corr(curv, ood_mean, \"Curvature entropy vs OOD acc\"))\n",
        "print(safe_corr(neg_delta, ood_mean, \"(-δ) vs OOD acc\"))\n",
        "\n",
        "# ---------------- Pretty print summary ----------------\n",
        "import json\n",
        "summary = {p: {k: (float(v) if isinstance(v, (int,float,np.floating)) else v) for k,v in metrics[p].items()} for p in processes}\n",
        "print(\"\\nJSON summary:\")\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujjE6Dvch5rz",
        "outputId": "2231f0f4-793a-4eca-d573-7c045fffb7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement graphricci (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for graphricci\u001b[0m\u001b[31m\n",
            "\u001b[0mTraining three processes on the SAME architecture...\n",
            "Done E1_vanilla: ID acc=91.73%, OOD(rot)=27.70%, OOD(elastic)=84.99%\n",
            "Done E2_entropy: ID acc=91.66%, OOD(rot)=30.54%, OOD(elastic)=85.75%\n",
            "Done E3_curriculum: ID acc=90.15%, OOD(rot)=35.52%, OOD(elastic)=81.44%\n",
            "\n",
            "=== Correlations across processes (small-N, indicative) ===\n",
            "Curvature entropy vs OOD acc: Spearman ρ=-0.500 (p=0.667), Pearson r=-0.842 (p=0.362)\n",
            "(-δ) vs OOD acc: Spearman ρ=0.500 (p=0.667), Pearson r=0.814 (p=0.394)\n",
            "\n",
            "JSON summary:\n",
            "{\n",
            "  \"E1_vanilla\": {\n",
            "    \"val_acc\": 0.9173,\n",
            "    \"ood_rot_acc\": 0.277,\n",
            "    \"ood_elastic_acc\": 0.8499,\n",
            "    \"ricci_node_mean\": -0.03358765069120345,\n",
            "    \"ricci_node_std\": 0.12109817245889386,\n",
            "    \"curv_entropy\": 2.696348779383931,\n",
            "    \"delta\": 1.7103328704833984,\n",
            "    \"H0_peak_r\": 0.0,\n",
            "    \"H0_peak_v\": 2000.0,\n",
            "    \"H1_peak_r\": 8.979591836734695,\n",
            "    \"H1_peak_v\": 159.0,\n",
            "    \"trust\": 0.9078135740397616,\n",
            "    \"continuity\": 0.999052891982573,\n",
            "    \"geo_stretch\": 0.14293920993804932\n",
            "  },\n",
            "  \"E2_entropy\": {\n",
            "    \"val_acc\": 0.9166,\n",
            "    \"ood_rot_acc\": 0.3054,\n",
            "    \"ood_elastic_acc\": 0.8575,\n",
            "    \"ricci_node_mean\": -0.05044835100581295,\n",
            "    \"ricci_node_std\": 0.10905075025784566,\n",
            "    \"curv_entropy\": 2.640166762558324,\n",
            "    \"delta\": 0.8737850189208984,\n",
            "    \"H0_peak_r\": 0.0,\n",
            "    \"H0_peak_v\": 2000.0,\n",
            "    \"H1_peak_r\": 4.6938775510204085,\n",
            "    \"H1_peak_v\": 195.0,\n",
            "    \"trust\": 0.8714813561019581,\n",
            "    \"continuity\": 0.9990366468025439,\n",
            "    \"geo_stretch\": 0.2554347813129425\n",
            "  },\n",
            "  \"E3_curriculum\": {\n",
            "    \"val_acc\": 0.9015,\n",
            "    \"ood_rot_acc\": 0.3552,\n",
            "    \"ood_elastic_acc\": 0.8144,\n",
            "    \"ricci_node_mean\": 0.014308369933555054,\n",
            "    \"ricci_node_std\": 0.11813501748594443,\n",
            "    \"curv_entropy\": 2.663342858497222,\n",
            "    \"delta\": 1.2554492950439453,\n",
            "    \"H0_peak_r\": 0.0,\n",
            "    \"H0_peak_v\": 2000.0,\n",
            "    \"H1_peak_r\": 6.122448979591837,\n",
            "    \"H1_peak_v\": 188.0,\n",
            "    \"trust\": 0.9157264489959437,\n",
            "    \"continuity\": 0.9990787420501778,\n",
            "    \"geo_stretch\": 0.15308985114097595\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [colab] Experiment 3: Holonomy / Path Dependence (closed-loop schedules A→B→C→A vs A→C→B→A)\n",
        "!pip -q install scikit-learn==1.5.2 networkx==3.2.1 ripser\n",
        "import numpy as np, random, json, math\n",
        "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from ripser import ripser\n",
        "\n",
        "SEED = 1337\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH = 256\n",
        "EPOCHS_PHASE = 3          # total = 4 phases * EPOCHS_PHASE\n",
        "LR_A, WD_A, SMOOTH_A, MIX_A = 3e-3, 1e-4, 0.00, 0.0\n",
        "LR_B, WD_B, SMOOTH_B, MIX_B = 1e-3, 1e-4, 0.10, 0.4   # \"noisier\" phase\n",
        "LR_C, WD_C, SMOOTH_C, MIX_C = 5e-4, 5e-4, 0.00, 0.0   # strong weight decay phase\n",
        "DROP_P = 0.2\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed()\n",
        "\n",
        "# ---------------- Data (ID + simple OODs) ----------------\n",
        "to_tensor = transforms.ToTensor()\n",
        "train_full = FashionMNIST(\"/content/data\", train=True, download=True, transform=to_tensor)\n",
        "test_ds    = FashionMNIST(\"/content/data\", train=False, download=True, transform=to_tensor)\n",
        "def loader(ds, shuffle, bs=BATCH): return DataLoader(ds, batch_size=bs, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
        "test_loader = loader(test_ds, shuffle=False)\n",
        "\n",
        "class TransformedCopy(Dataset):\n",
        "    def __init__(self, base, transform):\n",
        "        self.base, self.transform = base, transform\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x,y = self.base[i]\n",
        "        x = transforms.ToPILImage()(x)\n",
        "        x = self.transform(x)\n",
        "        return x,y\n",
        "\n",
        "test_rot = TransformedCopy(test_ds, transforms.Compose([transforms.RandomRotation((30,30)), transforms.ToTensor()]))\n",
        "test_elastic = TransformedCopy(test_ds, transforms.Compose([transforms.ElasticTransform(alpha=50.0, sigma=6.0), transforms.ToTensor()]))\n",
        "test_rot_loader = loader(test_rot, shuffle=False)\n",
        "test_elastic_loader = loader(test_elastic, shuffle=False)\n",
        "\n",
        "# A small, stratified subset of train for a linear probe\n",
        "labels = [train_full[i][1] for i in range(len(train_full))]\n",
        "labels = np.array(labels)\n",
        "probe_idx = []\n",
        "for c in range(10):\n",
        "    idx_c = np.where(labels==c)[0]\n",
        "    np.random.shuffle(idx_c)\n",
        "    probe_idx.append(idx_c[:500])   # 500/class = 5k total\n",
        "probe_idx = np.concatenate(probe_idx)\n",
        "probe_train = Subset(train_full, probe_idx)\n",
        "probe_loader = loader(probe_train, shuffle=False)\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, emb_dim=64, p_drop=DROP_P):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "        self.fc_emb = nn.Linear(128, 64)\n",
        "        self.fc_out = nn.Linear(64, 10)\n",
        "    def forward(self, x, return_emb=False):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop(F.gelu(self.fc1(x)))\n",
        "        emb = self.fc_emb(x)\n",
        "        logits = self.fc_out(F.gelu(emb))\n",
        "        return (emb, logits) if return_emb else logits\n",
        "\n",
        "# ---------------- Train utils ----------------\n",
        "def mixup_data(x, y, alpha):\n",
        "    if alpha <= 0: return x, y, 1.0, None\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    return lam * x + (1 - lam) * x[idx], (y, y[idx]), lam, idx\n",
        "\n",
        "def train_epoch(model, opt, loader, label_smoothing=0.0, mixup_alpha=0.0):\n",
        "    model.train(); n=0; correct=0; loss_sum=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        xb2, y_mix, lam, idx = mixup_data(xb, yb, mixup_alpha)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb2)\n",
        "        if isinstance(y_mix, tuple):\n",
        "            ya, yb2 = y_mix\n",
        "            loss = (lam * F.cross_entropy(logits, ya, label_smoothing=label_smoothing) +\n",
        "                    (1-lam) * F.cross_entropy(logits, yb2, label_smoothing=label_smoothing))\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits, yb, label_smoothing=label_smoothing)\n",
        "            pred = logits.argmax(1); correct += (pred==yb).sum().item()\n",
        "        loss.backward(); opt.step()\n",
        "        n += xb.size(0); loss_sum += loss.item()*xb.size(0)\n",
        "    return loss_sum/n, correct/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader):\n",
        "    model.eval(); n=0; correct=0; loss_sum=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb)\n",
        "        loss_sum += loss.item()*xb.size(0)\n",
        "        correct += (logits.argmax(1)==yb).sum().item(); n+=xb.size(0)\n",
        "    return loss_sum/n, correct/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings(model, loader):\n",
        "    model.eval(); embs=[]; ys=[]\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        e,_ = model(xb, return_emb=True)\n",
        "        embs.append(e.cpu().numpy()); ys.append(yb.numpy())\n",
        "    return np.concatenate(embs,0), np.concatenate(ys,0)\n",
        "\n",
        "# ---------------- Geometry probes used here ----------------\n",
        "def center_gram(K):\n",
        "    n = K.shape[0]; H = np.eye(n) - np.ones((n,n))/n\n",
        "    return H @ K @ H\n",
        "\n",
        "def cka(X, Y):\n",
        "    Kx = X @ X.T; Ky = Y @ Y.T\n",
        "    Kx_c = center_gram(Kx); Ky_c = center_gram(Ky)\n",
        "    hsic = np.sum(Kx_c * Ky_c)\n",
        "    var1 = np.sqrt(np.sum(Kx_c*Kx_c)); var2 = np.sqrt(np.sum(Ky_c*Ky_c))\n",
        "    return float(hsic / (var1*var2 + 1e-12))\n",
        "\n",
        "def procrustes_residual(X, Y):\n",
        "    # Orthogonal Procrustes: find R = UV^T minimizing ||XR - Y||\n",
        "    # Center first to remove mean offsets\n",
        "    Xc = X - X.mean(0, keepdims=True)\n",
        "    Yc = Y - Y.mean(0, keepdims=True)\n",
        "    M = Xc.T @ Yc\n",
        "    U,S,Vt = np.linalg.svd(M, full_matrices=False)\n",
        "    R = U @ Vt\n",
        "    XR = Xc @ R\n",
        "    num = np.linalg.norm(XR - Yc, ord='fro')\n",
        "    den = np.linalg.norm(Yc, ord='fro') + 1e-12\n",
        "    return float(num/den)\n",
        "\n",
        "def estimate_delta_hyperbolicity(X, n_samples=2000):\n",
        "    n = X.shape[0]\n",
        "    if n < 4: return float('nan')\n",
        "    idx = np.random.choice(n, size=(n_samples,4), replace=True)\n",
        "    D = pairwise_distances(X, metric=\"euclidean\")\n",
        "    deltas=[]\n",
        "    for a,b,c,d in idx:\n",
        "        s = sorted([D[a,b]+D[c,d], D[a,c]+D[b,d], D[a,d]+D[b,c]])\n",
        "        deltas.append(0.5*(s[2]-s[1]))\n",
        "    return float(np.median(deltas))\n",
        "\n",
        "# ---------------- Phase config & schedules ----------------\n",
        "PHASES = {\n",
        "    \"A\": dict(lr=LR_A, wd=WD_A, smooth=SMOOTH_A, mix=MIX_A),\n",
        "    \"B\": dict(lr=LR_B, wd=WD_B, smooth=SMOOTH_B, mix=MIX_B),\n",
        "    \"C\": dict(lr=LR_C, wd=WD_C, smooth=SMOOTH_C, mix=MIX_C),\n",
        "}\n",
        "\n",
        "def build_optimizer(model, lr, wd):\n",
        "    return optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "def run_schedule(schedule, name):\n",
        "    set_seed(SEED)\n",
        "    model = SmallCNN().to(DEVICE)\n",
        "    # fixed data loaders (shuffle True for stochasticity)\n",
        "    train_loader = DataLoader(train_full, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    for phase in schedule:\n",
        "        cfg = PHASES[phase]\n",
        "        opt = build_optimizer(model, cfg[\"lr\"], cfg[\"wd\"])\n",
        "        for _ in range(EPOCHS_PHASE):\n",
        "            train_epoch(model, opt, train_loader, label_smoothing=cfg[\"smooth\"], mixup_alpha=cfg[\"mix\"])\n",
        "    # Eval\n",
        "    id_loss, id_acc = eval_model(model, test_loader)\n",
        "    rot_loss, rot_acc = eval_model(model, test_rot_loader)\n",
        "    el_loss, el_acc   = eval_model(model, test_elastic_loader)\n",
        "    # Embeddings (test) for geometry comparisons\n",
        "    E, y = collect_embeddings(model, test_loader)\n",
        "    # Probe embeddings for linear probe\n",
        "    E_probe, y_probe = collect_embeddings(model, probe_loader)\n",
        "    # δ-hyperbolicity\n",
        "    delta = estimate_delta_hyperbolicity(E, n_samples=2000)\n",
        "    return {\n",
        "        \"model\": model, \"E_test\": E, \"y_test\": y, \"E_probe\": E_probe, \"y_probe\": y_probe,\n",
        "        \"id_acc\": id_acc, \"ood_rot_acc\": rot_acc, \"ood_elastic_acc\": el_acc, \"delta\": delta\n",
        "    }\n",
        "\n",
        "schedule_loop     = [\"A\",\"B\",\"C\",\"A\"]\n",
        "schedule_revloop  = [\"A\",\"C\",\"B\",\"A\"]\n",
        "\n",
        "print(\"Training closed-loop schedules...\")\n",
        "res_loop    = run_schedule(schedule_loop, \"loop_ABC\")\n",
        "res_rev     = run_schedule(schedule_revloop, \"loop_ACB\")\n",
        "\n",
        "# Endpoint behavioral parity (ID acc)\n",
        "print(f\"ID acc — loop: {res_loop['id_acc']*100:.2f}%  |  reverse: {res_rev['id_acc']*100:.2f}%\")\n",
        "print(f\"OOD(rot) — loop: {res_loop['ood_rot_acc']*100:.2f}%  |  reverse: {res_rev['ood_rot_acc']*100:.2f}%\")\n",
        "print(f\"OOD(elastic) — loop: {res_loop['ood_elastic_acc']*100:.2f}%  |  reverse: {res_rev['ood_elastic_acc']*100:.2f}%\")\n",
        "\n",
        "# Geometry difference: CKA + orthogonal Procrustes residual + δ\n",
        "cka_final = cka(res_loop[\"E_test\"], res_rev[\"E_test\"])\n",
        "proc_res  = procrustes_residual(res_loop[\"E_test\"], res_rev[\"E_test\"])\n",
        "print(f\"CKA(final embeddings): {cka_final:.4f}  (lower = more different)\")\n",
        "print(f\"Procrustes residual:   {proc_res:.4f}  (higher = more different)\")\n",
        "print(f\"δ-hyperbolicity — loop: {res_loop['delta']:.4f} | reverse: {res_rev['delta']:.4f}\")\n",
        "\n",
        "# Linear probe: train on *the same* probe subset for each model separately; test on the same test set\n",
        "def linear_probe_accuracy(E_probe, y_probe, E_test, y_test):\n",
        "    clf = LogisticRegression(max_iter=200, n_jobs=None)\n",
        "    clf.fit(E_probe, y_probe)\n",
        "    return float(clf.score(E_test, y_test))\n",
        "\n",
        "lp_loop = linear_probe_accuracy(res_loop[\"E_probe\"], res_loop[\"y_probe\"], res_loop[\"E_test\"], res_loop[\"y_test\"])\n",
        "lp_rev  = linear_probe_accuracy(res_rev[\"E_probe\"],  res_rev[\"y_probe\"],  res_rev[\"E_test\"],  res_rev[\"y_test\"])\n",
        "print(f\"Linear-probe acc — loop: {lp_loop*100:.2f}%  |  reverse: {lp_rev*100:.2f}%\")\n",
        "\n",
        "summary = {\n",
        "    \"id_acc\": {\"loop\": res_loop[\"id_acc\"], \"reverse\": res_rev[\"id_acc\"]},\n",
        "    \"ood_rot_acc\": {\"loop\": res_loop[\"ood_rot_acc\"], \"reverse\": res_rev[\"ood_rot_acc\"]},\n",
        "    \"ood_elastic_acc\": {\"loop\": res_loop[\"ood_elastic_acc\"], \"reverse\": res_rev[\"ood_elastic_acc\"]},\n",
        "    \"cka_final\": cka_final,\n",
        "    \"procrustes_residual\": proc_res,\n",
        "    \"delta\": {\"loop\": res_loop[\"delta\"], \"reverse\": res_rev[\"delta\"]},\n",
        "    \"linear_probe_acc\": {\"loop\": lp_loop, \"reverse\": lp_rev},\n",
        "    \"schedules\": {\"loop\": schedule_loop, \"reverse\": schedule_revloop},\n",
        "    \"phase_configs\": PHASES,\n",
        "    \"epochs_per_phase\": EPOCHS_PHASE\n",
        "}\n",
        "print(\"\\nJSON summary:\")\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO8QrOIgk8-1",
        "outputId": "cb98ed34-d61b-49ed-d5a0-2c3d2c692910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mTraining closed-loop schedules...\n",
            "ID acc — loop: 91.75%  |  reverse: 92.00%\n",
            "OOD(rot) — loop: 27.67%  |  reverse: 28.27%\n",
            "OOD(elastic) — loop: 86.30%  |  reverse: 86.07%\n",
            "CKA(final embeddings): 0.9509  (lower = more different)\n",
            "Procrustes residual:   0.2456  (higher = more different)\n",
            "δ-hyperbolicity — loop: 1.2079 | reverse: 1.2237\n",
            "Linear-probe acc — loop: 91.37%  |  reverse: 91.73%\n",
            "\n",
            "JSON summary:\n",
            "{\n",
            "  \"id_acc\": {\n",
            "    \"loop\": 0.9175,\n",
            "    \"reverse\": 0.92\n",
            "  },\n",
            "  \"ood_rot_acc\": {\n",
            "    \"loop\": 0.2767,\n",
            "    \"reverse\": 0.2827\n",
            "  },\n",
            "  \"ood_elastic_acc\": {\n",
            "    \"loop\": 0.863,\n",
            "    \"reverse\": 0.8607\n",
            "  },\n",
            "  \"cka_final\": 0.9509247667183135,\n",
            "  \"procrustes_residual\": 0.24563682079315186,\n",
            "  \"delta\": {\n",
            "    \"loop\": 1.2079219818115234,\n",
            "    \"reverse\": 1.2236766815185547\n",
            "  },\n",
            "  \"linear_probe_acc\": {\n",
            "    \"loop\": 0.9137,\n",
            "    \"reverse\": 0.9173\n",
            "  },\n",
            "  \"schedules\": {\n",
            "    \"loop\": [\n",
            "      \"A\",\n",
            "      \"B\",\n",
            "      \"C\",\n",
            "      \"A\"\n",
            "    ],\n",
            "    \"reverse\": [\n",
            "      \"A\",\n",
            "      \"C\",\n",
            "      \"B\",\n",
            "      \"A\"\n",
            "    ]\n",
            "  },\n",
            "  \"phase_configs\": {\n",
            "    \"A\": {\n",
            "      \"lr\": 0.003,\n",
            "      \"wd\": 0.0001,\n",
            "      \"smooth\": 0.0,\n",
            "      \"mix\": 0.0\n",
            "    },\n",
            "    \"B\": {\n",
            "      \"lr\": 0.001,\n",
            "      \"wd\": 0.0001,\n",
            "      \"smooth\": 0.1,\n",
            "      \"mix\": 0.4\n",
            "    },\n",
            "    \"C\": {\n",
            "      \"lr\": 0.0005,\n",
            "      \"wd\": 0.0005,\n",
            "      \"smooth\": 0.0,\n",
            "      \"mix\": 0.0\n",
            "    }\n",
            "  },\n",
            "  \"epochs_per_phase\": 3\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [colab] Experiment 4: Curvature \"phase transition\" across scales\n",
        "!pip -q install scikit-learn==1.5.2 ripser\n",
        "import numpy as np, random, json\n",
        "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from ripser import ripser\n",
        "\n",
        "SEED = 1337\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH = 256\n",
        "EPOCHS = 8\n",
        "LR = 3e-3\n",
        "WD = 1e-4\n",
        "\n",
        "WIDTHS = [16, 32, 64, 128]      # embedding dims / hidden width\n",
        "DATA_FRACS = [0.33, 0.66, 1.0]  # train data fractions\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed()\n",
        "\n",
        "to_tensor = transforms.ToTensor()\n",
        "train_full = FashionMNIST(\"/content/data\", train=True, download=True, transform=to_tensor)\n",
        "test_ds    = FashionMNIST(\"/content/data\", train=False, download=True, transform=to_tensor)\n",
        "def loader(ds, shuffle, bs=BATCH): return DataLoader(ds, batch_size=bs, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
        "\n",
        "class TransformedCopy(Dataset):\n",
        "    def __init__(self, base, transform): self.base, self.transform = base, transform\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x,y = self.base[i]; x = transforms.ToPILImage()(x)\n",
        "        return self.transform(x), y\n",
        "\n",
        "test_loader = loader(test_ds, shuffle=False)\n",
        "test_rot_loader = loader(TransformedCopy(test_ds, transforms.Compose([transforms.RandomRotation((30,30)), transforms.ToTensor()])), shuffle=False)\n",
        "test_elastic_loader = loader(TransformedCopy(test_ds, transforms.Compose([transforms.ElasticTransform(alpha=50.0, sigma=6.0), transforms.ToTensor()])), shuffle=False)\n",
        "\n",
        "class CNNWidth(nn.Module):\n",
        "    def __init__(self, emb_dim=64, hid=128, drop=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(64*7*7, hid)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.fc_emb = nn.Linear(hid, emb_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim, 10)\n",
        "    def forward(self, x, return_emb=False):\n",
        "        x = self.pool(F.relu(self.conv1(x))); x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop(F.gelu(self.fc1(x)))\n",
        "        emb = self.fc_emb(x)\n",
        "        logits = self.fc_out(F.gelu(emb))\n",
        "        return (emb, logits) if return_emb else logits\n",
        "\n",
        "def train_model(model, train_loader):\n",
        "    opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    for _ in range(EPOCHS):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            loss = F.cross_entropy(model(xb), yb)\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_acc(model, loader):\n",
        "    model.eval(); n=0; c=0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        pred = model(xb).argmax(1)\n",
        "        c += (pred==yb).sum().item(); n += xb.size(0)\n",
        "    return c/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings(model, loader):\n",
        "    model.eval(); embs=[]\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        e,_ = model(xb, return_emb=True)\n",
        "        embs.append(e.cpu().numpy())\n",
        "    return np.concatenate(embs,0)\n",
        "\n",
        "def delta_hyperbolicity(X, n_samples=2000):\n",
        "    n = X.shape[0]\n",
        "    if n<4: return float('nan')\n",
        "    idx = np.random.choice(n, size=(n_samples,4), replace=True)\n",
        "    D = pairwise_distances(X)\n",
        "    deltas=[]\n",
        "    for a,b,c,d in idx:\n",
        "        s = sorted([D[a,b]+D[c,d], D[a,c]+D[b,d], D[a,d]+D[b,c]])\n",
        "        deltas.append(0.5*(s[2]-s[1]))\n",
        "    return float(np.median(deltas))\n",
        "\n",
        "def betti_peaks(X, maxdim=1, n_sample=2000, thresh_q=0.95):\n",
        "    n = X.shape[0]\n",
        "    if n>n_sample:\n",
        "        Xs = X[np.random.choice(n, size=n_sample, replace=False)]\n",
        "    else:\n",
        "        Xs = X\n",
        "    D = pairwise_distances(Xs)\n",
        "    tri = D[np.triu_indices_from(D,1)]\n",
        "    tri = tri[np.isfinite(tri)]\n",
        "    thresh = float(np.quantile(tri, thresh_q)) if tri.size>0 else 1.0\n",
        "    res = ripser(Xs, maxdim=maxdim, thresh=thresh, metric='euclidean')\n",
        "    dgms = res['dgms']\n",
        "    # H1 peak count as a simple topo scalar\n",
        "    if len(dgms)>1 and dgms[1].size>0:\n",
        "        return int(max(1, dgms[1].shape[0]))\n",
        "    return 0\n",
        "\n",
        "# Build train subsets per fraction (stratified-ish)\n",
        "labels = np.array([train_full[i][1] for i in range(len(train_full))])\n",
        "per_class = {c: np.where(labels==c)[0] for c in range(10)}\n",
        "\n",
        "results = []\n",
        "for frac in DATA_FRACS:\n",
        "    idxs=[]\n",
        "    for c in range(10):\n",
        "        n_c = int(len(per_class[c])*frac)\n",
        "        idxs.extend(np.random.permutation(per_class[c])[:n_c])\n",
        "    idxs = np.array(idxs)\n",
        "    train_sub = Subset(train_full, idxs)\n",
        "    train_loader = loader(train_sub, shuffle=True)\n",
        "    for w in WIDTHS:\n",
        "        set_seed(SEED)\n",
        "        model = CNNWidth(emb_dim=w, hid=max(64, 2*w)).to(DEVICE)\n",
        "        train_model(model, train_loader)\n",
        "        id_acc = eval_acc(model, test_loader)\n",
        "        rot_acc = eval_acc(model, test_rot_loader)\n",
        "        el_acc  = eval_acc(model, test_elastic_loader)\n",
        "        E = collect_embeddings(model, test_loader)\n",
        "        delt = delta_hyperbolicity(E)\n",
        "        h1peak = betti_peaks(E)\n",
        "        results.append(dict(frac=frac, width=w, id=id_acc, ood_mean=(rot_acc+el_acc)/2, rot=rot_acc, el=el_acc, delta=delt, H1_peak=h1peak))\n",
        "        print(f\"frac={frac:.2f} width={w:3d} | ID {id_acc*100:5.2f}% | OOD_mean {( (rot_acc+el_acc)/2 )*100:5.2f}% | δ {delt:.3f} | H1 {h1peak}\")\n",
        "\n",
        "# Detect largest jump in OOD vs width for each frac and check δ threshold\n",
        "def detect_phase_transitions(res, delta_thresh=1.2, jump_min=0.10):\n",
        "    out=[]\n",
        "    for frac in sorted(set(r[\"frac\"] for r in res)):\n",
        "        sub = sorted([r for r in res if r[\"frac\"]==frac], key=lambda x:x[\"width\"])\n",
        "        oods = np.array([r[\"ood_mean\"] for r in sub])\n",
        "        widths = np.array([r[\"width\"] for r in sub])\n",
        "        deltas = np.array([r[\"delta\"] for r in sub])\n",
        "        jumps = np.diff(oods)\n",
        "        if len(jumps)==0:\n",
        "            out.append(dict(frac=frac, S_star=None, jump=None, delta_at=None, passes=False)); continue\n",
        "        j_idx = int(np.argmax(jumps))\n",
        "        S_star = int(widths[j_idx+1])\n",
        "        jump = float(jumps[j_idx])\n",
        "        delta_at = float(deltas[j_idx+1])\n",
        "        passes = (jump >= jump_min) and (delta_at <= delta_thresh)\n",
        "        out.append(dict(frac=frac, S_star=S_star, jump=jump, delta_at=delta_at, passes=passes))\n",
        "    return out\n",
        "\n",
        "transitions = detect_phase_transitions(results, delta_thresh=1.2, jump_min=0.10)  # 10-point OOD jump & δ <= 1.2\n",
        "print(\"\\nPhase transition detection (per data fraction):\")\n",
        "print(json.dumps(transitions, indent=2))\n",
        "\n",
        "print(\"\\nAll results JSON:\")\n",
        "print(json.dumps(results, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRLktRyynWAA",
        "outputId": "4d57c16c-3fb2-4a0a-bbdd-84b0d97666da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frac=0.33 width= 16 | ID 88.74% | OOD_mean 60.30% | δ 1.356 | H1 1119\n",
            "frac=0.33 width= 32 | ID 89.20% | OOD_mean 57.80% | δ 1.308 | H1 1113\n",
            "frac=0.33 width= 64 | ID 89.62% | OOD_mean 60.74% | δ 1.313 | H1 1228\n",
            "frac=0.33 width=128 | ID 90.14% | OOD_mean 61.03% | δ 1.478 | H1 1190\n",
            "frac=0.66 width= 16 | ID 90.33% | OOD_mean 56.36% | δ 1.560 | H1 1196\n",
            "frac=0.66 width= 32 | ID 90.16% | OOD_mean 58.45% | δ 1.324 | H1 1154\n",
            "frac=0.66 width= 64 | ID 90.91% | OOD_mean 56.06% | δ 1.581 | H1 1223\n",
            "frac=0.66 width=128 | ID 91.07% | OOD_mean 57.71% | δ 1.746 | H1 1174\n",
            "frac=1.00 width= 16 | ID 90.74% | OOD_mean 55.27% | δ 1.421 | H1 1217\n",
            "frac=1.00 width= 32 | ID 91.94% | OOD_mean 57.48% | δ 1.268 | H1 1207\n",
            "frac=1.00 width= 64 | ID 91.19% | OOD_mean 56.39% | δ 1.503 | H1 1221\n",
            "frac=1.00 width=128 | ID 91.53% | OOD_mean 54.41% | δ 1.647 | H1 1203\n",
            "\n",
            "Phase transition detection (per data fraction):\n",
            "[\n",
            "  {\n",
            "    \"frac\": 0.33,\n",
            "    \"S_star\": 64,\n",
            "    \"jump\": 0.029350000000000098,\n",
            "    \"delta_at\": 1.3130502700805664,\n",
            "    \"passes\": false\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.66,\n",
            "    \"S_star\": 32,\n",
            "    \"jump\": 0.02090000000000003,\n",
            "    \"delta_at\": 1.3243389129638672,\n",
            "    \"passes\": false\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 1.0,\n",
            "    \"S_star\": 32,\n",
            "    \"jump\": 0.022099999999999898,\n",
            "    \"delta_at\": 1.2683439254760742,\n",
            "    \"passes\": false\n",
            "  }\n",
            "]\n",
            "\n",
            "All results JSON:\n",
            "[\n",
            "  {\n",
            "    \"frac\": 0.33,\n",
            "    \"width\": 16,\n",
            "    \"id\": 0.8874,\n",
            "    \"ood_mean\": 0.60295,\n",
            "    \"rot\": 0.397,\n",
            "    \"el\": 0.8089,\n",
            "    \"delta\": 1.3563470840454102,\n",
            "    \"H1_peak\": 1119\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.33,\n",
            "    \"width\": 32,\n",
            "    \"id\": 0.892,\n",
            "    \"ood_mean\": 0.57805,\n",
            "    \"rot\": 0.326,\n",
            "    \"el\": 0.8301,\n",
            "    \"delta\": 1.3078689575195312,\n",
            "    \"H1_peak\": 1113\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.33,\n",
            "    \"width\": 64,\n",
            "    \"id\": 0.8962,\n",
            "    \"ood_mean\": 0.6074,\n",
            "    \"rot\": 0.3786,\n",
            "    \"el\": 0.8362,\n",
            "    \"delta\": 1.3130502700805664,\n",
            "    \"H1_peak\": 1228\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.33,\n",
            "    \"width\": 128,\n",
            "    \"id\": 0.9014,\n",
            "    \"ood_mean\": 0.6103000000000001,\n",
            "    \"rot\": 0.3793,\n",
            "    \"el\": 0.8413,\n",
            "    \"delta\": 1.4780330657958984,\n",
            "    \"H1_peak\": 1190\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.66,\n",
            "    \"width\": 16,\n",
            "    \"id\": 0.9033,\n",
            "    \"ood_mean\": 0.5636,\n",
            "    \"rot\": 0.2886,\n",
            "    \"el\": 0.8386,\n",
            "    \"delta\": 1.5604743957519531,\n",
            "    \"H1_peak\": 1196\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.66,\n",
            "    \"width\": 32,\n",
            "    \"id\": 0.9016,\n",
            "    \"ood_mean\": 0.5845,\n",
            "    \"rot\": 0.3176,\n",
            "    \"el\": 0.8514,\n",
            "    \"delta\": 1.3243389129638672,\n",
            "    \"H1_peak\": 1154\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.66,\n",
            "    \"width\": 64,\n",
            "    \"id\": 0.9091,\n",
            "    \"ood_mean\": 0.56065,\n",
            "    \"rot\": 0.2766,\n",
            "    \"el\": 0.8447,\n",
            "    \"delta\": 1.5805625915527344,\n",
            "    \"H1_peak\": 1223\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 0.66,\n",
            "    \"width\": 128,\n",
            "    \"id\": 0.9107,\n",
            "    \"ood_mean\": 0.5770500000000001,\n",
            "    \"rot\": 0.3044,\n",
            "    \"el\": 0.8497,\n",
            "    \"delta\": 1.7455902099609375,\n",
            "    \"H1_peak\": 1174\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 1.0,\n",
            "    \"width\": 16,\n",
            "    \"id\": 0.9074,\n",
            "    \"ood_mean\": 0.5526500000000001,\n",
            "    \"rot\": 0.2659,\n",
            "    \"el\": 0.8394,\n",
            "    \"delta\": 1.4213218688964844,\n",
            "    \"H1_peak\": 1217\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 1.0,\n",
            "    \"width\": 32,\n",
            "    \"id\": 0.9194,\n",
            "    \"ood_mean\": 0.57475,\n",
            "    \"rot\": 0.2916,\n",
            "    \"el\": 0.8579,\n",
            "    \"delta\": 1.2683439254760742,\n",
            "    \"H1_peak\": 1207\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 1.0,\n",
            "    \"width\": 64,\n",
            "    \"id\": 0.9119,\n",
            "    \"ood_mean\": 0.56395,\n",
            "    \"rot\": 0.2828,\n",
            "    \"el\": 0.8451,\n",
            "    \"delta\": 1.5026636123657227,\n",
            "    \"H1_peak\": 1221\n",
            "  },\n",
            "  {\n",
            "    \"frac\": 1.0,\n",
            "    \"width\": 128,\n",
            "    \"id\": 0.9153,\n",
            "    \"ood_mean\": 0.54405,\n",
            "    \"rot\": 0.2404,\n",
            "    \"el\": 0.8477,\n",
            "    \"delta\": 1.6473503112792969,\n",
            "    \"H1_peak\": 1203\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [colab] Experiment 5: Sidecar Bending — minimize δ-hyperbolicity with frozen base\n",
        "!pip -q install scikit-learn==1.5.2\n",
        "\n",
        "import numpy as np, random, json\n",
        "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "SEED = 1337\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH = 256\n",
        "EPOCHS_BASE = 10        # base training\n",
        "EPOCHS_SIDECAR = 6      # sidecar training; bump to 10–15 if you want a stronger effect\n",
        "LR_BASE = 3e-3\n",
        "LR_SIDECAR = 3e-3\n",
        "WD = 1e-4\n",
        "EMB_DIM = 64\n",
        "DELTA_QUADS_PER_BATCH = 256   # quadruples sampled per batch for δ-loss\n",
        "LAMBDA_GEOM = 0.05            # geometry loss weight; try 0.03–0.1 range\n",
        "LABEL_SMOOTH = 0.05\n",
        "\n",
        "def set_seed(s=SEED):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
        "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "set_seed()\n",
        "\n",
        "# ---------------- Data & OOD ----------------\n",
        "to_tensor = transforms.ToTensor()\n",
        "train_ds = FashionMNIST(\"/content/data\", train=True, download=True, transform=to_tensor)\n",
        "test_ds  = FashionMNIST(\"/content/data\", train=False, download=True, transform=to_tensor)\n",
        "\n",
        "def make_loader(ds, shuffle, bs=BATCH):\n",
        "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
        "\n",
        "train_loader = make_loader(train_ds, shuffle=True)\n",
        "test_loader  = make_loader(test_ds,  shuffle=False)\n",
        "\n",
        "class TransformedCopy(Dataset):\n",
        "    def __init__(self, base, transform): self.base, self.transform = base, transform\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x,y = self.base[i]\n",
        "        x = transforms.ToPILImage()(x)\n",
        "        return self.transform(x), y\n",
        "\n",
        "test_rot = TransformedCopy(test_ds, transforms.Compose([transforms.RandomRotation((30,30)), transforms.ToTensor()]))\n",
        "test_elastic = TransformedCopy(test_ds, transforms.Compose([transforms.ElasticTransform(alpha=50.0, sigma=6.0), transforms.ToTensor()]))\n",
        "test_rot_loader = make_loader(test_rot, shuffle=False)\n",
        "test_elastic_loader = make_loader(test_elastic, shuffle=False)\n",
        "\n",
        "# ---------------- Model ----------------\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, emb_dim=EMB_DIM, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "        self.fc_emb = nn.Linear(128, emb_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim, 10)\n",
        "    def features(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.drop(F.gelu(self.fc1(x)))\n",
        "        return self.fc_emb(x)         # embedding (no activation)\n",
        "    def forward(self, x, return_emb=False):\n",
        "        emb = self.features(x)\n",
        "        logits = self.fc_out(F.gelu(emb))\n",
        "        return (emb, logits) if return_emb else logits\n",
        "\n",
        "class Sidecar(nn.Module):\n",
        "    \"\"\"Tiny residual MLP that bends the embedding space: e' = e + α * f(e).\"\"\"\n",
        "    def __init__(self, d=EMB_DIM, h=128, alpha_init=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(d),\n",
        "            nn.Linear(d, h),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(h, d)\n",
        "        )\n",
        "        # Learnable scalar to limit the sidecar's influence\n",
        "        self.alpha = nn.Parameter(torch.tensor(alpha_init, dtype=torch.float32))\n",
        "    def forward(self, e):\n",
        "        return e + self.alpha * self.net(e)\n",
        "\n",
        "class FrozenExtractor(nn.Module):\n",
        "    \"\"\"Wraps a frozen base; exposes only 'features' (embedding) forward.\"\"\"\n",
        "    def __init__(self, base):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        for p in self.base.parameters():\n",
        "            p.requires_grad = False\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            return self.base.features(x)\n",
        "\n",
        "def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "# ---------------- Train base CNN ----------------\n",
        "def train_base():\n",
        "    set_seed(SEED)\n",
        "    m = SmallCNN().to(DEVICE)\n",
        "    opt = optim.AdamW(m.parameters(), lr=LR_BASE, weight_decay=WD)\n",
        "    for ep in range(1, EPOCHS_BASE+1):\n",
        "        m.train(); n=0; c=0; loss_sum=0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            logits = m(xb)\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "            loss.backward(); opt.step()\n",
        "            n += xb.size(0); c += (logits.argmax(1)==yb).sum().item(); loss_sum += loss.item()*xb.size(0)\n",
        "        vl, va = eval_model(m, test_loader)\n",
        "        print(f\"[Base ep {ep:02d}] train_acc={c/n*100:5.2f}% | val_acc={va*100:5.2f}%\")\n",
        "    return m\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader):\n",
        "    model.eval(); n=0; c=0; loss_sum=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = model(xb)\n",
        "        loss_sum += F.cross_entropy(logits, yb).item()*xb.size(0)\n",
        "        c += (logits.argmax(1)==yb).sum().item(); n += xb.size(0)\n",
        "    return loss_sum/max(n,1), c/max(n,1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings(extractor, loader):\n",
        "    embs=[]; ys=[]\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        e = extractor(xb)\n",
        "        embs.append(e.cpu().numpy()); ys.append(yb.numpy())\n",
        "    return np.concatenate(embs,0), np.concatenate(ys,0)\n",
        "\n",
        "def estimate_delta_hyperbolicity_numpy(X, n_samples=2000):\n",
        "    n = X.shape[0]\n",
        "    idx = np.random.choice(n, size=(n_samples,4), replace=True)\n",
        "    D = pairwise_distances(X, metric=\"euclidean\")\n",
        "    deltas=[]\n",
        "    for a,b,c,d in idx:\n",
        "        s = sorted([D[a,b]+D[c,d], D[a,c]+D[b,d], D[a,d]+D[b,c]])\n",
        "        deltas.append(0.5*(s[2]-s[1]))\n",
        "    return float(np.median(deltas))\n",
        "\n",
        "# Differentiable δ-loss (per batch)\n",
        "def batch_delta_loss(emb, n_quads=DELTA_QUADS_PER_BATCH):\n",
        "    \"\"\"\n",
        "    emb: [B, d] torch tensor\n",
        "    returns: mean δ over sampled quadruples (differentiable)\n",
        "    \"\"\"\n",
        "    B = emb.size(0)\n",
        "    if B < 4:\n",
        "        return torch.tensor(0.0, device=emb.device)\n",
        "    idx = torch.randint(0, B, (n_quads, 4), device=emb.device)\n",
        "    a,b,c,d = idx[:,0], idx[:,1], idx[:,2], idx[:,3]\n",
        "    def pdist(u, v):\n",
        "        return torch.norm(u - v, dim=-1)  # Euclidean\n",
        "    dab = pdist(emb[a], emb[b])\n",
        "    dac = pdist(emb[a], emb[c])\n",
        "    dad = pdist(emb[a], emb[d])\n",
        "    dbc = pdist(emb[b], emb[c])\n",
        "    dbd = pdist(emb[b], emb[d])\n",
        "    dcd = pdist(emb[c], emb[d])\n",
        "    s1 = dab + dcd\n",
        "    s2 = dac + dbd\n",
        "    s3 = dad + dbc\n",
        "    # δ = 0.5*(largest - middle)\n",
        "    stacked = torch.stack([s1, s2, s3], dim=1)\n",
        "    top2, _ = torch.topk(stacked, k=2, dim=1)    # [n_quads, 2], sorted descending\n",
        "    delta = 0.5 * (top2[:,0] - top2[:,1])\n",
        "    return delta.mean()\n",
        "\n",
        "# ---------------- Sidecar training ----------------\n",
        "class SidecarHead(nn.Module):\n",
        "    \"\"\"Frozen extractor -> Sidecar -> fresh head (copy of base head)\"\"\"\n",
        "    def __init__(self, frozen_extractor, base_head, d=EMB_DIM, h=128):\n",
        "        super().__init__()\n",
        "        self.extractor = frozen_extractor\n",
        "        self.sidecar = Sidecar(d=d, h=h)\n",
        "        self.head = nn.Linear(d, 10)\n",
        "        # initialize head from base to start near the original decision boundary\n",
        "        self.head.load_state_dict(base_head.state_dict())\n",
        "    def forward(self, x, return_emb=False):\n",
        "        e = self.extractor(x)             # no grad\n",
        "        e2 = self.sidecar(e)              # bend geometry\n",
        "        logits = self.head(F.gelu(e2))\n",
        "        return (e2, logits) if return_emb else logits\n",
        "\n",
        "def train_sidecar(frozen_extractor, base_head):\n",
        "    model = SidecarHead(frozen_extractor, base_head).to(DEVICE)\n",
        "    params = list(model.sidecar.parameters()) + list(model.head.parameters())\n",
        "    opt = optim.AdamW(params, lr=LR_SIDECAR, weight_decay=WD)\n",
        "    for ep in range(1, EPOCHS_SIDECAR+1):\n",
        "        model.train(); n=0; c=0; ce_sum=0.0; dl_sum=0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            e2, logits = model(xb, return_emb=True)\n",
        "            ce = F.cross_entropy(logits, yb, label_smoothing=LABEL_SMOOTH)\n",
        "            dl = batch_delta_loss(e2, n_quads=DELTA_QUADS_PER_BATCH)\n",
        "            loss = ce + LAMBDA_GEOM * dl\n",
        "            loss.backward(); opt.step()\n",
        "            n += xb.size(0); c += (logits.argmax(1)==yb).sum().item()\n",
        "            ce_sum += ce.item()*xb.size(0); dl_sum += dl.item()*xb.size(0)\n",
        "        vl, va = eval_model(model, test_loader)\n",
        "        print(f\"[Sidecar ep {ep:02d}] train_acc={c/n*100:5.2f}% | val_acc={va*100:5.2f}% | mean δ-loss(batch)={dl_sum/n:.4f}\")\n",
        "    return model\n",
        "\n",
        "# ---------------- Run: train base, evaluate δ/ID/OOD, then bend and re-evaluate ----------------\n",
        "base = train_base()\n",
        "\n",
        "# Baseline metrics\n",
        "base_id = eval_model(base, test_loader)[1]\n",
        "base_rot = eval_model(base, test_rot_loader)[1]\n",
        "base_el  = eval_model(base, test_elastic_loader)[1]\n",
        "with torch.no_grad():\n",
        "    extractor = FrozenExtractor(base).to(DEVICE)\n",
        "E_base, _ = collect_embeddings(extractor, test_loader)\n",
        "delta_base = estimate_delta_hyperbolicity_numpy(E_base, n_samples=3000)\n",
        "\n",
        "print(\"\\nBaseline:\")\n",
        "print(f\"ID acc={base_id*100:.2f}% | OOD(rot)={base_rot*100:.2f}% | OOD(elastic)={base_el*100:.2f}% | δ≈{delta_base:.4f}\")\n",
        "\n",
        "# Train sidecar (freeze base)\n",
        "frozen_extractor = FrozenExtractor(base).to(DEVICE)\n",
        "sidecar_model = train_sidecar(frozen_extractor, base.fc_out)\n",
        "\n",
        "# Sidecar metrics\n",
        "sc_id = eval_model(sidecar_model, test_loader)[1]\n",
        "sc_rot = eval_model(sidecar_model, test_rot_loader)[1]\n",
        "sc_el  = eval_model(sidecar_model, test_elastic_loader)[1]\n",
        "with torch.no_grad():\n",
        "    # collect sidecar embeddings by forwarding and grabbing e'\n",
        "    embs=[];\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        e2, _ = sidecar_model(xb, return_emb=True)\n",
        "        embs.append(e2.cpu().numpy())\n",
        "    E_sc = np.concatenate(embs, 0)\n",
        "delta_sc = estimate_delta_hyperbolicity_numpy(E_sc, n_samples=3000)\n",
        "\n",
        "print(\"\\nAfter Sidecar Bending:\")\n",
        "print(f\"ID acc={sc_id*100:.2f}% | OOD(rot)={sc_rot*100:.2f}% | OOD(elastic)={sc_el*100:.2f}% | δ≈{delta_sc:.4f}\")\n",
        "\n",
        "# Report deltas\n",
        "def pct(x): return f\"{x:+.2f} pts\"\n",
        "print(\"\\nΔ (sidecar - baseline):\")\n",
        "print(f\"ID acc  {pct((sc_id - base_id)*100)}\")\n",
        "print(f\"OOD rot {pct((sc_rot - base_rot)*100)}\")\n",
        "print(f\"OOD el  {pct((sc_el  - base_el )*100)}\")\n",
        "print(f\"δ change {delta_sc - delta_base:+.4f} (negative = more tree-like)\")\n",
        "\n",
        "# JSON summary\n",
        "summary = {\n",
        "  \"baseline\": {\"id\": float(base_id), \"ood_rot\": float(base_rot), \"ood_elastic\": float(base_el), \"delta\": float(delta_base)},\n",
        "  \"sidecar\":  {\"id\": float(sc_id),   \"ood_rot\": float(sc_rot),  \"ood_elastic\": float(sc_el),  \"delta\": float(delta_sc)},\n",
        "  \"hyperparams\": {\"lambda_geom\": LAMBDA_GEOM, \"epochs_sidecar\": EPOCHS_SIDECAR, \"quads_per_batch\": DELTA_QUADS_PER_BATCH}\n",
        "}\n",
        "print(\"\\nJSON summary:\")\n",
        "print(json.dumps(summary, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdwA4MFdr9IQ",
        "outputId": "fa831f2b-0fb8-4e51-af7b-8efede1ddca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Base ep 01] train_acc=81.21% | val_acc=87.58%\n",
            "[Base ep 02] train_acc=89.14% | val_acc=88.97%\n",
            "[Base ep 03] train_acc=90.91% | val_acc=90.04%\n",
            "[Base ep 04] train_acc=92.07% | val_acc=91.40%\n",
            "[Base ep 05] train_acc=92.92% | val_acc=91.47%\n",
            "[Base ep 06] train_acc=93.58% | val_acc=91.85%\n",
            "[Base ep 07] train_acc=94.34% | val_acc=91.17%\n",
            "[Base ep 08] train_acc=94.91% | val_acc=91.99%\n",
            "[Base ep 09] train_acc=95.27% | val_acc=92.16%\n",
            "[Base ep 10] train_acc=96.00% | val_acc=91.91%\n",
            "\n",
            "Baseline:\n",
            "ID acc=91.91% | OOD(rot)=32.06% | OOD(elastic)=85.36% | δ≈1.8931\n",
            "[Sidecar ep 01] train_acc=96.60% | val_acc=92.08% | mean δ-loss(batch)=1.7042\n",
            "[Sidecar ep 02] train_acc=96.78% | val_acc=92.12% | mean δ-loss(batch)=0.9427\n",
            "[Sidecar ep 03] train_acc=96.96% | val_acc=92.12% | mean δ-loss(batch)=0.7558\n",
            "[Sidecar ep 04] train_acc=96.99% | val_acc=92.42% | mean δ-loss(batch)=0.6944\n",
            "[Sidecar ep 05] train_acc=97.10% | val_acc=92.46% | mean δ-loss(batch)=0.6476\n",
            "[Sidecar ep 06] train_acc=97.19% | val_acc=92.06% | mean δ-loss(batch)=0.6294\n",
            "\n",
            "After Sidecar Bending:\n",
            "ID acc=92.06% | OOD(rot)=32.88% | OOD(elastic)=85.62% | δ≈0.4203\n",
            "\n",
            "Δ (sidecar - baseline):\n",
            "ID acc  +0.15 pts\n",
            "OOD rot +0.82 pts\n",
            "OOD el  +0.26 pts\n",
            "δ change -1.4728 (negative = more tree-like)\n",
            "\n",
            "JSON summary:\n",
            "{\n",
            "  \"baseline\": {\n",
            "    \"id\": 0.9191,\n",
            "    \"ood_rot\": 0.3206,\n",
            "    \"ood_elastic\": 0.8536,\n",
            "    \"delta\": 1.893143653869629\n",
            "  },\n",
            "  \"sidecar\": {\n",
            "    \"id\": 0.9206,\n",
            "    \"ood_rot\": 0.3288,\n",
            "    \"ood_elastic\": 0.8562,\n",
            "    \"delta\": 0.4203071594238281\n",
            "  },\n",
            "  \"hyperparams\": {\n",
            "    \"lambda_geom\": 0.05,\n",
            "    \"epochs_sidecar\": 6,\n",
            "    \"quads_per_batch\": 256\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Controls for Experiment 5: ablations (λ=0 sidecar; head-only), plus CKA/Procrustes vs base\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# This cell assumes the following from Experiment 5 are already defined in your notebook:\n",
        "# - DEVICE, train_loader, test_loader, test_rot_loader, test_elastic_loader\n",
        "# - FrozenExtractor, SidecarHead, batch_delta_loss\n",
        "# - base  (trained baseline CNN)\n",
        "# - sidecar_model  (geometry-bent model you just trained)\n",
        "# - LABEL_SMOOTH, EPOCHS_SIDECAR, LR_SIDECAR, WD\n",
        "# If any are missing, please run the Experiment 5 cell first.\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "@torch.no_grad()\n",
        "def eval_model(model, loader, device=DEVICE):\n",
        "    model.eval(); n=0; c=0; loss_sum=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss_sum += F.cross_entropy(logits, yb).item() * xb.size(0)\n",
        "        c += (logits.argmax(1) == yb).sum().item()\n",
        "        n += xb.size(0)\n",
        "    return loss_sum / max(n,1), c / max(n,1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings(extractor, loader, device=DEVICE):\n",
        "    embs, ys = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        e = extractor(xb)\n",
        "        embs.append(e.detach().cpu().numpy()); ys.append(yb.numpy())\n",
        "    return np.concatenate(embs, 0), np.concatenate(ys, 0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_embeddings_from_logits_model(model, loader, device=DEVICE):\n",
        "    \"\"\"For SidecarHead-like models: returns post-sidecar embeddings e' via return_emb=True.\"\"\"\n",
        "    embs = []\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(device)\n",
        "        e2, _ = model(xb, return_emb=True)\n",
        "        embs.append(e2.detach().cpu().numpy())\n",
        "    return np.concatenate(embs, 0)\n",
        "\n",
        "def estimate_delta_hyperbolicity_numpy(X, n_samples=3000):\n",
        "    n = X.shape[0]\n",
        "    idx = np.random.choice(n, size=(n_samples, 4), replace=True)\n",
        "    D = pairwise_distances(X)\n",
        "    deltas = []\n",
        "    for a,b,c,d in idx:\n",
        "        s = sorted([D[a,b] + D[c,d], D[a,c] + D[b,d], D[a,d] + D[b,c]])\n",
        "        deltas.append(0.5 * (s[2] - s[1]))\n",
        "    return float(np.median(deltas))\n",
        "\n",
        "def center_gram(K):\n",
        "    n = K.shape[0]; H = np.eye(n) - np.ones((n,n))/n\n",
        "    return H @ K @ H\n",
        "\n",
        "def cka(X, Y):\n",
        "    Kx = X @ X.T; Ky = Y @ Y.T\n",
        "    Kx_c = center_gram(Kx); Ky_c = center_gram(Ky)\n",
        "    hsic = np.sum(Kx_c * Ky_c)\n",
        "    var1 = np.sqrt(np.sum(Kx_c*Kx_c)); var2 = np.sqrt(np.sum(Ky_c*Ky_c))\n",
        "    return float(hsic / (var1*var2 + 1e-12))\n",
        "\n",
        "def procrustes_residual(X, Y):\n",
        "    Xc = X - X.mean(0, keepdims=True)\n",
        "    Yc = Y - Y.mean(0, keepdims=True)\n",
        "    U, S, Vt = np.linalg.svd(Xc.T @ Yc, full_matrices=False)\n",
        "    R = U @ Vt\n",
        "    num = np.linalg.norm(Xc @ R - Yc, ord='fro')\n",
        "    den = np.linalg.norm(Yc, ord='fro') + 1e-12\n",
        "    return float(num / den)\n",
        "\n",
        "# ---------- Controls ----------\n",
        "def train_sidecar_variant(frozen_extractor, base_head, lambda_geom=0.0, freeze_alpha=False, epochs=None):\n",
        "    \"\"\"\n",
        "    lambda_geom=0.0 -> capacity-matched control (no geometry term)\n",
        "    freeze_alpha=True -> head-only control (sidecar disabled)\n",
        "    \"\"\"\n",
        "    if epochs is None:\n",
        "        epochs = EPOCHS_SIDECAR  # from Experiment 5 cell\n",
        "\n",
        "    model = SidecarHead(frozen_extractor, base_head).to(DEVICE)\n",
        "\n",
        "    if freeze_alpha:\n",
        "        # Disable sidecar by zeroing α and freezing all sidecar params\n",
        "        with torch.no_grad():\n",
        "            model.sidecar.alpha.data.zero_()\n",
        "        for p in model.sidecar.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    opt = torch.optim.AdamW(params, lr=LR_SIDECAR, weight_decay=WD)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            e2, logits = model(xb, return_emb=True)\n",
        "            ce = F.cross_entropy(logits, yb, label_smoothing=LABEL_SMOOTH)\n",
        "            loss = ce\n",
        "            if lambda_geom > 0.0:\n",
        "                # Only used for a geometry-regularized variant; here our controls use 0.0\n",
        "                loss = loss + lambda_geom * batch_delta_loss(e2, n_quads=256)\n",
        "            loss.backward(); opt.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# ---------- Run ablations + comparisons ----------\n",
        "# Ensure base & sidecar_model exist\n",
        "try:\n",
        "    base\n",
        "    sidecar_model\n",
        "except NameError as e:\n",
        "    raise RuntimeError(\"Please run Experiment 5 first to define 'base' and 'sidecar_model'.\") from e\n",
        "\n",
        "# Build frozen extractor from the trained base\n",
        "frozen_extractor = FrozenExtractor(base).to(DEVICE)\n",
        "\n",
        "# Baseline metrics + embeddings (unpack to avoid tuple error)\n",
        "base_id = eval_model(base, test_loader)[1]\n",
        "base_rot = eval_model(base, test_rot_loader)[1]\n",
        "base_el  = eval_model(base, test_elastic_loader)[1]\n",
        "E_base, y_base = collect_embeddings(frozen_extractor, test_loader)\n",
        "\n",
        "# Control A: capacity-matched sidecar (λ=0.0, sidecar trainable but no geometry loss)\n",
        "ctrlA = train_sidecar_variant(frozen_extractor, base.fc_out, lambda_geom=0.0, freeze_alpha=False, epochs=EPOCHS_SIDECAR)\n",
        "ctrlA_id = eval_model(ctrlA, test_loader)[1]\n",
        "ctrlA_rot = eval_model(ctrlA, test_rot_loader)[1]\n",
        "ctrlA_el  = eval_model(ctrlA, test_elastic_loader)[1]\n",
        "E_ctrlA = collect_embeddings_from_logits_model(ctrlA, test_loader)\n",
        "delta_ctrlA = estimate_delta_hyperbolicity_numpy(E_ctrlA)\n",
        "\n",
        "# Control B: head-only (freeze sidecar; α=0)\n",
        "ctrlB = train_sidecar_variant(frozen_extractor, base.fc_out, lambda_geom=0.0, freeze_alpha=True, epochs=EPOCHS_SIDECAR)\n",
        "ctrlB_id = eval_model(ctrlB, test_loader)[1]\n",
        "ctrlB_rot = eval_model(ctrlB, test_rot_loader)[1]\n",
        "ctrlB_el  = eval_model(ctrlB, test_elastic_loader)[1]\n",
        "E_ctrlB = collect_embeddings_from_logits_model(ctrlB, test_loader)\n",
        "delta_ctrlB = estimate_delta_hyperbolicity_numpy(E_ctrlB)\n",
        "\n",
        "# Geometry-bent model from Experiment 5 (already trained with geometry loss)\n",
        "E_sidecar = collect_embeddings_from_logits_model(sidecar_model, test_loader)\n",
        "delta_sidecar = estimate_delta_hyperbolicity_numpy(E_sidecar)\n",
        "\n",
        "# Similarity to base (did geometry actually move?)\n",
        "cka_ctrlA  = cka(E_base, E_ctrlA);   proc_ctrlA  = procrustes_residual(E_base, E_ctrlA)\n",
        "cka_ctrlB  = cka(E_base, E_ctrlB);   proc_ctrlB  = procrustes_residual(E_base, E_ctrlB)\n",
        "cka_geom   = cka(E_base, E_sidecar); proc_geom   = procrustes_residual(E_base, E_sidecar)\n",
        "\n",
        "summary = {\n",
        "  \"baseline\": {\"id\": float(base_id), \"rot\": float(base_rot), \"el\": float(base_el)},\n",
        "  \"control_lambda0\": {\n",
        "    \"id\": float(ctrlA_id), \"rot\": float(ctrlA_rot), \"el\": float(ctrlA_el),\n",
        "    \"delta\": float(delta_ctrlA),\n",
        "    \"cka_to_base\": float(cka_ctrlA), \"procrustes_to_base\": float(proc_ctrlA)\n",
        "  },\n",
        "  \"control_head_only\": {\n",
        "    \"id\": float(ctrlB_id), \"rot\": float(ctrlB_rot), \"el\": float(ctrlB_el),\n",
        "    \"delta\": float(delta_ctrlB),\n",
        "    \"cka_to_base\": float(cka_ctrlB), \"procrustes_to_base\": float(proc_ctrlB)\n",
        "  },\n",
        "  \"geometry_bent\": {\n",
        "    \"id\": float(eval_model(sidecar_model, test_loader)[1]),\n",
        "    \"rot\": float(eval_model(sidecar_model, test_rot_loader)[1]),\n",
        "    \"el\": float(eval_model(sidecar_model, test_elastic_loader)[1]),\n",
        "    \"delta\": float(delta_sidecar),\n",
        "    \"cka_to_base\": float(cka_geom), \"procrustes_to_base\": float(proc_geom)\n",
        "  },\n",
        "  \"hyperparams\": {\"epochs_sidecar\": int(EPOCHS_SIDECAR), \"lambda_geom\": float(LABEL_SMOOTH*0.0)}  # λ for controls is 0.0\n",
        "}\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNQ0dERZu2ur",
        "outputId": "6dcfddf1-6d27-4219-8444-a6e65a6245e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"baseline\": {\n",
            "    \"id\": 0.9191,\n",
            "    \"rot\": 0.3206,\n",
            "    \"el\": 0.8538\n",
            "  },\n",
            "  \"control_lambda0\": {\n",
            "    \"id\": 0.9251,\n",
            "    \"rot\": 0.3118,\n",
            "    \"el\": 0.8655,\n",
            "    \"delta\": 2.2775497436523438,\n",
            "    \"cka_to_base\": 0.834693576275763,\n",
            "    \"procrustes_to_base\": 0.43296971917152405\n",
            "  },\n",
            "  \"control_head_only\": {\n",
            "    \"id\": 0.9212,\n",
            "    \"rot\": 0.3353,\n",
            "    \"el\": 0.854,\n",
            "    \"delta\": 1.8793773651123047,\n",
            "    \"cka_to_base\": 1.0,\n",
            "    \"procrustes_to_base\": 1.2924449777074187e-07\n",
            "  },\n",
            "  \"geometry_bent\": {\n",
            "    \"id\": 0.9206,\n",
            "    \"rot\": 0.3288,\n",
            "    \"el\": 0.8583,\n",
            "    \"delta\": 0.4317750930786133,\n",
            "    \"cka_to_base\": 0.6916181266454534,\n",
            "    \"procrustes_to_base\": 0.9119651317596436\n",
            "  },\n",
            "  \"hyperparams\": {\n",
            "    \"epochs_sidecar\": 6,\n",
            "    \"lambda_geom\": 0.0\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}