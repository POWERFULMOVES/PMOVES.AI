{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SANL — Spiral-Atlas Number Learner (Atlas Number Learning Model)\n",
        "\n",
        "This notebook trains a neural model to learn **Dirichlet-character-like waves** over the\n",
        "multiplicative group \\((\\mathbb{Z}/p\\mathbb{Z})^\\times\\) using a **spiral modular atlas**.\n",
        "\n",
        "Core ideas:\n",
        "\n",
        "- Integers mod `p` are embedded as **harmonic points on a spiral/cylinder**.\n",
        "- The network learns complex-valued “characters” \\(\\chi(n) \\in S^1\\) such that:\n",
        "  - **Multiplicativity:**  \\(\\chi(ab) \\approx \\chi(a)\\chi(b)\\)\n",
        "  - **Scale equivariance:** \\(\\chi(sn) \\approx \\chi(n)\\) for \\(s\\) coprime to \\(p\\)\n",
        "  - **Orthogonality:** different heads behave like distinct Dirichlet characters.\n",
        "\n",
        "This is the **Atlas Number Learning** model: the SANL network operates directly inside\n",
        "a number-theoretic geometry where arithmetic becomes wave mechanics instead of\n",
        "symbolic manipulation."
      ],
      "metadata": {
        "id": "zLkJ0LErM1yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SANL — Spiral-Atlas Number Learner\n",
        "# Dependencies: torch>=2.1, numpy\n",
        "\n",
        "import math, time, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ===========================\n",
        "# Config\n",
        "# ===========================\n",
        "class Cfg:\n",
        "    device         = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    p              = 97                    # prime modulus\n",
        "    K_harm         = 4                     # harmonics per modulus (cos/sin pairs)\n",
        "    M_mods         = [3,5,7,11,13,17,19,23,\n",
        "                      29,31,37,41,43,47,53,59,\n",
        "                      61,67,71,73,79,83,89,97]  # includes p\n",
        "    use_only_p     = True                  # True = use only modulus p as atlas (fast)\n",
        "    dims_hidden    = 256                   # MLP hidden size\n",
        "    n_chars        = 6                     # number of character heads\n",
        "    epochs         = 120\n",
        "    batch_pairs    = 8192                  # (a,b) pairs per step\n",
        "    batch_scale    = 4096                  # (n,s) pairs for scale-invariance\n",
        "    lr             = 3e-3\n",
        "    wd             = 1e-4\n",
        "    grad_accum     = 2\n",
        "    amp            = True                  # mixed precision on CUDA\n",
        "    cosine_lr      = True\n",
        "    torch_compile  = False                 # set True on PyTorch 2.x for extra speed\n",
        "    seed           = 42\n",
        "\n",
        "    # Loss weights\n",
        "    w_mult         = 1.0       # χ(ab) ≈ χ(a)χ(b)\n",
        "    w_scale        = 0.5       # χ(sn) ≈ χ(n) for coprime s\n",
        "    w_unit         = 1e-3      # |χ| ≈ 1\n",
        "    w_ortho        = 1e-2      # characters ≈ orthogonal\n",
        "    w_anchor       = 5e-3      # fix global phase drift via residue 1\n",
        "\n",
        "    # Supervision mode (for later, if you want analytic χ supervision)\n",
        "    supervision    = \"self\"    # \"self\" or \"dlog\"\n",
        "    dlog_r_index   = 1         # which discrete-log character to anchor if using \"dlog\"\n",
        "\n",
        "cfg = Cfg()\n",
        "\n",
        "# Repro\n",
        "random.seed(cfg.seed)\n",
        "np.random.seed(cfg.seed)\n",
        "torch.manual_seed(cfg.seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHgp4xVXMZQo",
        "outputId": "7bb2e59f-7a77-46ac-e1da-073836419ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4b113a9010>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Number-theory helpers\n",
        "# ===========================\n",
        "\n",
        "def is_coprime(a: int, b: int) -> bool:\n",
        "    return math.gcd(a, b) == 1\n",
        "\n",
        "def primitive_root(p: int) -> int:\n",
        "    \"\"\"Primitive root mod prime p.\"\"\"\n",
        "    assert p > 2 and isinstance(p, int)\n",
        "    phi = p - 1\n",
        "    fac, m, d = set(), phi, 2\n",
        "    while d * d <= m:\n",
        "        while m % d == 0:\n",
        "            fac.add(d)\n",
        "            m //= d\n",
        "        d += 1\n",
        "    if m > 1:\n",
        "        fac.add(m)\n",
        "    for g in range(2, p):\n",
        "        for q in fac:\n",
        "            if pow(g, phi // q, p) == 1:\n",
        "                break\n",
        "        else:\n",
        "            return g\n",
        "    raise RuntimeError(\"No primitive root?\")\n",
        "\n",
        "def dlog_table(p: int, g: int = None):\n",
        "    \"\"\"Discrete log table k such that g^k ≡ n (mod p) for all n != 0.\"\"\"\n",
        "    if g is None:\n",
        "        g = primitive_root(p)\n",
        "    tab = {1: 0}\n",
        "    x = 1\n",
        "    for k in range(1, p-1):\n",
        "        x = (x * g) % p\n",
        "        tab[x] = k\n",
        "    return g, tab\n",
        "\n",
        "def dl_character_factory(p: int, r: int, g: int = None, tab=None):\n",
        "    \"\"\"\n",
        "    Analytic Dirichlet-like character via discrete log:\n",
        "    χ_r(g^k) = exp(2πi r k / (p-1)).\n",
        "    \"\"\"\n",
        "    g, tab = dlog_table(p, g)\n",
        "    tw = 2*math.pi*r/(p-1)\n",
        "\n",
        "    def chi(n: int):\n",
        "        nm = n % p\n",
        "        if nm == 0:\n",
        "            return complex(0.0, 0.0)\n",
        "        k = tab[nm]\n",
        "        return complex(math.cos(tw*k), math.sin(tw*k))\n",
        "\n",
        "    return chi"
      ],
      "metadata": {
        "id": "T9xlTZc5Mbu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Spiral Atlas\n",
        "# ===========================\n",
        "\n",
        "class SpiralAtlas:\n",
        "    \"\"\"\n",
        "    Spiral/harmonic embedding over moduli.\n",
        "    If use_only_p=True, we only use modulus p as the atlas (fast + clean).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p, mods, K, use_only_p, device):\n",
        "        if use_only_p:\n",
        "            mods = [p]\n",
        "        self.mods = mods\n",
        "        self.p = p\n",
        "        self.K = K\n",
        "        self.device = device\n",
        "\n",
        "        # Precompute harmonic banks for each modulus\n",
        "        self.blocks = []\n",
        "        for m in mods:\n",
        "            r = torch.arange(1, m, device=device).float()   # residues 1..m-1\n",
        "            theta = 2*math.pi*r/m                           # base phase\n",
        "            ks = torch.arange(1, K+1, device=device).float().view(1, -1)\n",
        "            TH = theta.view(-1, 1) * ks                     # [m-1, K]\n",
        "            H = torch.cat([torch.cos(TH), torch.sin(TH)], dim=1)  # [m-1, 2K]\n",
        "            self.blocks.append((m, H))\n",
        "\n",
        "        # For dlog-based supervision (analytic characters)\n",
        "        self.g = primitive_root(p)\n",
        "        g, tab = dlog_table(p, self.g)\n",
        "        self.residue_to_k = torch.full((p,), -1, device=device, dtype=torch.long)\n",
        "        for res, k in tab.items():\n",
        "            self.residue_to_k[res] = k\n",
        "\n",
        "    def features(self, n: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        n: [B] (ints) on device\n",
        "        Returns: [B, D] harmonic feature vectors across all atlas moduli.\n",
        "        \"\"\"\n",
        "        # We only really need mod p in the current config, but code is generic\n",
        "        n = torch.remainder(n, self.mods[-1])\n",
        "        feats = []\n",
        "        for (m, H) in self.blocks:\n",
        "            nm = torch.remainder(n, m)      # [B]\n",
        "            mask = nm != 0\n",
        "            idx  = nm[mask] - 1             # residues 1..m-1 map to 0..m-2\n",
        "            block = torch.zeros((n.shape[0], H.shape[1]), device=self.device)\n",
        "            if mask.any():\n",
        "                block[mask] = H[idx]\n",
        "            feats.append(block)\n",
        "        return torch.cat(feats, dim=1)"
      ],
      "metadata": {
        "id": "3TjLeK5VMejI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Model & helpers\n",
        "# ===========================\n",
        "\n",
        "class SANL(nn.Module):\n",
        "    \"\"\"\n",
        "    Spiral-Atlas Number Learner:\n",
        "    Takes harmonic features and outputs n_chars complex heads χ_j(n) in ℂ.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim: int, hidden: int, n_chars: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, 2*n_chars)  # (Re,Im) per head\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: [B, D]  ->  out: [B, H, 2], where H=n_chars, last dim is (Re, Im)\n",
        "        \"\"\"\n",
        "        out = self.net(x)\n",
        "        return out.view(x.shape[0], -1, 2)\n",
        "\n",
        "def unit_project(z: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Project complex vectors z[...,2] onto the unit circle.\"\"\"\n",
        "    return z / torch.clamp(torch.norm(z, dim=-1, keepdim=True), min=1e-8)\n",
        "\n",
        "def complex_mul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Complex multiply (a⊗b) in R^2 representation: [...,2].\"\"\"\n",
        "    return torch.stack([\n",
        "        a[..., 0]*b[..., 0] - a[..., 1]*b[..., 1],\n",
        "        a[..., 0]*b[..., 1] + a[..., 1]*b[..., 0]\n",
        "    ], dim=-1)\n",
        "\n",
        "def complex_conj(a: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Complex conjugate.\"\"\"\n",
        "    return torch.stack([a[..., 0], -a[..., 1]], dim=-1)"
      ],
      "metadata": {
        "id": "iOD3hAvrMhgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Losses\n",
        "# ===========================\n",
        "\n",
        "def multiplicativity_loss(ha, hb, hab):\n",
        "    \"\"\"\n",
        "    ha, hb, hab: [B, H, 2]\n",
        "    Enforce χ(ab) ≈ χ(a)χ(b).\n",
        "    \"\"\"\n",
        "    return F.mse_loss(complex_mul(ha, hb), hab)\n",
        "\n",
        "def scale_loss(hn, hsn):\n",
        "    \"\"\"\n",
        "    hn, hsn: [B, H, 2]\n",
        "    Enforce χ(sn) ≈ χ(n) for random coprime s.\n",
        "    \"\"\"\n",
        "    return F.mse_loss(hn, hsn)\n",
        "\n",
        "def unit_penalty(head):\n",
        "    \"\"\"\n",
        "    head: [B, H, 2]\n",
        "    Encourage |χ| ≈ 1.\n",
        "    \"\"\"\n",
        "    mag = torch.norm(head, dim=-1)\n",
        "    return ((mag - 1.0)**2).mean()\n",
        "\n",
        "def ortho_penalty_over_group(model, atlas, p: int, sample: int = 2048):\n",
        "    \"\"\"\n",
        "    Encourage heads to behave like orthogonal characters on the group (Z/pZ)^×.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x = torch.randint(1, p, (sample,), device=cfg.device)  # units\n",
        "    Fbank = atlas.features(x)                       # [S, D]\n",
        "    out = unit_project(model(Fbank))               # [S, H, 2]\n",
        "    a = out[..., 0] - 1j*out[..., 1]              # complex form: [S, H]\n",
        "    G = (a.unsqueeze(2)*a.unsqueeze(1).conj()).mean(0)  # [H, H] Gram\n",
        "    Id = torch.eye(out.shape[1], device=cfg.device)\n",
        "    return (G.real - Id).abs().mean()\n",
        "\n",
        "def anchor_phase_loss(model, atlas, p: int, residue: int = 1):\n",
        "    \"\"\"\n",
        "    Fix mean phase at residue=1 to [1,0] in R^2, to break global phase drift.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x = torch.full((256,), residue, device=cfg.device)\n",
        "    Fx = atlas.features(x)\n",
        "    out = unit_project(model(Fx)).mean(0)          # [H,2]\n",
        "    target = torch.zeros_like(out)\n",
        "    target[..., 0] = 1.0\n",
        "    return F.mse_loss(out, target)"
      ],
      "metadata": {
        "id": "Aq6LnPugMkfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Training batch builder\n",
        "# ===========================\n",
        "\n",
        "def build_batches(p: int, n_ab: int, n_ns: int, device: str):\n",
        "    \"\"\"\n",
        "    Build:\n",
        "      - (a,b,ab) samples for multiplicativity\n",
        "      - (n,sn) samples for scale invariance (s coprime to p)\n",
        "    \"\"\"\n",
        "    # (a,b) pairs\n",
        "    a = torch.randint(1, p, (n_ab,), device=device)\n",
        "    b = torch.randint(1, p, (n_ab,), device=device)\n",
        "    ab = (a * b) % p\n",
        "    ab[ab == 0] = 1  # avoid zero residue\n",
        "\n",
        "    # (n,sn) for scale invariance\n",
        "    n = torch.randint(1, p, (n_ns,), device=device)\n",
        "    S = []\n",
        "    while len(S) < n_ns:\n",
        "        s = random.randrange(2, p-1)\n",
        "        if math.gcd(s, p) == 1:\n",
        "            S.append(s)\n",
        "    s = torch.tensor(S, device=device)\n",
        "    sn = (s * n) % p\n",
        "    sn[sn == 0] = 1\n",
        "    return a, b, ab, n, s, sn\n",
        "\n",
        "# ===========================\n",
        "# Model construction & training\n",
        "# ===========================\n",
        "\n",
        "def build():\n",
        "    mods = [cfg.p] if cfg.use_only_p else cfg.M_mods\n",
        "    atlas = SpiralAtlas(cfg.p, mods, cfg.K_harm, cfg.use_only_p, cfg.device)\n",
        "    D = len(mods) * (2 * cfg.K_harm)\n",
        "    model = SANL(D, cfg.dims_hidden, cfg.n_chars).to(cfg.device)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        opt, T_max=cfg.epochs, eta_min=cfg.lr * 0.1\n",
        "    )\n",
        "    return atlas, model, opt, sched\n",
        "\n",
        "def train():\n",
        "    atlas, model, opt, sched = build()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(cfg.amp and cfg.device == \"cuda\"))\n",
        "\n",
        "    for ep in range(1, cfg.epochs + 1):\n",
        "        t0 = time.time()\n",
        "        a, b, ab, n, s, sn = build_batches(\n",
        "            cfg.p, cfg.batch_pairs, cfg.batch_scale, cfg.device\n",
        "        )\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(cfg.amp and cfg.device == \"cuda\")):\n",
        "            Fa, Fb, Fab = atlas.features(a), atlas.features(b), atlas.features(ab)\n",
        "            Fn, Fsn     = atlas.features(n), atlas.features(sn)\n",
        "\n",
        "            ha = unit_project(model(Fa))\n",
        "            hb = unit_project(model(Fb))\n",
        "            hab = unit_project(model(Fab))\n",
        "            hn = unit_project(model(Fn))\n",
        "            hsn = unit_project(model(Fsn))\n",
        "\n",
        "            L_mult  = multiplicativity_loss(ha, hb, hab)\n",
        "            L_scale = scale_loss(hn, hsn)\n",
        "            L_unit  = unit_penalty(torch.cat([ha, hb, hab, hn, hsn], dim=0))\n",
        "            L_ortho = ortho_penalty_over_group(model, atlas, cfg.p, sample=1024)\n",
        "            L_anchor= anchor_phase_loss(model, atlas, cfg.p, residue=1)\n",
        "\n",
        "            L = (cfg.w_mult * L_mult +\n",
        "                 cfg.w_scale * L_scale +\n",
        "                 cfg.w_unit * L_unit +\n",
        "                 cfg.w_ortho * L_ortho +\n",
        "                 cfg.w_anchor * L_anchor)\n",
        "\n",
        "        scaler.scale(L / cfg.grad_accum).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        sched.step()\n",
        "\n",
        "        if ep % 10 == 0 or ep == 1:\n",
        "            print(\n",
        "                f\"epoch {ep:03d} | loss={L.item():.6f} | \"\n",
        "                f\"mult={L_mult.item():.4e} | scale={L_scale.item():.4e} | \"\n",
        "                f\"unit={L_unit.item():.4e} | ortho={L_ortho.item():.4e} | \"\n",
        "                f\"anchor={L_anchor.item():.4e} | dt={time.time()-t0:.2f}s\"\n",
        "            )\n",
        "\n",
        "    print(\"\\nDONE.\")\n",
        "    return atlas, model"
      ],
      "metadata": {
        "id": "W2xaSOyCMnPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take some minutes on Colab GPU, depending on epochs and batch sizes.\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "atlas, model = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzRqraTDMqSO",
        "outputId": "7863c43c-78f1-45cc-a62c-8c9dce0b912b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1915062220.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(cfg.amp and cfg.device == \"cuda\"))\n",
            "/tmp/ipython-input-1915062220.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(cfg.amp and cfg.device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 001 | loss=1.474181 | mult=1.2551e+00 | scale=4.1702e-01 | unit=1.5731e-15 | ortho=2.9069e-01 | anchor=1.5314e+00 | dt=0.60s\n",
            "epoch 010 | loss=0.029065 | mult=1.9592e-02 | scale=2.7009e-03 | unit=1.4801e-15 | ortho=8.0426e-01 | anchor=1.5925e-02 | dt=0.33s\n",
            "epoch 020 | loss=0.011463 | mult=2.6450e-03 | scale=1.0138e-03 | unit=1.5456e-15 | ortho=8.3023e-01 | anchor=1.6837e-03 | dt=0.39s\n",
            "epoch 030 | loss=0.010793 | mult=2.2317e-03 | scale=4.9357e-04 | unit=1.6432e-15 | ortho=8.3028e-01 | anchor=2.3140e-03 | dt=0.38s\n",
            "epoch 040 | loss=0.008866 | mult=4.3645e-04 | scale=1.9859e-04 | unit=1.6821e-15 | ortho=8.3288e-01 | anchor=3.7048e-04 | dt=0.39s\n",
            "epoch 050 | loss=0.008689 | mult=3.0775e-04 | scale=1.0147e-04 | unit=1.6056e-15 | ortho=8.3297e-01 | anchor=1.2500e-04 | dt=0.47s\n",
            "epoch 060 | loss=0.008469 | mult=1.0979e-04 | scale=5.3550e-05 | unit=1.3874e-15 | ortho=8.3325e-01 | anchor=3.8761e-05 | dt=0.33s\n",
            "epoch 070 | loss=0.008405 | mult=5.6395e-05 | scale=3.0260e-05 | unit=1.5394e-15 | ortho=8.3329e-01 | anchor=3.7115e-05 | dt=0.37s\n",
            "epoch 080 | loss=0.008378 | mult=3.4653e-05 | scale=2.0935e-05 | unit=1.4639e-15 | ortho=8.3331e-01 | anchor=2.6278e-05 | dt=0.37s\n",
            "epoch 090 | loss=0.008366 | mult=2.5171e-05 | scale=1.6045e-05 | unit=1.4632e-15 | ortho=8.3332e-01 | anchor=1.7614e-05 | dt=0.32s\n",
            "epoch 100 | loss=0.008361 | mult=2.1046e-05 | scale=1.3320e-05 | unit=1.3361e-15 | ortho=8.3332e-01 | anchor=1.3994e-05 | dt=0.34s\n",
            "epoch 110 | loss=0.008359 | mult=1.8986e-05 | scale=1.2489e-05 | unit=1.4204e-15 | ortho=8.3332e-01 | anchor=1.1783e-05 | dt=0.32s\n",
            "epoch 120 | loss=0.008356 | mult=1.7063e-05 | scale=1.1519e-05 | unit=1.3424e-15 | ortho=8.3332e-01 | anchor=1.0310e-05 | dt=0.38s\n",
            "\n",
            "DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SANL Post-Training Demo ===\n",
        "# What the model can do after training.\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "def to_phase_deg(z: torch.Tensor):\n",
        "    \"\"\"Convert complex vectors [...,2] to phase (degrees).\"\"\"\n",
        "    return torch.rad2deg(torch.atan2(z[..., 1], z[..., 0]))\n",
        "\n",
        "def mult_score(model, atlas, p: int, trials: int = 2000) -> float:\n",
        "    \"\"\"\n",
        "    Quick multiplicativity score:\n",
        "    χ(ab) vs χ(a)χ(b) averaged over random units.\n",
        "    \"\"\"\n",
        "    a = torch.randint(1, p, (trials,), device=cfg.device)\n",
        "    b = torch.randint(1, p, (trials,), device=cfg.device)\n",
        "    ab = (a * b) % p\n",
        "\n",
        "    Fa, Fb, Fab = atlas.features(a), atlas.features(b), atlas.features(ab)\n",
        "    ha = unit_project(model(Fa))   # [T,H,2]\n",
        "    hb = unit_project(model(Fb))   # [T,H,2]\n",
        "    hab = unit_project(model(Fab)) # [T,H,2]\n",
        "\n",
        "    prod = complex_mul(ha, hb)\n",
        "    diff = (prod - hab).pow(2).sum(dim=-1).mean().sqrt()  # RMS vector error\n",
        "    # Map small error to score near 1.0\n",
        "    return float(torch.exp(-10.0 * diff).mean().item())\n",
        "\n",
        "def scale_probe(model, atlas, p: int, scales=(2,3,5,7,11,13,17,19,23), trials: int = 1024):\n",
        "    \"\"\"\n",
        "    For random n and coprime s, measure cosine between χ(n) and χ(sn).\n",
        "    Returns mean cosine per scale.\n",
        "    \"\"\"\n",
        "    n = torch.randint(1, p, (trials,), device=cfg.device)\n",
        "    Fn = atlas.features(n)\n",
        "    hn = unit_project(model(Fn))   # [T,H,2]\n",
        "\n",
        "    cos = []\n",
        "    for s in scales:\n",
        "        ns = (s * n) % p\n",
        "        Fns = atlas.features(ns)\n",
        "        hns = unit_project(model(Fns))\n",
        "        # flatten over heads\n",
        "        flat1 = hn.reshape(trials, -1)\n",
        "        flat2 = hns.reshape(trials, -1)\n",
        "        num = (flat1 * flat2).sum(dim=-1)\n",
        "        den = torch.norm(flat1, dim=-1) * torch.norm(flat2, dim=-1) + 1e-9\n",
        "        c = (num / den).mean().item()\n",
        "        cos.append(c)\n",
        "    return cos\n",
        "\n",
        "print(\"\\n[final] quick multiplicativity score:\",\n",
        "      mult_score(model, atlas, cfg.p, trials=5000))\n",
        "\n",
        "print(\"[final] scale probe (mean cos per s):\")\n",
        "scales = (2,3,5,7,11,13,17,19,23)\n",
        "cos_vals = scale_probe(model, atlas, cfg.p, scales=scales, trials=4096)\n",
        "for s, c in zip(scales, cos_vals):\n",
        "    print(f\"  s={s:2d} -> cos={c:+.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncnaIVqgMsjE",
        "outputId": "cb765f35-be83-4a77-b843-536d30a9358b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[final] quick multiplicativity score: 0.9427040815353394\n",
            "[final] scale probe (mean cos per s):\n",
            "  s= 2 -> cos=+1.000\n",
            "  s= 3 -> cos=+1.000\n",
            "  s= 5 -> cos=+1.000\n",
            "  s= 7 -> cos=+1.000\n",
            "  s=11 -> cos=+1.000\n",
            "  s=13 -> cos=+1.000\n",
            "  s=17 -> cos=+1.000\n",
            "  s=19 -> cos=+1.000\n",
            "  s=23 -> cos=+1.000\n"
          ]
        }
      ]
    }
  ]
}