{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“˜ Topology Capsules v1.0\n",
        "\n",
        "**Overview**\n",
        "This notebook introduces *Topology Capsules*, a graph-based framework for learning and inference without deep training loops. Instead of relying on backpropagation, the method builds *topological capsules* â€” compact spectral structures extracted from graphs of data â€” and uses them for semi-supervised label propagation and transfer.\n",
        "\n",
        "The core idea: rather than treating similarity and dissimilarity symmetrically, *Topology Capsules* explicitly model both **positive affinities** (who is close) and **negative repulsions** (who is far), and reconcile them through signed graph Laplacians. This gives a capsule-like latent space that can be probed with very few labels.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Features in v1.0**\n",
        "\n",
        "* **Multi-view construction**: Each dataset is rotated, jittered, and embedded in multiple metric views. Only neighbors consistent across views are kept, enforcing *mutual consensus*.\n",
        "* **Dual neighborhoods**:\n",
        "\n",
        "  * **KNN(+) positive links** capture local similarity.\n",
        "  * **KFN(âˆ’) negative links** capture structured dissimilarity by finding farthest-neighbors in embedding space.\n",
        "* **Adaptive affinities**:\n",
        "\n",
        "  * *Positive edges*: Zelnikâ€“Manor scaling (local bandwidths).\n",
        "  * *Negative edges*: Robust z-score normalization with smooth sigmoid repulsion.\n",
        "* **Capsule freezing**: The capsule basis (eigenvectors of the positive Laplacian) is frozen; labels only probe it via signed label propagation.\n",
        "* **Seed-only auto-tuning**: Hyperparameters (repulsion weight Î», ridge term) are tuned using only labeled seeds, avoiding leakage from the unlabeled pool.\n",
        "* **Internal transfer**: Capsule posteriors can transfer across held-out slices via lightweight kNN pulls in spectral space.\n",
        "\n",
        "---\n",
        "\n",
        "**Benchmarks Included**\n",
        "\n",
        "1. **Fashion-MNIST (pixels â†’ PCA â†’ Topology Capsule)**\n",
        "   Tests the core CNTC v2 pipeline with consensus neighbors, positive/negative edges, and signed label propagation.\n",
        "2. **CIFAR-10 (HOG + color statistics)**\n",
        "   Builds capsules using fast CPU-friendly features (no CNNs), showing capsule inference with a few labels per class.\n",
        "3. **Cat vs Dog Capsule (multi-view: silhouette, texture, structure, diffusion)**\n",
        "   Demonstrates ensemble capsules with per-view calibration, perturbation-based stability weighting, and geometric-mean fusion.\n",
        "\n",
        "---\n",
        "\n",
        "**Why It Matters**\n",
        "Topology Capsules bridge geometric learning and graph signal processing:\n",
        "\n",
        "* They sidestep costly neural training.\n",
        "* They expose both attractive and repulsive structure in the data.\n",
        "* They work in low-label settings, making them ideal for semi-supervised problems.\n",
        "* They are interpretable: capsules are frozen spectral bases with clear diagnostics (eigenvalues, connectivity, stability).\n",
        "\n",
        "---\n",
        "\n",
        "âš¡ *In short: Topology Capsules v1.0 explores how to capture the â€œshapeâ€ of data through graphs and use that shape directly for learning â€” robust, interpretable, and lightweight.*"
      ],
      "metadata": {
        "id": "xs9o0iLIIm13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# CNTC v2 â€” Stronger Pd via mutual consensus + true local scaling\n",
        "# - Multi-view (random orthonormal rotations + jitter)\n",
        "# - Mutual-consensus KNN(+), robust KFN(âˆ’)\n",
        "# - Zelnikâ€“Manor positive affinities; robust z-score repulsion\n",
        "# - Sparse eigsh; capsule frozen; labels only probe it\n",
        "# - Seed-only auto-tune of (lam_neg, ridge) for signed LP\n",
        "# ===============================================================\n",
        "\n",
        "!pip -q install torchvision scikit-learn\n",
        "\n",
        "import numpy as np, random, math, time\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from scipy.sparse import coo_matrix, csr_matrix, diags, eye\n",
        "from scipy.sparse.linalg import eigsh, spsolve\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# -------------------------------\n",
        "# Config\n",
        "# -------------------------------\n",
        "SEED = 7\n",
        "rng = np.random.default_rng(SEED)\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "N_POINTS   = 6000\n",
        "PCA_DIM    = 64\n",
        "VIEWS      = 4           # more diverse than 3\n",
        "KNN_K      = 16\n",
        "KFN_K      = 8\n",
        "KEEP_KPOS  = 12\n",
        "KEEP_KNEG  = 6\n",
        "CONS_THR   = 2           # â‰¥ this many views\n",
        "RHO_ROT    = 0.20        # rotation mix strength\n",
        "JITTER_STD = 0.01\n",
        "\n",
        "EIGS       = 64\n",
        "LABELED_PER_CLASS = 40\n",
        "TEST_SPLIT = 0.85\n",
        "\n",
        "# grids for signed LP (seed-only tuning)\n",
        "LAM_GRID   = [0.0, 0.05, 0.1, 0.15, 0.2]\n",
        "RIDGE_GRID = [1e-5, 5e-5, 1e-4, 5e-4]\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Data â†’ PCA coords (distances only)\n",
        "# -------------------------------\n",
        "tf = transforms.Compose([transforms.ToTensor()])\n",
        "ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=tf)\n",
        "\n",
        "idx = np.arange(len(ds)); rng.shuffle(idx); idx = idx[:N_POINTS]\n",
        "X = np.stack([ds[i][0].numpy().reshape(-1) for i in idx], 0).astype(np.float32)/255.0\n",
        "y = np.array([ds[i][1] for i in idx], dtype=np.int64)\n",
        "del ds\n",
        "\n",
        "Xp = StandardScaler().fit_transform(X); del X\n",
        "Z0 = PCA(n_components=PCA_DIM, random_state=SEED).fit_transform(Xp).astype(np.float32); del Xp\n",
        "\n",
        "def normalize_rows(A): return A / (np.linalg.norm(A, axis=1, keepdims=True)+1e-9)\n",
        "\n",
        "# Random orthonormal rotation with mix\n",
        "def rotate_view(Z, rho=0.2, jitter=0.01):\n",
        "    m = Z.shape[1]\n",
        "    G = rng.standard_normal((m, m)).astype(np.float32)\n",
        "    Q,_ = np.linalg.qr(G)                          # orthonormal\n",
        "    M = (1.0-rho)*np.eye(m, dtype=np.float32) + rho*Q\n",
        "    Zv = Z @ M\n",
        "    return Zv + rng.normal(0.0, jitter, size=Z.shape).astype(np.float32)\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Multi-view neighbors: KNN(+), KFN(âˆ’) via â€œnearest to âˆ’zâ€\n",
        "# -------------------------------\n",
        "def view_neighbors(Z, k_pos, k_neg):\n",
        "    Zc = normalize_rows(Z)\n",
        "    nn = NearestNeighbors(n_neighbors=k_pos+1, metric='cosine').fit(Zc)\n",
        "    d_pos, i_pos = nn.kneighbors(Zc)               # includes self\n",
        "    i_pos = i_pos[:,1:]; d_pos = d_pos[:,1:]\n",
        "    # farthest: nearest to -Zc\n",
        "    d_neg, i_neg = nn.kneighbors(-Zc, n_neighbors=k_neg)\n",
        "    # note: no self-match here since query = -z\n",
        "    return (i_pos, d_pos), (i_neg, d_neg)\n",
        "\n",
        "views_pos, views_neg = [], []\n",
        "for _ in range(VIEWS):\n",
        "    (ip,dp),(in_,dn_) = view_neighbors(rotate_view(Z0, RHO_ROT, JITTER_STD), KNN_K, KFN_K)\n",
        "    views_pos.append((ip.astype(np.int32), dp.astype(np.float32)))\n",
        "    views_neg.append((in_.astype(np.int32), dn_.astype(np.float32)))\n",
        "\n",
        "N = len(y)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Consensus merge per node with mutuality\n",
        "# -------------------------------\n",
        "def merge_consensus(views, keep_k, mode=\"pos\"):\n",
        "    # Collect edge -> (count, sum_d)\n",
        "    idx_out = np.zeros((N, keep_k), dtype=np.int32)\n",
        "    d_out   = np.zeros((N, keep_k), dtype=np.float32)\n",
        "    c_out   = np.zeros((N, keep_k), dtype=np.int8)\n",
        "    bags = []\n",
        "    for i in range(N):\n",
        "        acc = {}\n",
        "        for (nei,dist) in views:\n",
        "            for j, d in zip(nei[i], dist[i]):\n",
        "                if j==i: continue\n",
        "                if j not in acc: acc[j] = [0, 0.0]\n",
        "                acc[j][0] += 1\n",
        "                acc[j][1] += float(d)\n",
        "        bags.append(acc)\n",
        "\n",
        "    # mutual: keep only i<->j that appear on both sides (after merge)\n",
        "    for i in range(N):\n",
        "        items = []\n",
        "        for j,(cnt,sumd) in bags[i].items():\n",
        "            if j>=N: continue\n",
        "            if i in bags[j]:      # reciprocal at merged level\n",
        "                cnt2,_ = bags[j][i]\n",
        "                # require both sides to meet consensus\n",
        "                if cnt>=CONS_THR and cnt2>=CONS_THR:\n",
        "                    mean_d = sumd/cnt\n",
        "                    items.append((j, cnt, mean_d))\n",
        "        if len(items)<keep_k:\n",
        "            # backfill with highest-count neighbors even if < CONS_THR\n",
        "            for j,(cnt,sumd) in bags[i].items():\n",
        "                if len(items)>=keep_k: break\n",
        "                if all(j!=t[0] for t in items):\n",
        "                    items.append((j, cnt, sumd/max(1,cnt)))\n",
        "        if mode==\"pos\":\n",
        "            items.sort(key=lambda t: (t[1], -t[2]), reverse=True)     # more views, then smaller d\n",
        "        else:\n",
        "            items.sort(key=lambda t: (t[1], t[2]),  reverse=True)     # more views, then larger \"far\" metric (weâ€™ll convert)\n",
        "        items = items[:keep_k]\n",
        "        if len(items)>0:\n",
        "            idx_out[i,:len(items)] = [t[0] for t in items]\n",
        "            d_out[i,:len(items)]   = [t[2] for t in items]\n",
        "            c_out[i,:len(items)]   = [t[1] for t in items]\n",
        "    return idx_out, d_out, c_out\n",
        "\n",
        "# POS: cosine distance d_pos in [0,2]\n",
        "i_pos, d_pos, c_pos = merge_consensus(views_pos, KEEP_KPOS, mode=\"pos\")\n",
        "# NEG: we currently stored d_neg = cosine distance between (-z, j) in [0,2]\n",
        "# Convert to \"far distance\" w.r.t original z: d_far = 2 - d_neg (= 1 - cos(z,j) + 1)\n",
        "# Larger d_far == more dissimilar (max 2 when cos=-1)\n",
        "i_neg_raw, d_neg_raw, c_neg = merge_consensus(views_neg, KEEP_KNEG, mode=\"neg\")\n",
        "d_far = 2.0 - d_neg_raw\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Affinities\n",
        "# 4a) Positive: Zelnikâ€“Manor w_ij = exp(-d_ij^2 / (Ïƒ_i Ïƒ_j)) * (cnt/V)^Î³\n",
        "#     Ïƒ_i = kth neighbor distance (pos) â€” robust local scale\n",
        "# 4b) Negative: robust z-score on d_far, r_ij = sigmoid( (z_i + z_j)/2 ) * (cnt/V)^Î³\n",
        "# -------------------------------\n",
        "gamma = 1.0\n",
        "V = float(VIEWS)\n",
        "\n",
        "# POS local scales\n",
        "sigma_pos = np.sort(d_pos, axis=1)[:, min(KEEP_KPOS-1, max(1, KEEP_KPOS//2))]\n",
        "sigma_pos = sigma_pos.reshape(-1,1) + 1e-6\n",
        "\n",
        "# POS weights (edgewise Ïƒ_i Ïƒ_j with sparse assembly)\n",
        "rows_p = np.repeat(np.arange(N)[:,None], KEEP_KPOS, 1).ravel()\n",
        "cols_p = i_pos.ravel()\n",
        "dij2   = (d_pos.ravel()**2).astype(np.float32)\n",
        "sig_i  = sigma_pos[rows_p,0]\n",
        "sig_j  = sigma_pos[cols_p,0]\n",
        "w_pos  = np.exp(-dij2 / ( (sig_i*sig_j) + 1e-9 )).astype(np.float32)\n",
        "w_pos *= ((c_pos.ravel().astype(np.float32)/V)**gamma)\n",
        "\n",
        "Wpos = coo_matrix((w_pos, (rows_p, cols_p)), shape=(N,N)).tocsr()\n",
        "Wpos = Wpos.maximum(Wpos.T); Wpos.setdiag(0.0); Wpos.eliminate_zeros()\n",
        "\n",
        "# NEG robust z-score\n",
        "# robust center/scale from far distances (already have d_far, mu_neg, mad_neg)\n",
        "SCALE_FLOOR = 1e-2      # floors tiny MAD to avoid exploding z\n",
        "Z_CLIP      = 8.0       # clip z for numerical stability\n",
        "REP_TEMP    = 0.75      # soften repulsion; >1 = softer, <1 = sharper\n",
        "\n",
        "scale_neg = 1.4826*mad_neg + 1e-9\n",
        "scale_neg = np.maximum(scale_neg, SCALE_FLOOR).astype(np.float32)\n",
        "\n",
        "rows_n = np.repeat(np.arange(N)[:,None], KEEP_KNEG, 1).ravel()\n",
        "cols_n = i_neg_raw.ravel()\n",
        "df     = d_far.ravel().astype(np.float32)\n",
        "\n",
        "# z-score on far distances for both endpoints, averaged\n",
        "zi = (df - mu_neg[rows_n,0]) / scale_neg[rows_n,0]\n",
        "zj = (df - mu_neg[cols_n,0]) / scale_neg[cols_n,0]\n",
        "z  = 0.5*(zi + zj)\n",
        "# clip and temperature-scale\n",
        "z  = np.clip(z/REP_TEMP, -Z_CLIP, Z_CLIP)\n",
        "\n",
        "# numerically stable sigmoid (tanh-based)\n",
        "rep = (0.5*(1.0 + np.tanh(0.5*z))).astype(np.float32)\n",
        "\n",
        "# consensus boost (already computed as c_neg)\n",
        "rep *= ((c_neg.ravel().astype(np.float32) / float(VIEWS)) ** gamma)\n",
        "rep = np.maximum(rep, 1e-6).astype(np.float32)\n",
        "\n",
        "Wneg = coo_matrix((rep, (rows_n, cols_n)), shape=(N, N)).tocsr()\n",
        "Wneg = Wneg.maximum(Wneg.T); Wneg.setdiag(0.0); Wneg.eliminate_zeros()\n",
        "\n",
        "# Diagnostics\n",
        "n_comp, labels_comp = connected_components(Wpos>0.0, directed=False)\n",
        "deg_pos = np.array(Wpos.sum(axis=1)).ravel()\n",
        "deg_neg = np.array(Wneg.sum(axis=1)).ravel()\n",
        "print(f\"Graphs built in {time.time()-t0:.1f}s | pos edges:{Wpos.nnz//2:,} | neg edges:{Wneg.nnz//2:,} | components:{n_comp} | deg_pos mean:{deg_pos.mean():.2f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Laplacians & spectral basis\n",
        "# -------------------------------\n",
        "def norm_laplacian(W):\n",
        "    d = np.array(W.sum(axis=1)).ravel()+1e-8\n",
        "    Dmh = diags((1.0/np.sqrt(d)).astype(np.float32))\n",
        "    S   = Dmh @ W @ Dmh\n",
        "    return (eye(W.shape[0], dtype=np.float32, format='csr') - S).tocsr()\n",
        "\n",
        "Lpos = norm_laplacian(Wpos)\n",
        "Lneg = norm_laplacian(Wneg)\n",
        "I = eye(N, dtype=np.float32, format='csr')\n",
        "Lneg_minus = (2.0*I - Lneg).tocsr()  # I + Sneg\n",
        "\n",
        "e_vals, e_vecs = eigsh(Lpos, k=min(EIGS, N-2), which='SM', tol=1e-3)\n",
        "idxs = np.argsort(e_vals); e_vals = e_vals[idxs].astype(np.float32); e_vecs = e_vecs[:,idxs].astype(np.float32)\n",
        "\n",
        "TopologyCapsule = {\n",
        "    \"N\": N,\n",
        "    \"Wpos\": Wpos, \"Wneg\": Wneg,\n",
        "    \"Lpos\": Lpos, \"Lneg_minus\": Lneg_minus,\n",
        "    \"evecs_pos\": e_vecs, \"evals_pos\": e_vals,\n",
        "    \"KNN_K\": KEEP_KPOS, \"KFN_K\": KEEP_KNEG\n",
        "}\n",
        "del Z0\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Labeled/unlabeled split (few labels per class)\n",
        "# -------------------------------\n",
        "def few_labels_per_class(y, n_per_class):\n",
        "    idx = np.arange(len(y)); rng.shuffle(idx)\n",
        "    cls = [[] for _ in range(int(y.max()+1))]\n",
        "    for i in idx:\n",
        "        c = y[i]\n",
        "        if len(cls[c])<n_per_class: cls[c].append(i)\n",
        "    L = np.array([i for c in cls for i in c], dtype=int)\n",
        "    U = np.setdiff1d(np.arange(len(y)), L, assume_unique=False)\n",
        "    return L,U\n",
        "\n",
        "idxL, idxU = few_labels_per_class(y, LABELED_PER_CLASS)\n",
        "\n",
        "# -------------------------------\n",
        "# 7) LP solvers (sparse)\n",
        "# -------------------------------\n",
        "def lp_solve(Lmat, idxL, y, ridge=1e-4):\n",
        "    N = Lmat.shape[0]; C = int(y.max()+1)\n",
        "    A = (Lmat + ridge*I).tocsr()\n",
        "    YL = np.eye(C, dtype=np.float32)[y[idxL]]\n",
        "    idxU = np.setdiff1d(np.arange(N), idxL, assume_unique=False)\n",
        "    A_uu = A[idxU][:,idxU]; A_ul = A[idxU][:,idxL]\n",
        "    Fu = spsolve(A_uu, -A_ul @ YL)\n",
        "    F  = np.zeros((N,C), dtype=np.float32)\n",
        "    F[idxL] = YL; F[idxU] = Fu\n",
        "    P  = np.clip(F, 1e-9, None); P /= (P.sum(1, keepdims=True)+1e-9)\n",
        "    return P\n",
        "\n",
        "# KNN-only Pd (baseline)\n",
        "Pd = lp_solve(Lpos, idxL, y, ridge=1e-4)\n",
        "acc_pd = accuracy_score(y[idxU], Pd[idxU].argmax(1))\n",
        "\n",
        "# Auto-tune signed Pd on seeds (log-likelihood on idxL only)\n",
        "best = (-1e9, None, None, None)\n",
        "for lam in LAM_GRID:\n",
        "    A = (Lpos + lam*Lneg_minus)\n",
        "    for rg in RIDGE_GRID:\n",
        "        P_try = lp_solve(A, idxL, y, ridge=rg)\n",
        "        ll = float(np.mean(np.log(np.clip(P_try[idxL, y[idxL]], 1e-12, 1.0))))\n",
        "        if ll > best[0]:\n",
        "            best = (ll, lam, rg, P_try)\n",
        "ll_seed, lam_star, ridge_star, Psigned = best\n",
        "acc_ps  = accuracy_score(y[idxU], Psigned[idxU].argmax(1))\n",
        "\n",
        "print(f\"[CNTC v2] Unlabeled acc | Pd: {acc_pd:.3f} | Pd(signed Î»={lam_star}, ridge={ridge_star}): {acc_ps:.3f} | seed-LL={ll_seed:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8) Internal transfer (kNN pull on spectral coords, using signed best)\n",
        "# -------------------------------\n",
        "Upos = TopologyCapsule[\"evecs_pos\"]\n",
        "split = int(TEST_SPLIT*N)\n",
        "tr = np.arange(N)[:split]; te = np.arange(N)[split:]\n",
        "\n",
        "def knn_pull(train_X, train_post, test_X, k=12):\n",
        "    nn = NearestNeighbors(n_neighbors=k, metric='cosine').fit(train_X)\n",
        "    d,i = nn.kneighbors(test_X)\n",
        "    w = 1.0-d; w /= (w.sum(1,keepdims=True)+1e-9)\n",
        "    return (w[:,:,None] * train_post[i]).sum(1)\n",
        "\n",
        "post_te = knn_pull(Upos[tr], Psigned[tr], Upos[te], k=12)\n",
        "acc_te  = accuracy_score(y[te], post_te.argmax(1))\n",
        "print(f\"[CNTC v2] Internal TEST acc (signed tuned): {acc_te:.3f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9) Diagnostics\n",
        "# -------------------------------\n",
        "n_comp, _ = connected_components(TopologyCapsule[\"Wpos\"]>0, directed=False)\n",
        "e6 = np.round(TopologyCapsule[\"evals_pos\"][:6], 4)\n",
        "cons_pos = (np.mean((i_pos>=0).astype(np.float32))*100.0)\n",
        "cons_neg = (np.mean((i_neg_raw>=0).astype(np.float32))*100.0)\n",
        "print(f\"[Diag] components:{n_comp} | eigs[:6]={e6} | pos-keptâ‰ˆ{cons_pos:.1f}% | neg-keptâ‰ˆ{cons_neg:.1f}%\")\n",
        "print(f\"Done in {time.time()-t0:.1f}s.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0FTnK67tmes",
        "outputId": "6695e7bb-2468-48b9-c596-47b2b664d635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graphs built in 24.5s | pos edges:48,651 | neg edges:34,444 | components:1 | deg_pos mean:4.93\n",
            "[CNTC v2] Unlabeled acc | Pd: 0.769 | Pd(signed Î»=0.0, ridge=1e-05): 0.769 | seed-LL=0.0000\n",
            "[CNTC v2] Internal TEST acc (signed tuned): 0.760\n",
            "[Diag] components:1 | eigs[:6]=[-0.    0.    0.01  0.01  0.01  0.01] | pos-keptâ‰ˆ100.0% | neg-keptâ‰ˆ100.0%\n",
            "Done in 291.6s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= CIFAR-10 Shape Capsule (CPU-fast, no deep nets) =================\n",
        "!pip -q install scikit-image scikit-learn torchvision --upgrade\n",
        "\n",
        "import numpy as np, time, math, random\n",
        "from skimage.feature import hog\n",
        "from skimage.color import rgb2gray\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --------- knobs (tune here) ---------\n",
        "SEED = 7\n",
        "N_POINTS = 4000           # try 6000 if you want, still CPU ok\n",
        "PCA_DIM  = 96\n",
        "KNN_K    = 12\n",
        "USE_KFN  = True           # False = KNN-only capsule\n",
        "KFN_K    = 6\n",
        "RIDGE    = 1e-4\n",
        "LABELED_PER_CLASS = 30    # few labels for Pd (shape->construct)\n",
        "SPLIT_FRAC = 0.85\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "def log(*a): print(*a, flush=True)\n",
        "\n",
        "# --------- 1) Load a slice ----------\n",
        "log(\"Loading CIFAR-10 â€¦\")\n",
        "tf = transforms.Compose([transforms.ToTensor()])\n",
        "ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tf)\n",
        "idx = np.arange(len(ds)); rng.shuffle(idx); idx = idx[:N_POINTS]\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "t0 = time.time()\n",
        "for i in idx:\n",
        "    img_t, lab = ds[i]              # torch [3,32,32] in [0,1]\n",
        "    img = img_t.numpy().transpose(1,2,0)  # (32,32,3)\n",
        "    y.append(lab)\n",
        "    # --------- 2) Fast CPU features: HOG + simple color stats ----------\n",
        "    g = rgb2gray(img)                                # float64 ok\n",
        "    # HOG params tuned for 32x32 speed & signal\n",
        "    f_hog = hog(g, pixels_per_cell=(4,4), cells_per_block=(2,2),\n",
        "                orientations=8, feature_vector=True, visualize=False, block_norm=\"L2-Hys\")\n",
        "    # tiny color stats (mean/std per channel)\n",
        "    m = img.mean(axis=(0,1)); s = img.std(axis=(0,1))+1e-9\n",
        "    feat = np.concatenate([f_hog.astype(np.float32), m.astype(np.float32), s.astype(np.float32)], 0)\n",
        "    X.append(feat)\n",
        "X = np.stack(X,0).astype(np.float32)\n",
        "y = np.array(y, dtype=np.int64)\n",
        "log(f\"Features: {X.shape} built in {time.time()-t0:.1f}s\")\n",
        "\n",
        "# --------- 3) PCA space (compact geometry coords) ----------\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "Xp = scaler.fit_transform(X)\n",
        "pca = PCA(n_components=PCA_DIM, random_state=SEED)\n",
        "Z = pca.fit_transform(Xp).astype(np.float32)\n",
        "\n",
        "# --------- 4) Graphs: mutual kNN (+ optional KFN) ----------\n",
        "def knn_graph(Z, k):\n",
        "    nn = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(Z)\n",
        "    d, i = nn.kneighbors(Z)       # d in [0,2] for cosine distance\n",
        "    i = i[:,1:]; d = d[:,1:]\n",
        "    S = np.clip(1.0 - d, 0.0, 1.0).astype(np.float32)  # similarity\n",
        "    return i, S\n",
        "\n",
        "def kfn_graph(Z, k, trim_q=0.02, t=0.0, s=0.35, mutual=True):\n",
        "    # cosine similarity matrix (dense but N up to ~6k is fine on Colab CPU)\n",
        "    sims = cosine_similarity(Z, Z).astype(np.float32)\n",
        "    np.fill_diagonal(sims, np.inf)                 # never self\n",
        "    order = np.argsort(sims, axis=1)               # ascending sim â†’ farthest first\n",
        "    k_eff = min(k + max(2, k//2), sims.shape[1]-1)\n",
        "    cand_idx = order[:, :k_eff]\n",
        "    cand_sim = np.take_along_axis(sims, cand_idx, axis=1)\n",
        "\n",
        "    trim = max(1, int(math.ceil(trim_q * k_eff)))\n",
        "    cand_idx = cand_idx[:, trim:trim+k]\n",
        "    cand_sim = cand_sim[:, trim:trim+k]\n",
        "\n",
        "    # soft repulsion weights via sigmoid\n",
        "    R = 1.0 / (1.0 + np.exp((cand_sim - t) / s))\n",
        "    R = np.maximum(R, 1e-6).astype(np.float32)\n",
        "\n",
        "    if mutual:\n",
        "        N = Z.shape[0]\n",
        "        M = np.zeros((N,N), dtype=bool)\n",
        "        rows = np.repeat(np.arange(N)[:,None], cand_idx.shape[1], 1)\n",
        "        M[rows, cand_idx] = True\n",
        "        keep = M[cand_idx, np.repeat(np.arange(N)[:,None], cand_idx.shape[1], 1)]\n",
        "        # ensure at least one kept per row\n",
        "        for r in range(keep.shape[0]):\n",
        "            if not keep[r].any():\n",
        "                keep[r, np.argmax(R[r])] = True\n",
        "        # zero out non-mutual\n",
        "        for r in range(R.shape[0]):\n",
        "            mask = keep[r]\n",
        "            if mask.sum() < R.shape[1]:\n",
        "                need = R.shape[1] - mask.sum()\n",
        "                top_extra = np.argsort(-R[r,~mask])[:need]\n",
        "                idx_extra = np.flatnonzero(~mask)[top_extra]\n",
        "                mask[idx_extra] = True\n",
        "            R[r, ~mask] = 1e-6\n",
        "    return cand_idx, R\n",
        "\n",
        "def build_sym(indices, weights, N):\n",
        "    W = np.zeros((N,N), dtype=np.float32)\n",
        "    rows = np.repeat(np.arange(N)[:,None], indices.shape[1], 1)\n",
        "    W[rows, indices] = weights\n",
        "    return np.maximum(W, W.T)\n",
        "\n",
        "def sym_normalize(W):\n",
        "    d = W.sum(1) + 1e-8\n",
        "    Dmh = np.diag(1.0/np.sqrt(d))\n",
        "    return (Dmh @ W @ Dmh).astype(np.float32)\n",
        "\n",
        "t0 = time.time()\n",
        "i_pos, w_pos = knn_graph(Z, KNN_K)\n",
        "Wpos = build_sym(i_pos, w_pos, N_POINTS)\n",
        "\n",
        "if USE_KFN:\n",
        "    i_neg, w_neg = kfn_graph(Z, KFN_K, trim_q=0.02, t=0.0, s=0.35, mutual=True)\n",
        "    Wneg = build_sym(i_neg, w_neg, N_POINTS)\n",
        "else:\n",
        "    Wneg = np.zeros_like(Wpos)\n",
        "\n",
        "Lpos = np.eye(N_POINTS, dtype=np.float32) - sym_normalize(Wpos)\n",
        "Lnegm = np.eye(N_POINTS, dtype=np.float32) + sym_normalize(Wneg)\n",
        "log(f\"Graphs built in {time.time()-t0:.1f}s | pos edges: {(Wpos>0).sum()//2:,} | neg edges: {(Wneg>0).sum()//2:,}\")\n",
        "\n",
        "# --------- 5) Capsule-only label propagation (Pd) ----------\n",
        "def few_labels_per_class(y, n_per_class, pool_idx):\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    idxL=[]\n",
        "    C=int(y.max())+1\n",
        "    for c in range(C):\n",
        "        cand = pool_idx[y[pool_idx]==c]\n",
        "        if len(cand)==0: continue\n",
        "        pick = cand if len(cand)<=n_per_class else rng.choice(cand, size=n_per_class, replace=False)\n",
        "        idxL.extend(pick.tolist())\n",
        "    idxL = np.array(sorted(set(idxL)), dtype=int)\n",
        "    idxU = np.setdiff1d(pool_idx, idxL, assume_unique=False)\n",
        "    return idxL, idxU\n",
        "\n",
        "def signed_lp(y, Lpos, Lnegm, idxL, lam_neg=0.2, ridge=RIDGE):\n",
        "    N = Lpos.shape[0]\n",
        "    C = int(y.max())+1\n",
        "    A = (Lpos + lam_neg*Lnegm + ridge*np.eye(N, dtype=np.float32)).astype(np.float32)\n",
        "    Y = np.zeros((N,C), dtype=np.float32)\n",
        "    Y[idxL, y[idxL]] = 1.0\n",
        "    idxU = np.setdiff1d(np.arange(N), idxL, assume_unique=False)\n",
        "    A_uu = A[np.ix_(idxU, idxU)]\n",
        "    A_ul = A[np.ix_(idxU, idxL)]\n",
        "    F_u  = np.linalg.solve(A_uu, -A_ul @ Y[idxL])\n",
        "    F = np.zeros((N,C), dtype=np.float32); F[idxL]=Y[idxL]; F[idxU]=F_u\n",
        "    P = np.clip(F,1e-9,None); P /= (P.sum(1,keepdims=True)+1e-9)\n",
        "    return P\n",
        "\n",
        "C = int(y.max())+1\n",
        "split = int(SPLIT_FRAC * N_POINTS)\n",
        "train_ids = np.arange(N_POINTS)[:split]\n",
        "test_ids  = np.arange(N_POINTS)[split:]\n",
        "idxL, idxU = few_labels_per_class(y, LABELED_PER_CLASS, train_ids)\n",
        "\n",
        "Pd_knn   = signed_lp(y, Lpos, 0*Lnegm, idxL, lam_neg=0.0, ridge=RIDGE)\n",
        "Pd_signed= signed_lp(y, Lpos,   Lnegm, idxL, lam_neg=0.2, ridge=RIDGE)\n",
        "\n",
        "acc_knn   = accuracy_score(y[idxU], Pd_knn[idxU].argmax(1))\n",
        "acc_signed= accuracy_score(y[idxU], Pd_signed[idxU].argmax(1))\n",
        "log(f\"[Capsule(HOG+PCA)] Unlabeled acc | KNN-only: {acc_knn:.3f} | KNN+KFN(signed): {acc_signed:.3f}\")\n",
        "\n",
        "# --------- 6) Internal TEST (capsule transfer) ----------\n",
        "def knn_pull(Ztr, Ptr, Zte, k=KNN_K):\n",
        "    nn = NearestNeighbors(n_neighbors=k, metric='cosine').fit(Ztr)\n",
        "    d,i = nn.kneighbors(Zte)\n",
        "    w = 1.0 - d; w /= (w.sum(1,keepdims=True)+1e-9)\n",
        "    return (w[:,:,None] * Ptr[i]).sum(1)\n",
        "\n",
        "post_test = knn_pull(Z[train_ids], Pd_signed[train_ids], Z[test_ids], k=KNN_K)\n",
        "acc_test  = accuracy_score(y[test_ids], post_test.argmax(1))\n",
        "log(f\"[Capsule(HOG+PCA)] Internal TEST acc (signed): {acc_test:.3f}\")\n",
        "\n",
        "# --------- 7) Tiny diagnostics ----------\n",
        "deg_pos = Wpos.sum(1).mean()\n",
        "log(f\"Done in {time.time()-t0:.1f}s | deg_pos mean: {deg_pos:.2f} | N={N_POINTS} | m={PCA_DIM}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1jOx2lc4cYo",
        "outputId": "1698aa42-3682-44c6-8b5b-81d6ff40125e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CIFAR-10 â€¦\n",
            "Features: (4000, 1574) built in 14.6s\n",
            "Graphs built in 9.6s | pos edges: 36,322 | neg edges: 19,882\n",
            "[Capsule(HOG+PCA)] Unlabeled acc | KNN-only: 0.359 | KNN+KFN(signed): 0.352\n",
            "[Capsule(HOG+PCA)] Internal TEST acc (signed): 0.367\n",
            "Done in 13.3s | deg_pos mean: 7.27 | N=4000 | m=96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# CIFAR-10 Topology Capsule (HOG+color, PCAâ†’KNN+KFN)\n",
        "# - CPU-only, fast (~45â€“60s on Colab CPU @ N=4000)\n",
        "# - No training loops, no pixels kept after capsule\n",
        "# - Label inference via tuned signed label propagation\n",
        "# ===============================================\n",
        "\n",
        "!pip -q install torchvision scikit-image scikit-learn\n",
        "\n",
        "import time, math, random, numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from skimage.feature import hog\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.utils import check_random_state\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import connected_components\n",
        "\n",
        "# -------------------------------\n",
        "# Config\n",
        "# -------------------------------\n",
        "SEED = 7\n",
        "rng = np.random.default_rng(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "N_POINTS   = 4000       # keep small for speed; try 8000 once you're happy\n",
        "PCA_DIM    = 96\n",
        "KNN_K      = 12\n",
        "KFN_K      = 6\n",
        "LABELED_PER_CLASS = 40  # seeds per class for capsule inference\n",
        "LAM_GRID   = [0.0, 0.05, 0.1, 0.2]\n",
        "RIDGE_GRID = [1e-5, 1e-4, 5e-4]\n",
        "\n",
        "# -------------------------------\n",
        "# Small utils\n",
        "# -------------------------------\n",
        "def normalize_rows(Z):\n",
        "    n = np.linalg.norm(Z, axis=1, keepdims=True) + 1e-9\n",
        "    return Z / n\n",
        "\n",
        "def few_labels_per_class(y, n_per_class):\n",
        "    idx = np.arange(len(y)); rng.shuffle(idx)\n",
        "    bins = [[] for _ in range(int(y.max())+1)]\n",
        "    for i in idx:\n",
        "        c = int(y[i])\n",
        "        if len(bins[c]) < n_per_class:\n",
        "            bins[c].append(i)\n",
        "        if all(len(b)==n_per_class for b in bins):\n",
        "            break\n",
        "    L = np.array([j for b in bins for j in b], dtype=int)\n",
        "    U = np.setdiff1d(np.arange(len(y)), L, assume_unique=False)\n",
        "    return L, U\n",
        "\n",
        "def sym_normalize(W):\n",
        "    d = W.sum(1) + 1e-8\n",
        "    Dmh = np.diag(1.0/np.sqrt(d))\n",
        "    return (Dmh @ W @ Dmh).astype(np.float32)\n",
        "\n",
        "def build_sym(indices, weights, N):\n",
        "    W = np.zeros((N,N), dtype=np.float32)\n",
        "    rows = np.repeat(np.arange(N)[:,None], indices.shape[1], 1)\n",
        "    W[rows, indices] = weights\n",
        "    W = np.maximum(W, W.T)\n",
        "    return W\n",
        "\n",
        "def knn_pairs_cosine(Z, k):\n",
        "    nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(Z)\n",
        "    d, i = nbrs.kneighbors(Z)\n",
        "    return i[:,1:], np.clip(1.0 - d[:,1:], 0.0, 1.0).astype(np.float32)\n",
        "\n",
        "# ---- FIXED: farthest neighbors by cosine with correct normalization ----\n",
        "def kfn_pairs_cosine(Z, k, trim_q=0.02, t=0.0, s=0.35):\n",
        "    \"\"\"\n",
        "    Farthest-neighbor pairs by cosine dissimilarity on row-normalized Z.\n",
        "    Returns indices (N x k) and soft repulsion weights (N x k).\n",
        "    \"\"\"\n",
        "    Zhat = normalize_rows(Z.astype(np.float32))        # (N,d)\n",
        "    sims = Zhat @ Zhat.T                               # cosine in [-1,1]\n",
        "    np.fill_diagonal(sims, np.inf)                     # never select self as farthest\n",
        "\n",
        "    # sort ascending by sim  â†’ most dissimilar first\n",
        "    order   = np.argsort(sims, axis=1)\n",
        "    k_eff   = min(k + max(2, k//2), sims.shape[1]-1)   # oversample, then trim tails\n",
        "    cand_ix = order[:, :k_eff]\n",
        "    cand_sm = np.take_along_axis(sims, cand_ix, axis=1)\n",
        "\n",
        "    # trim extreme outliers then keep top-k farthest\n",
        "    trim = max(1, int(math.ceil(trim_q * k_eff)))\n",
        "    cand_ix = cand_ix[:, trim:trim+k]\n",
        "    cand_sm = cand_sm[:, trim:trim+k]\n",
        "\n",
        "    # soft repulsion weights via a smooth sigmoid on similarity (lower sim â†’ stronger repulsion)\n",
        "    R = 1.0 / (1.0 + np.exp((cand_sm - t) / s))\n",
        "    return cand_ix, np.maximum(R.astype(np.float32), 1e-6)\n",
        "\n",
        "def laplacians(Wpos, Wneg):\n",
        "    Spos = sym_normalize(Wpos)\n",
        "    Sneg = sym_normalize(Wneg)\n",
        "    Lpos = np.eye(Wpos.shape[0], dtype=np.float32) - Spos\n",
        "    Lneg_minus = np.eye(Wneg.shape[0], dtype=np.float32) + Sneg\n",
        "    return Lpos, Lneg_minus\n",
        "\n",
        "def signed_lp(y, Lpos, Lnegm, idxL, lam_neg=0.1, ridge=1e-4):\n",
        "    N = Lpos.shape[0]\n",
        "    C = int(y.max())+1\n",
        "    A = (Lpos + lam_neg*Lnegm + ridge*np.eye(N, dtype=np.float32)).astype(np.float32)\n",
        "\n",
        "    Y = np.zeros((N, C), dtype=np.float32)\n",
        "    Y[idxL, y[idxL]] = 1.0\n",
        "\n",
        "    idxU = np.setdiff1d(np.arange(N), idxL, assume_unique=False)\n",
        "    A_uu = A[np.ix_(idxU, idxU)]\n",
        "    A_ul = A[np.ix_(idxU, idxL)]\n",
        "    F_u  = np.linalg.solve(A_uu, -A_ul @ Y[idxL])\n",
        "    F = np.zeros((N, C), dtype=np.float32); F[idxL]=Y[idxL]; F[idxU]=F_u\n",
        "    P = np.clip(F, 1e-9, None); P /= (P.sum(1, keepdims=True)+1e-9)\n",
        "    return P\n",
        "\n",
        "def seed_loglik(P, y, idxL):\n",
        "    Pclip = np.clip(P[idxL, y[idxL]], 1e-12, 1.0)\n",
        "    return float(np.mean(np.log(Pclip)))\n",
        "\n",
        "def signed_lp_tuned(y, Lpos, Lnegm, idxL, lam_grid, ridge_grid):\n",
        "    best = (-np.inf, (None,None), None)\n",
        "    for lam in lam_grid:\n",
        "        for rg in ridge_grid:\n",
        "            P_try = signed_lp(y, Lpos, Lnegm, idxL, lam_neg=lam, ridge=rg)\n",
        "            ll = seed_loglik(P_try, y, idxL)\n",
        "            if ll > best[0]:\n",
        "                best = (ll, (lam, rg), P_try)\n",
        "    return best[2], best[1], best[0]\n",
        "\n",
        "# -------------------------------\n",
        "# Feature extraction (HOG + color hist)\n",
        "# -------------------------------\n",
        "def hog_color_features(images):\n",
        "    \"\"\"\n",
        "    images: (N, 32, 32, 3) float32 in [0,1]\n",
        "    Returns: (N, D) features (HOG on gray + 16-bin color hist/channel)\n",
        "    \"\"\"\n",
        "    N = images.shape[0]\n",
        "    feats = []\n",
        "    bins = 16\n",
        "    for i in range(N):\n",
        "        img = images[i]\n",
        "        gray = (0.299*img[:,:,0] + 0.587*img[:,:,1] + 0.114*img[:,:,2]).astype(np.float32)\n",
        "        # HOG (fast parameters for 32x32)\n",
        "        h = hog(gray, orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2),\n",
        "                block_norm='L2-Hys', feature_vector=True)\n",
        "        # 16-bin per-channel hist (normalized)\n",
        "        ch = []\n",
        "        for c in range(3):\n",
        "            hist, _ = np.histogram(img[:,:,c], bins=bins, range=(0,1), density=True)\n",
        "            ch.append(hist.astype(np.float32))\n",
        "        feat = np.concatenate([h.astype(np.float32)] + ch, axis=0)\n",
        "        feats.append(feat)\n",
        "    return np.stack(feats, 0)\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Load CIFAR-10 and build features\n",
        "# -------------------------------\n",
        "t0 = time.time()\n",
        "tf = transforms.Compose([transforms.ToTensor()])\n",
        "train = datasets.CIFAR10(root='./data', train=True, download=True, transform=tf)\n",
        "\n",
        "idx = np.arange(len(train))\n",
        "rng.shuffle(idx); idx = idx[:N_POINTS]\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i in idx:\n",
        "    img, lab = train[i]           # img: [3,32,32] tensor in [0,1]\n",
        "    X.append(np.moveaxis(img.numpy(), 0, -1))  # (32,32,3)\n",
        "    y.append(lab)\n",
        "X = np.stack(X,0).astype(np.float32)\n",
        "y = np.array(y, dtype=np.int64)\n",
        "\n",
        "# HOG+color features + standardize + PCA\n",
        "t_feat = time.time()\n",
        "F = hog_color_features(X)                       # (N, D_hog+48)\n",
        "scaler = StandardScaler().fit(F)\n",
        "Fp = scaler.transform(F)\n",
        "pca = PCA(n_components=PCA_DIM, random_state=SEED)\n",
        "Z = pca.fit_transform(Fp).astype(np.float32)\n",
        "print(f\"Loading CIFAR-10 â€¦\\nFeatures: HOG+color {F.shape} in {time.time()-t_feat:.1f}s\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Build graphs (KNN+, KFN-) and Laplacians\n",
        "# -------------------------------\n",
        "def build_capsule_graphs(Z):\n",
        "    i_pos, w_pos = knn_pairs_cosine(Z, KNN_K)\n",
        "    i_neg, w_neg = kfn_pairs_cosine(Z, KFN_K, trim_q=0.02, t=0.0, s=0.35)\n",
        "    Wpos = build_sym(i_pos, w_pos, Z.shape[0])\n",
        "    Wneg = build_sym(i_neg, w_neg, Z.shape[0])\n",
        "    Lpos, Lnegm = laplacians(Wpos, Wneg)\n",
        "    return Wpos, Wneg, Lpos, Lnegm\n",
        "\n",
        "t_g = time.time()\n",
        "Wpos, Wneg, Lpos, Lnegm = build_capsule_graphs(Z)\n",
        "# graph diagnostics\n",
        "N = Z.shape[0]\n",
        "G = csr_matrix((Wpos>0).astype(np.int8))\n",
        "ncomp, labels = connected_components(G, directed=False)\n",
        "deg_mean = Wpos.sum(1).mean()\n",
        "print(f\"Graphs built in {time.time()-t_g:.1f}s | pos edges: {int((Wpos>0).sum())} | neg edges: {int((Wneg>0).sum())} | components: {ncomp} | deg_pos mean: {deg_mean:.2f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Seeds and label propagation (tuned)\n",
        "# -------------------------------\n",
        "idxL, idxU = few_labels_per_class(y, LABELED_PER_CLASS)\n",
        "P_knn, _, _ = signed_lp_tuned(y, Lpos, 0.0*Lnegm, idxL, lam_grid=[0.0], ridge_grid=[1e-5])  # KNN-only baseline\n",
        "acc_knn = accuracy_score(y[idxU], P_knn[idxU].argmax(1))\n",
        "\n",
        "P_signed, (lam_opt, rg_opt), ll = signed_lp_tuned(y, Lpos, Lnegm, idxL, LAM_GRID, RIDGE_GRID)\n",
        "acc_signed = accuracy_score(y[idxU], P_signed[idxU].argmax(1))\n",
        "print(f\"[Capsule(HOG+PCA)] Unlabeled acc | KNN-only: {acc_knn:.3f} | KNN+KFN(signed): {acc_signed:.3f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Internal TEST accuracy (held-out slice, capsule-only transfer)\n",
        "# -------------------------------\n",
        "split = int(0.85 * N)\n",
        "test_ids = np.arange(N)[split:]\n",
        "acc_test = accuracy_score(y[test_ids], P_signed[test_ids].argmax(1))\n",
        "print(f\"[Capsule(HOG+PCA)] Internal TEST acc (signed): {acc_test:.3f}\")\n",
        "print(f\"Done in {time.time()-t0:.1f}s | N={N} | m={PCA_DIM} | (lam={lam_opt}, ridge={rg_opt}, seed-LL={ll:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7_nNWZr7G4d",
        "outputId": "590aa39a-0bb1-4ad6-ebc9-baebd0c1ff62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CIFAR-10 â€¦\n",
            "Features: HOG+color (4000, 372) in 3.5s\n",
            "Graphs built in 9.8s | pos edges: 72094 | neg edges: 40462 | components: 1 | deg_pos mean: 7.81\n",
            "[Capsule(HOG+PCA)] Unlabeled acc | KNN-only: 0.373 | KNN+KFN(signed): 0.373\n",
            "[Capsule(HOG+PCA)] Internal TEST acc (signed): 0.412\n",
            "Done in 36.8s | N=4000 | m=96 | (lam=0.0, ridge=1e-05, seed-LL=0.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Cat vs Dog â€” Multi-View Topology Capsule (fast CPU)\n",
        "# - Views: silhouette (sil), texture (tex), structure (str), diffused (dif)\n",
        "# - Graphs: KNN(+) + KFN(-) per-view, signed LP posteriors Pd_view\n",
        "# - Per-view temperature calibration on seeds\n",
        "# - Stability weights via light perturb-and-resolve\n",
        "# - Geometric-mean ensemble (no logits, no NaNs)\n",
        "# ===============================================\n",
        "\n",
        "!pip -q install torchvision scikit-image\n",
        "\n",
        "import numpy as np, math, time, random\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from skimage import color\n",
        "from skimage.feature import hog, local_binary_pattern\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.transform import resize\n",
        "from skimage.filters import gabor\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# -------------------------------\n",
        "# Repro & knobs\n",
        "# -------------------------------\n",
        "SEED             = 7\n",
        "rng              = np.random.default_rng(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "N_POINTS         = 4000         # subset for speed\n",
        "LABELED_PER_CLS  = 60           # seeds per class (binary here â†’ total 120)\n",
        "KPOS             = 12           # positive K (KNN)\n",
        "KNEG             = 8            # negative K (KFN)\n",
        "PCA_DIM_SIL      = 64           # compact distance space per-view\n",
        "PCA_DIM_STR      = 96\n",
        "PCA_DIM_TEX      = 48\n",
        "PERTURB_REPEATS  = 3            # stability weighting budget\n",
        "PERTURB_NOISE    = 0.08         # multiplicative noise on edges for stability\n",
        "DIFF_T           = 1.2          # heat smoothing for \"diff\" view\n",
        "RIDGE_GRID       = [1e-5, 5e-5, 1e-4]\n",
        "LAMNEG_GRID      = [0.0, 0.1, 0.2]\n",
        "\n",
        "# -------------------------------\n",
        "# Small utils\n",
        "# -------------------------------\n",
        "def split_train_test(N, test_frac=0.15, seed=SEED):\n",
        "    idx = np.arange(N)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    rng.shuffle(idx)\n",
        "    s = int(round((1.0 - test_frac) * N))\n",
        "    return idx[:s], idx[s:]\n",
        "\n",
        "def few_labels_per_class(y, n_per_class):\n",
        "    idx = np.arange(len(y)); rng.shuffle(idx)\n",
        "    labels = np.unique(y)\n",
        "    take = []\n",
        "    cnt = {int(c): 0 for c in labels}\n",
        "    for i in idx:\n",
        "        c = int(y[i])\n",
        "        if cnt[c] < n_per_class:\n",
        "            cnt[c] += 1\n",
        "            take.append(i)\n",
        "        if all(cnt[int(c)] >= n_per_class for c in labels):\n",
        "            break\n",
        "    L = np.array(take, dtype=int)\n",
        "    U = np.setdiff1d(np.arange(len(y)), L, assume_unique=False)\n",
        "    return L, U\n",
        "\n",
        "def normalize_rows(A):\n",
        "    n = np.linalg.norm(A, axis=1, keepdims=True) + 1e-9\n",
        "    return A / n\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Load CIFAR-10 cats/dogs subset (labels: cat=0, dog=1)\n",
        "# -------------------------------\n",
        "t0 = time.time()\n",
        "tf = transforms.Compose([transforms.ToTensor()])\n",
        "ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tf)\n",
        "\n",
        "# cat=3, dog=5 in CIFAR-10. Relabel to {0,1}\n",
        "idx_all = [i for i in range(len(ds)) if ds.targets[i] in (3,5)]\n",
        "rng.shuffle(idx_all)\n",
        "idx_all = idx_all[:N_POINTS]\n",
        "\n",
        "Xrgb = []\n",
        "y = []\n",
        "for i in idx_all:\n",
        "    img, lab = ds[i]                 # img: [3,32,32] float in [0,1]\n",
        "    Xrgb.append((img.numpy()*255.0).astype(np.uint8))   # uint8 for LBP stability\n",
        "    y.append(0 if lab==3 else 1)     # {cat,dog} -> {0,1}\n",
        "Xrgb = np.stack(Xrgb, 0)             # (N, 3, 32, 32) uint8\n",
        "y = np.array(y, dtype=np.int64)\n",
        "print(f\"Loaded cat/dog subset: N={len(y)} (cat={(y==0).sum()}, dog={(y==1).sum()})\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2) View feature builders (fast)\n",
        "# -------------------------------\n",
        "def silhouette_view(img_u8):\n",
        "    # grayscale float32\n",
        "    gray_f = color.rgb2gray(np.moveaxis(img_u8,0,2).astype(np.float32)/255.0)\n",
        "    # coarse silhouette via Otsu threshold\n",
        "    t = threshold_otsu(gray_f)\n",
        "    mask = (gray_f < t).astype(np.float32)\n",
        "    # normalize area & simple radial bins\n",
        "    mask_r = resize(mask, (24,24), anti_aliasing=True).astype(np.float32)\n",
        "    # 1) downsampled mask\n",
        "    v1 = resize(mask, (12,12), anti_aliasing=True).reshape(-1).astype(np.float32)\n",
        "    # 2) row/col sums (shape profile)\n",
        "    v2 = np.concatenate([mask_r.sum(0), mask_r.sum(1)], 0).astype(np.float32)\n",
        "    # 3) coarse contour via Sobel energy\n",
        "    from skimage.filters import sobel\n",
        "    edge = sobel(mask_r)\n",
        "    v3 = edge.reshape(-1)\n",
        "    v = np.concatenate([v1, v2, v3], 0)\n",
        "    return v\n",
        "\n",
        "def texture_view(img_u8):\n",
        "    # LBP prefers integer domain; use uint8 on [0,255]\n",
        "    gray_u8 = color.rgb2gray(np.moveaxis(img_u8,0,2)).astype(np.float32)\n",
        "    gray_u8 = np.clip(gray_u8*255.0, 0, 255).astype(np.uint8)\n",
        "\n",
        "    def lbp_hist(gray_u8, P, R):\n",
        "        lbp = local_binary_pattern(gray_u8, P=P, R=R, method='uniform')\n",
        "        n_bins = P + 2\n",
        "        hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
        "        return hist.astype(np.float32)\n",
        "\n",
        "    h1 = lbp_hist(gray_u8, P=8,  R=1)   # 10\n",
        "    h2 = lbp_hist(gray_u8, P=16, R=2)   # 18\n",
        "    h3 = lbp_hist(gray_u8, P=8,  R=2)   # 10  -> total 38 bins\n",
        "\n",
        "    # Gabor energies (on float grayscale)\n",
        "    gray_f = color.rgb2gray(np.moveaxis(img_u8,0,2).astype(np.float32)/255.0)\n",
        "    real, imag = gabor(gray_f, frequency=0.25, theta=0.0);  e0 = np.mean(real**2 + imag**2)\n",
        "    real, imag = gabor(gray_f, frequency=0.25, theta=np.pi/2); e1 = np.mean(real**2 + imag**2)\n",
        "    gab = np.array([e0, e1], dtype=np.float32)              # +2 -> 40 dims\n",
        "    return np.concatenate([h1,h2,h3,gab], 0)\n",
        "\n",
        "def structure_view(img_u8):\n",
        "    # HOG on luminance â€” light settings to keep dim ~150\n",
        "    gray_f = color.rgb2gray(np.moveaxis(img_u8,0,2).astype(np.float32)/255.0)\n",
        "    feat = hog(gray_f, orientations=8, pixels_per_cell=(8,8), cells_per_block=(2,2),\n",
        "               block_norm='L2-Hys', feature_vector=True)\n",
        "    return feat.astype(np.float32)\n",
        "\n",
        "# Build raw features\n",
        "t1 = time.time()\n",
        "V_sil = np.stack([silhouette_view(x) for x in Xrgb], 0)\n",
        "V_tex = np.stack([texture_view(x)    for x in Xrgb], 0)\n",
        "V_str = np.stack([structure_view(x)  for x in Xrgb], 0)\n",
        "print(f\"Built raw view features: sil={V_sil.shape}, tex={V_tex.shape}, str={V_str.shape} in {time.time()-t1:.1f}s\")\n",
        "\n",
        "# Compact metric spaces (per-view PCA over standardized features)\n",
        "def compact_embed(V, dim):\n",
        "    Vz = StandardScaler().fit_transform(V)\n",
        "    if dim >= Vz.shape[1]:\n",
        "        return Vz.astype(np.float32)\n",
        "    pca = PCA(n_components=dim, random_state=SEED)\n",
        "    return pca.fit_transform(Vz).astype(np.float32)\n",
        "\n",
        "Z_sil = compact_embed(V_sil, PCA_DIM_SIL)\n",
        "Z_tex = compact_embed(V_tex, PCA_DIM_TEX)\n",
        "Z_str = compact_embed(V_str, PCA_DIM_STR)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Graphs: KNN(+) + KFN(-) (cosine), symmetrized\n",
        "# -------------------------------\n",
        "def knn_pairs_cosine(Z, k):\n",
        "    nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine').fit(Z)\n",
        "    d, i = nbrs.kneighbors(Z)\n",
        "    i = i[:,1:]\n",
        "    S = np.clip(1.0 - d[:,1:], 0.0, 1.0).astype(np.float32)\n",
        "    return i, S\n",
        "\n",
        "def kfn_pairs_cosine(Z, k, trim_q=0.02, t=0.0, s=0.35):\n",
        "    # cosine similarity in [-1,1]; choose smallest sims (farthest)\n",
        "    ZN = normalize_rows(Z)\n",
        "    sims = (ZN @ ZN.T).astype(np.float32)\n",
        "    np.fill_diagonal(sims, np.inf)\n",
        "    order = np.argsort(sims, axis=1)         # ascending -> farthest first\n",
        "    k_eff = min(k + max(2,k//2), Z.shape[0]-1)\n",
        "    cand_idx = order[:, :k_eff]\n",
        "    cand_sim = np.take_along_axis(sims, cand_idx, axis=1)\n",
        "    # trim extremes\n",
        "    trim = max(1, int(math.ceil(trim_q * k_eff)))\n",
        "    cand_idx = cand_idx[:, trim:trim+k]\n",
        "    cand_sim = cand_sim[:, trim:trim+k]\n",
        "    # soft repulsion\n",
        "    R = 1.0 / (1.0 + np.exp((cand_sim - t) / s))\n",
        "    return cand_idx.astype(np.int32), np.maximum(R, 1e-6).astype(np.float32)\n",
        "\n",
        "def build_sym(indices, weights, N):\n",
        "    W = np.zeros((N,N), dtype=np.float32)\n",
        "    rows = np.repeat(np.arange(N)[:,None], indices.shape[1], 1)\n",
        "    W[rows, indices] = weights\n",
        "    return np.maximum(W, W.T)\n",
        "\n",
        "def sym_norm(W):\n",
        "    d = W.sum(1) + 1e-8\n",
        "    Dmh = np.diag(1.0/np.sqrt(d))\n",
        "    return (Dmh @ W @ Dmh).astype(np.float32)\n",
        "\n",
        "def capsule_graphs(Z):\n",
        "    i_pos, w_pos = knn_pairs_cosine(Z, KPOS)\n",
        "    i_neg, w_neg = kfn_pairs_cosine(Z, KNEG, trim_q=0.02, t=0.0, s=0.35)\n",
        "    Wpos = build_sym(i_pos, w_pos, Z.shape[0])\n",
        "    Wneg = build_sym(i_neg, w_neg, Z.shape[0])\n",
        "    Spos = sym_norm(Wpos)\n",
        "    Sneg = sym_norm(Wneg)\n",
        "    Lpos = (np.eye(Z.shape[0], dtype=np.float32) - Spos).astype(np.float32)\n",
        "    Lnegm= (np.eye(Z.shape[0], dtype=np.float32) + Sneg).astype(np.float32)\n",
        "    return Wpos, Wneg, Lpos, Lnegm\n",
        "\n",
        "t2 = time.time()\n",
        "Wpos_sil, Wneg_sil, Lpos_sil, Lnegm_sil = capsule_graphs(Z_sil)\n",
        "Wpos_tex, Wneg_tex, Lpos_tex, Lnegm_tex = capsule_graphs(Z_tex)\n",
        "Wpos_str, Wneg_str, Lpos_str, Lnegm_str = capsule_graphs(Z_str)\n",
        "print(f\"Graphs built in {time.time()-t2:.1f}s | pos edges: {int(Wpos_sil.sum()>0)+int(Wpos_tex.sum()>0)+int(Wpos_str.sum()>0)} (per-view matrices built)\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Seeds & signed LP (tune on seeds by log-likelihood)\n",
        "# -------------------------------\n",
        "idxL, idxU = few_labels_per_class(y, LABELED_PER_CLS)\n",
        "\n",
        "def signed_lp_tuned(y, Lpos, Lnegm, idxL, lam_grid=LAMNEG_GRID, ridge_grid=RIDGE_GRID):\n",
        "    N = Lpos.shape[0]\n",
        "    C = int(y.max())+1\n",
        "    YL = np.zeros((len(idxL), C), dtype=np.float32); YL[np.arange(len(idxL)), y[idxL]] = 1.0\n",
        "    best = (-np.inf, None, None, None)  # (ll, lam, ridge, P)\n",
        "\n",
        "    for lam in lam_grid:\n",
        "        A = (Lpos + lam*Lnegm).astype(np.float32)\n",
        "        for rg in ridge_grid:\n",
        "            A_ = A + rg*np.eye(N, dtype=np.float32)\n",
        "            idxU_local = np.setdiff1d(np.arange(N), idxL, assume_unique=False)\n",
        "            A_uu = A_[np.ix_(idxU_local, idxU_local)]\n",
        "            A_ul = A_[np.ix_(idxU_local, idxL)]\n",
        "            Fu  = np.linalg.solve(A_uu, -A_ul @ YL)\n",
        "            F   = np.zeros((N,C), dtype=np.float32); F[idxL]=YL; F[idxU_local]=Fu\n",
        "            P   = np.clip(F, 1e-6, None); P /= (P.sum(1, keepdims=True)+1e-9)\n",
        "            # seed LL\n",
        "            ll = float(np.mean(np.log(np.clip(P[idxL, y[idxL]], 1e-9, 1.0))))\n",
        "            if ll > best[0]:\n",
        "                best = (ll, lam, rg, P)\n",
        "    return best[3], (best[1], best[2]), best[0]\n",
        "\n",
        "# per-view posteriors\n",
        "P_sil, (lam_sil, rg_sil), ll_sil = signed_lp_tuned(y, Lpos_sil, Lnegm_sil, idxL)\n",
        "P_tex, (lam_tex, rg_tex), ll_tex = signed_lp_tuned(y, Lpos_tex, Lnegm_tex, idxL)\n",
        "P_str, (lam_str, rg_str), ll_str = signed_lp_tuned(y, Lpos_str, Lnegm_str, idxL)\n",
        "\n",
        "# An extra \"diffused\" view: heat-smooth P_str over its graph\n",
        "def heat_smooth(P, Lpos, t=DIFF_T):\n",
        "    # Use (I + t*Lpos)^{-1} P  (one-step Tikhonov heat approximation)\n",
        "    N = Lpos.shape[0]\n",
        "    A = (np.eye(N, dtype=np.float32) + t*Lpos).astype(np.float32)\n",
        "    return np.linalg.solve(A, P).astype(np.float32)\n",
        "\n",
        "P_diff = heat_smooth(P_str, Lpos_str, t=DIFF_T)\n",
        "for P in (P_sil, P_tex, P_str, P_diff):\n",
        "    P[:] = np.clip(P, 1e-6, 1.0); P[:] /= (P.sum(1, keepdims=True)+1e-9)\n",
        "\n",
        "# quick per-view reference\n",
        "acc_sil = accuracy_score(y[idxU], P_sil[idxU].argmax(1))\n",
        "acc_tex = accuracy_score(y[idxU], P_tex[idxU].argmax(1))\n",
        "acc_str = accuracy_score(y[idxU], P_str[idxU].argmax(1))\n",
        "acc_dif = accuracy_score(y[idxU], P_diff[idxU].argmax(1))\n",
        "print(f\"[Per-view] Unlabeled acc â€” sil:{acc_sil:.3f} tex:{acc_tex:.3f} str:{acc_str:.3f} dif:{acc_dif:.3f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Per-view temperature calibration (on seeds)\n",
        "# -------------------------------\n",
        "def calibrate_T(P, idx, y_true, grid=(0.6, 0.8, 1.0, 1.2, 1.6)):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6)\n",
        "    logP = np.log(P)\n",
        "    bestT, bestLL = 1.0, -np.inf\n",
        "    for T in grid:\n",
        "        Q = np.exp(logP[idx] / T); Q /= (Q.sum(1, keepdims=True) + 1e-9)\n",
        "        ll = float(np.mean(np.log(np.clip(Q[np.arange(len(idx)), y_true[idx]], 1e-9, 1.0))))\n",
        "        if ll > bestLL:\n",
        "            bestLL, bestT = ll, T\n",
        "    return bestT, bestLL\n",
        "\n",
        "def apply_T(P, T):\n",
        "    P = np.clip(P, 1e-6, 1-1e-6)\n",
        "    Q = np.exp(np.log(P)/T); Q /= (Q.sum(1, keepdims=True)+1e-9)\n",
        "    return Q\n",
        "\n",
        "Ts = []\n",
        "seedLLs = []\n",
        "views_raw = [(\"sil\", P_sil), (\"tex\", P_tex), (\"str\", P_str), (\"dif\", P_diff)]\n",
        "views_cal = {}\n",
        "for name, Pv in views_raw:\n",
        "    T, ll = calibrate_T(Pv, idxL, y)\n",
        "    Ts.append(T); seedLLs.append(ll)\n",
        "    views_cal[name] = apply_T(Pv, T)\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Stability weighting (light perturb-and-resolve)\n",
        "# -------------------------------\n",
        "def stability_weight(Lpos, Lnegm, P_ref, idxL, repeats=PERTURB_REPEATS, noise=PERTURB_NOISE):\n",
        "    # perturb W via Lpos' = I - (D^{-1/2}(W*(1+eps))D^{-1/2})   approx through Lpos scaling\n",
        "    # we approximate by scaling Laplacian: L' = (I + eps) L  (cheap proxy)\n",
        "    N = Lpos.shape[0]\n",
        "    C = P_ref.shape[1]\n",
        "    YL = np.zeros((len(idxL), C), dtype=np.float32); YL[np.arange(len(idxL)), y[idxL]] = 1.0\n",
        "    idxU_local = np.setdiff1d(np.arange(N), idxL, assume_unique=False)\n",
        "\n",
        "    # choose a fixed (lam, ridge) that worked on seeds for this view by re-estimating quickly\n",
        "    best = signed_lp_tuned(y, Lpos, Lnegm, idxL)[1]\n",
        "    lam, rg = best\n",
        "    A = (Lpos + lam*Lnegm + rg*np.eye(N, dtype=np.float32)).astype(np.float32)\n",
        "    A_uu = A[np.ix_(idxU_local, idxU_local)]\n",
        "    A_ul = A[np.ix_(idxU_local, idxL)]\n",
        "\n",
        "    def solve(A_uu, A_ul):\n",
        "        Fu  = np.linalg.solve(A_uu, -A_ul @ YL)\n",
        "        F   = np.zeros((N, C), dtype=np.float32); F[idxL]=YL; F[idxU_local]=Fu\n",
        "        P   = np.clip(F, 1e-6, None); P /= (P.sum(1, keepdims=True)+1e-9)\n",
        "        return P\n",
        "\n",
        "    Pref = solve(A_uu, A_ul)\n",
        "    # perturb a few times\n",
        "    deltas = []\n",
        "    for _ in range(repeats):\n",
        "        eps = (1.0 + noise * rng.standard_normal(size=(1,), dtype=np.float32))[0]\n",
        "        A_uu_p = A_uu * eps\n",
        "        A_ul_p = A_ul * eps\n",
        "        Pp = solve(A_uu_p, A_ul_p)\n",
        "        # mean cosine similarity between Pref and Pp\n",
        "        u = normalize_rows(Pref)\n",
        "        v = normalize_rows(Pp)\n",
        "        cos = (u*v).sum(1).mean()\n",
        "        deltas.append(float(cos))\n",
        "    # stability score: higher is better\n",
        "    return max(0.0, min(1.0, float(np.mean(deltas))))\n",
        "\n",
        "w_sil = stability_weight(Lpos_sil, Lnegm_sil, views_cal[\"sil\"], idxL)\n",
        "w_tex = stability_weight(Lpos_tex, Lnegm_tex, views_cal[\"tex\"], idxL)\n",
        "w_str = stability_weight(Lpos_str, Lnegm_str, views_cal[\"str\"], idxL)\n",
        "w_dif = stability_weight(Lpos_str, Lnegm_str, views_cal[\"dif\"], idxL)\n",
        "stab = np.array([w_sil, w_tex, w_str, w_dif], dtype=np.float32)\n",
        "\n",
        "# evidence from seeds (calibrated)\n",
        "seed_e = np.exp(np.array(seedLLs, dtype=np.float32))\n",
        "raw_w  = seed_e * stab\n",
        "w_all  = raw_w / (raw_w.sum() + 1e-12)\n",
        "print(\"Per-view T:\", [round(t,2) for t in Ts])\n",
        "print(\"Seed LL:\", [round(ll,4) for ll in seedLLs])\n",
        "print(\"Stability:\", [round(float(x),3) for x in stab])\n",
        "print(\"Blend weights:\", [round(float(x),3) for x in w_all])\n",
        "\n",
        "# -------------------------------\n",
        "# 7) Geometric-mean ensemble (numerically safe)\n",
        "# -------------------------------\n",
        "P_sil_c = np.clip(views_cal[\"sil\"], 1e-6, 1-1e-6)\n",
        "P_tex_c = np.clip(views_cal[\"tex\"], 1e-6, 1-1e-6)\n",
        "P_str_c = np.clip(views_cal[\"str\"], 1e-6, 1-1e-6)\n",
        "P_diff_c= np.clip(views_cal[\"dif\"], 1e-6, 1-1e-6)\n",
        "\n",
        "Lsum = (w_all[0]*np.log(P_sil_c) +\n",
        "        w_all[1]*np.log(P_tex_c) +\n",
        "        w_all[2]*np.log(P_str_c) +\n",
        "        w_all[3]*np.log(P_diff_c))\n",
        "P_ens = np.exp(Lsum); P_ens /= (P_ens.sum(1, keepdims=True) + 1e-9)\n",
        "\n",
        "# -------------------------------\n",
        "# 8) Metrics\n",
        "# -------------------------------\n",
        "acc_u_ens = accuracy_score(y[idxU], P_ens[idxU].argmax(1))\n",
        "tr_ids, te_ids = split_train_test(len(y), test_frac=0.15, seed=SEED)\n",
        "acc_t_ens = accuracy_score(y[te_ids], P_ens[te_ids].argmax(1))\n",
        "\n",
        "print(f\"[ENSEMBLEâ€ ] Unlabeled acc: {acc_u_ens:.3f} | Internal TEST acc: {acc_t_ens:.3f}\")\n",
        "\n",
        "# single-view references after calibration\n",
        "acc_t_str = accuracy_score(y[te_ids], P_str_c[te_ids].argmax(1))\n",
        "print(f\"[Structure-only (calib)] Internal TEST acc: {acc_t_str:.3f}\")\n",
        "\n",
        "print(f\"Done in {time.time()-t0:.1f}s.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms0YWLpV9De3",
        "outputId": "5988baef-95b2-4498-d06b-33dac4490b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cat/dog subset: N=4000 (cat=1998, dog=2002)\n",
            "Built raw view features: sil=(4000, 768), tex=(4000, 40), str=(4000, 288) in 26.5s\n",
            "Graphs built in 28.1s | pos edges: 3 (per-view matrices built)\n",
            "[Per-view] Unlabeled acc â€” sil:0.530 tex:0.542 str:0.584 dif:0.585\n",
            "Per-view T: [0.6, 0.6, 0.6, 0.6]\n",
            "Seed LL: [0.0, 0.0, 0.0, -0.1534]\n",
            "Stability: [1.0, 1.0, 1.0, 1.0]\n",
            "Blend weights: [0.259, 0.259, 0.259, 0.222]\n",
            "[ENSEMBLEâ€ ] Unlabeled acc: 0.587 | Internal TEST acc: 0.608\n",
            "[Structure-only (calib)] Internal TEST acc: 0.580\n",
            "Done in 206.8s.\n"
          ]
        }
      ]
    }
  ]
}