{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ca125b1",
      "metadata": {
        "id": "3ca125b1"
      },
      "source": [
        "\n",
        "# HRM Transformer Sidecar (Colab-ready)\n",
        "\n",
        "This notebook implements a **Transformer Sidecar** that adds **latent recurrent refinement** with **ACT-style halting** to a base encoder, inspired by the *Hierarchical Reasoning Model (HRM)* line of work. It focuses on the *L-module* refinement loop and the key idea of **using the halting policy during evaluation**, which the critique shows is crucial for performance.\n",
        "\n",
        "**Highlights**  \n",
        "- One-step gradient (no BPTT through time): we detach latent state at each refinement step.  \n",
        "- ACT-like halting: train a Q-head and **use it at inference** (stop when `sigmoid(Q_halt)>0.5` or when `Mmax` reached).  \n",
        "- Pluggable sidecar: can wrap a toy transformer encoder here; you can adapt it to your own model’s hidden states.\n",
        "\n",
        "> Demo task: a tiny **sequence editing** problem (sorting digits) that benefits from iterative refinement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06409bd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06409bd2",
        "outputId": "46a7117f-ad17-40e8-8713-72933e90907f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Setup\n",
        "import math, random, time\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57aa80d0",
      "metadata": {
        "id": "57aa80d0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Tiny tokenizer for toy tasks\n",
        "# We'll use tokens 0..9 for digits plus special tokens.\n",
        "VOCAB = list(\"0123456789\")\n",
        "stoi = {ch:i for i,ch in enumerate(VOCAB)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "V = len(VOCAB)\n",
        "\n",
        "def encode(seq):\n",
        "    return torch.tensor([stoi[c] for c in seq], dtype=torch.long)\n",
        "\n",
        "def decode(ids):\n",
        "    return \"\".join(itos[int(i)] for i in ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695d638a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "695d638a",
        "outputId": "d5dbae6b-66a5-4993-df2d-181f48705083"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 1000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "#@title Toy dataset: \"sort the digits\" sequences\n",
        "# Input: a random permutation of k digits from 0..9 (without replacement)\n",
        "# Target: the digits sorted ascending as a string\n",
        "\n",
        "class SortDigitsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, n_samples=20000, length=8, seed=0):\n",
        "        random.seed(seed)\n",
        "        self.samples = []\n",
        "        for _ in range(n_samples):\n",
        "            digits = random.sample(list(\"0123456789\"), length)\n",
        "            x = \"\".join(digits)\n",
        "            y = \"\".join(sorted(digits))\n",
        "            self.samples.append((x,y))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x,y = self.samples[idx]\n",
        "        return encode(x), encode(y)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    xs = [x for x,_ in batch]\n",
        "    ys = [y for _,y in batch]\n",
        "    X = torch.stack(xs, dim=0)\n",
        "    Y = torch.stack(ys, dim=0)\n",
        "    return X, Y\n",
        "\n",
        "# Small train/val/test splits\n",
        "train_ds = SortDigitsDataset(n_samples=8000, length=8, seed=1)\n",
        "val_ds   = SortDigitsDataset(n_samples=1000, length=8, seed=2)\n",
        "test_ds  = SortDigitsDataset(n_samples=1000, length=8, seed=3)\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
        "val_dl   = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate_batch)\n",
        "test_dl  = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac14c9f",
      "metadata": {
        "id": "aac14c9f"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Base tiny Transformer encoder (token -> hidden)\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=4, num_layers=2, dim_feedforward=512, dropout=0.1, vocab_size=V):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                   dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.pos = nn.Parameter(torch.randn(1, 32, d_model) * 0.01)  # supports length<=32\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T) tokens\n",
        "        B, T = x.shape\n",
        "        h = self.emb(x) + self.pos[:, :T, :]\n",
        "        h = self.encoder(h)  # (B, T, d)\n",
        "        return h\n",
        "\n",
        "# Prediction head\n",
        "class TokenHead(nn.Module):\n",
        "    def __init__(self, d_model=128, vocab_size=V):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, h):\n",
        "        return self.proj(self.ln(h))  # (B,T,V)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242f5de9",
      "metadata": {
        "id": "242f5de9"
      },
      "source": [
        "\n",
        "## Sidecar HRM (L-module only)\n",
        "\n",
        "- **Refinement block (L-module):** a small Transformer layer that updates the latent state.\n",
        "- **Q-head:** predicts a *halt* logit each step. We train with a simple ACT-style loss.\n",
        "- **One-step gradient:** we detach the state at each step so gradients don't flow across time (no BPTT).\n",
        "- **Evaluation:** we **use halting** (`sigmoid(Q_halt)>0.5`) and stop early per sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d275a4a",
      "metadata": {
        "id": "7d275a4a"
      },
      "outputs": [],
      "source": [
        "#@title Sidecar HRM module\n",
        "class LModule(nn.Module):\n",
        "    def __init__(self, d_model=128, nhead=4, dim_feedforward=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                           dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.block = nn.TransformerEncoder(layer, num_layers=1)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, h):\n",
        "        return self.ln(self.block(h))\n",
        "\n",
        "class SidecarHRM(nn.Module):\n",
        "    def __init__(self, base_encoder, token_head, d_model=128, Mmax=6, Mmin=2):\n",
        "        super().__init__()\n",
        "        self.base = base_encoder\n",
        "        self.head = token_head\n",
        "        self.l_module = LModule(d_model=d_model)\n",
        "        # Q-head predicts a single logit \"halt\"\n",
        "        self.q_head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))\n",
        "        self.Mmax = Mmax\n",
        "        self.Mmin = Mmin\n",
        "\n",
        "    def forward_once(self, x, h=None):\n",
        "        # returns new h, logits, q_halt_logit per token position (we pool for decision)\n",
        "        if h is None:\n",
        "            h = self.base(x)  # (B,T,d)\n",
        "        h2 = h + self.l_module(h)\n",
        "        logits = self.head(h2)  # (B,T,V)\n",
        "        # Aggregate Q over sequence by mean-pooling token features\n",
        "        q_logit = self.q_head(h2).mean(dim=1)  # (B,1)\n",
        "        return h2, logits, q_logit.squeeze(1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def infer(self, x, threshold=0.5):\n",
        "        self.eval()\n",
        "        B, T = x.shape\n",
        "        done = torch.zeros(B, dtype=torch.bool, device=x.device)\n",
        "        h = None\n",
        "        steps_taken = torch.zeros(B, dtype=torch.long, device=x.device)\n",
        "        y_best = None\n",
        "\n",
        "        for m in range(1, self.Mmax+1):\n",
        "            h, logits, q = self.forward_once(x, h=h)\n",
        "            y_hat = logits.argmax(dim=-1)\n",
        "            # decide halting\n",
        "            will_halt = (torch.sigmoid(q) > threshold) & (m >= self.Mmin)\n",
        "            now_done = (~done) & will_halt\n",
        "            steps_taken[now_done] = m\n",
        "            done = done | will_halt\n",
        "            if y_best is None:\n",
        "                y_best = y_hat.clone()\n",
        "            # keep last y_hat per sequence\n",
        "            y_best[~done] = y_hat[~done]\n",
        "            if done.all():\n",
        "                break\n",
        "\n",
        "        # for any not halted, set steps taken to Mmax\n",
        "        steps_taken[~done] = self.Mmax\n",
        "        return y_best, steps_taken\n",
        "\n",
        "    def training_step(self, x, y, ce_weight=1.0, act_weight=0.1):\n",
        "        # \"one-step\" gradient: don't backprop through time\n",
        "        B = x.size(0)\n",
        "        h = None\n",
        "        total_ce = 0.0\n",
        "        total_act = 0.0\n",
        "\n",
        "        for m in range(1, self.Mmax+1):\n",
        "            if h is not None:\n",
        "                h = h.detach()  # no BPTT\n",
        "            h, logits, q = self.forward_once(x, h=h)\n",
        "            y_hat = logits.argmax(dim=-1)\n",
        "            ce = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n",
        "            # simple halt targets: halt if fully correct, else continue\n",
        "            correct = (y_hat == y).all(dim=1).float()  # (B,)\n",
        "            if m >= self.Mmin:\n",
        "                target_halt = correct\n",
        "            else:\n",
        "                target_halt = torch.zeros_like(correct)\n",
        "\n",
        "            act = F.binary_cross_entropy_with_logits(q, target_halt)\n",
        "            total_ce += ce\n",
        "            total_act += act\n",
        "\n",
        "        loss = ce_weight * (total_ce / self.Mmax) + act_weight * (total_act / self.Mmax)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3f2397",
      "metadata": {
        "id": "1a3f2397"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Train/eval utilities\n",
        "@torch.no_grad()\n",
        "def exact_match_acc(model, dl):\n",
        "    model.eval()\n",
        "    n_ok = 0\n",
        "    n_all = 0\n",
        "    total_steps = 0\n",
        "    for X, Y in dl:\n",
        "        X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
        "        Y_hat, steps = model.infer(X)\n",
        "        n_ok += (Y_hat == Y).all(dim=1).sum().item()\n",
        "        n_all += X.size(0)\n",
        "        total_steps += steps.sum().item()\n",
        "    return n_ok / n_all, total_steps / n_all\n",
        "\n",
        "def train(model, train_dl, val_dl, epochs=8, lr=3e-4):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_val = 0.0\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for X, Y in train_dl:\n",
        "            X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
        "            loss = model.training_step(X, Y, ce_weight=1.0, act_weight=0.1)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            running += loss.item()\n",
        "        val_acc, val_steps = exact_match_acc(model, val_dl)\n",
        "        print(f\"[ep {ep:02d}] train_loss={running/len(train_dl):.4f}  val_acc={val_acc:.3f}  avg_steps={val_steps:.2f}\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa47e642",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa47e642",
        "outputId": "0a4f7305-e36b-4016-83b1-2d7d291c80c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ep 01] train_loss=2.0921  val_acc=0.029  avg_steps=6.00\n",
            "[ep 02] train_loss=0.5695  val_acc=0.991  avg_steps=3.24\n",
            "[ep 03] train_loss=0.1439  val_acc=1.000  avg_steps=2.04\n",
            "[ep 04] train_loss=0.0701  val_acc=1.000  avg_steps=2.00\n",
            "[ep 05] train_loss=0.0427  val_acc=1.000  avg_steps=2.00\n",
            "[ep 06] train_loss=0.0272  val_acc=1.000  avg_steps=2.00\n",
            "[ep 07] train_loss=0.0190  val_acc=1.000  avg_steps=2.00\n",
            "[ep 08] train_loss=0.0115  val_acc=1.000  avg_steps=2.00\n",
            "TEST  acc=1.000  avg_steps=2.00\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Initialize and train\n",
        "base = TinyTransformer(d_model=128, nhead=4, num_layers=2, dim_feedforward=256).to(DEVICE)\n",
        "head = TokenHead(d_model=128).to(DEVICE)\n",
        "model = SidecarHRM(base, head, d_model=128, Mmax=6, Mmin=2).to(DEVICE)\n",
        "\n",
        "model = train(model, train_dl, val_dl, epochs=8, lr=3e-4)\n",
        "test_acc, test_steps = exact_match_acc(model, test_dl)\n",
        "print(f\"TEST  acc={test_acc:.3f}  avg_steps={test_steps:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46a383f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46a383f",
        "outputId": "eac0c48f-62ed-47ff-bff9-0acf88a15228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in : 38274906 -> out: 02346789  (gold: 02346789)  steps=2\n",
            "in : 07481532 -> out: 01234578  (gold: 01234578)  steps=2\n",
            "in : 87651942 -> out: 12456789  (gold: 12456789)  steps=2\n",
            "in : 60174825 -> out: 01245678  (gold: 01245678)  steps=2\n",
            "in : 47653890 -> out: 03456789  (gold: 03456789)  steps=2\n",
            "in : 51083629 -> out: 01235689  (gold: 01235689)  steps=2\n",
            "in : 64983725 -> out: 23456789  (gold: 23456789)  steps=2\n",
            "in : 96325087 -> out: 02356789  (gold: 02356789)  steps=2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title Quick qualitative check\n",
        "@torch.no_grad()\n",
        "def show_examples(model, k=5):\n",
        "    model.eval()\n",
        "    for i, (X,Y) in enumerate(test_dl):\n",
        "        X,Y = X.to(DEVICE), Y.to(DEVICE)\n",
        "        Y_hat, steps = model.infer(X)\n",
        "        for j in range(min(k, X.size(0))):\n",
        "            x = decode(X[j].cpu())\n",
        "            y = decode(Y[j].cpu())\n",
        "            yh = decode(Y_hat[j].cpu())\n",
        "            st = int(steps[j].cpu())\n",
        "            print(f\"in : {x} -> out: {yh}  (gold: {y})  steps={st}\")\n",
        "        break\n",
        "\n",
        "show_examples(model, k=8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f598bb2",
      "metadata": {
        "id": "3f598bb2"
      },
      "source": [
        "\n",
        "## Plugging into your own model (sketch)\n",
        "\n",
        "If your base model exposes a hidden state `h` (shape `(B,T,d)`), you can:\n",
        "1. Replace `TinyTransformer` with your encoder.\n",
        "2. Keep `TokenHead(d_model=...)` or adapt to your head.\n",
        "3. Feed your tokenized inputs `x` into `SidecarHRM`. The sidecar will refine `h` several times, predict, and **halt early** at inference if confident.\n",
        "\n",
        "Key switches to try:\n",
        "- `Mmax` (max refinement steps)\n",
        "- `Mmin` (minimum steps before halting allowed)\n",
        "- ACT weight in `training_step(..., act_weight=0.1)`\n",
        "- Use a stronger L-module (stack 2–3 layers) or a deeper base encoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555b3b7e",
      "metadata": {
        "id": "555b3b7e"
      },
      "source": [
        "\n",
        "### Why use halting at inference?\n",
        "\n",
        "The critique observes that **ignoring** the halting policy at evaluation (always running full steps) can **hurt accuracy**; continuing to edit after a correct solution can introduce errors. Enabling ACT halting at inference often **improves** both accuracy and efficiency by stopping when the solution is ready.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}