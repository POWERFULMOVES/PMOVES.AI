#!/usr/bin/env python3

"""
PMOVES • Consciousness Harvest Builder

Transforms the scaffold created by consciousness_downloader.{sh,ps1}
into PMOVES-ready artifacts:
  - processed-for-rag/embeddings-ready/consciousness-chunks.jsonl
  - processed-for-rag/supabase-import/consciousness-schema.sql
  - processed-for-rag/supabase-import/consciousness-seed.jsonl
  - geometry/geometry_payload.json (sample CGP packet)

This script is intentionally lightweight and avoids external dependencies.
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
import textwrap
import uuid
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Iterable, List, Optional


HARVEST_SUFFIX = "pmoves/data/consciousness/Constellation-Harvest-Regularization"


@dataclass
class Chunk:
    chunk_id: str
    title: str
    url: Optional[str]
    category: str
    content: str


def strip_html(html: str) -> str:
    """Very small HTML → text helper; removes scripts/styles and tags."""
    cleaned = re.sub(r"(?is)<(script|style).*?>.*?(</\1>)", "", html)
    cleaned = re.sub(r"(?is)<(head|header|footer).*?>.*?(</\1>)", "", cleaned)
    cleaned = re.sub(r"(?is)<br\s*/?>", "\n", cleaned)
    cleaned = re.sub(r"(?is)</p>", "\n\n", cleaned)
    cleaned = re.sub(r"(?is)<[^>]+>", " ", cleaned)
    cleaned = re.sub(r"\s+", " ", cleaned)
    cleaned = re.sub(r"\n\s+", "\n", cleaned)
    return cleaned.strip()


def collect_chunks(base: Path) -> List[Chunk]:
    chunks: List[Chunk] = []

    # Research papers
    research_dir = base / "research-papers"
    if research_dir.exists():
        for html_path in sorted(research_dir.glob("*.html")):
            try:
                text = strip_html(html_path.read_text(encoding="utf-8", errors="ignore"))
            except Exception as exc:  # pragma: no cover
                print(f"[warn] Failed to process {html_path}: {exc}", file=sys.stderr)
                continue
            if not text:
                continue
            title = html_path.stem.replace("_", " ").replace("-", " ").title()
            chunk_id = f"consciousness-paper-{uuid.uuid4().hex[:12]}"
            chunks.append(
                Chunk(
                    chunk_id=chunk_id,
                    title=title,
                    url=None,
                    category="research-paper",
                    content=text,
                )
            )

    # Discovered links (from Selenium helper)
    links_path = base / "data-exports" / "discovered-links.json"
    if links_path.exists():
        try:
            links = json.loads(links_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            links = []
        for entry in links:
            url = entry.get("Url") or entry.get("url")
            label = entry.get("Text") or entry.get("text") or url
            if not label:
                continue
            chunk_id = f"consciousness-link-{uuid.uuid4().hex[:12]}"
            content = f"Discovered theory/category link: {label} ({url})"
            chunks.append(
                Chunk(
                    chunk_id=chunk_id,
                    title=label.strip(),
                    url=url,
                    category="link",
                    content=content,
                )
            )

    # Fallback: create placeholder chunk if nothing harvested
    if not chunks:
        chunks.append(
            Chunk(
                chunk_id=f"consciousness-placeholder-{uuid.uuid4().hex[:12]}",
                title="Landscape of Consciousness (placeholder)",
                url=str(research_dir),
                category="placeholder",
                content="No harvestable content was found. Ensure the downloader scripts run on a host with access to the source site.",
            )
        )
    return chunks


def write_jsonl(chunks: Iterable[Chunk], dest: Path) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    with dest.open("w", encoding="utf-8") as fh:
        for chunk in chunks:
            fh.write(
                json.dumps(
                    {
                        "id": chunk.chunk_id,
                        "title": chunk.title,
                        "url": chunk.url,
                        "category": chunk.category,
                        "content": chunk.content,
                        "namespace": "pmoves.consciousness",
                        "created_at": datetime.utcnow().isoformat() + "Z",
                    },
                    ensure_ascii=False,
                )
            )
            fh.write("\n")


def write_schema(dest: Path) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    schema_sql = textwrap.dedent(
        """
        -- Generated by consciousness_build.py
        create table if not exists public.consciousness_theories (
            id text primary key,
            title text not null,
            url text,
            category text not null default 'research-paper',
            content text not null,
            namespace text not null default 'pmoves.consciousness',
            created_at timestamp with time zone not null default now()
        );

        create index if not exists consciousness_theories_category_idx on public.consciousness_theories(category);
        create index if not exists consciousness_theories_namespace_idx on public.consciousness_theories(namespace);
        """
    ).strip()
    dest.write_text(schema_sql + "\n", encoding="utf-8")


def write_seed_sql(chunks: Iterable[Chunk], dest: Path) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    statements = []
    for chunk in chunks:
        content_sql = chunk.content.replace("'", "''")
        title_sql = chunk.title.replace("'", "''")
        category_sql = chunk.category.replace("'", "''")
        url_value = "null"
        if chunk.url:
            url_value = "'" + chunk.url.replace("'", "''") + "'"

        statements.append(
            "insert into public.consciousness_theories (id, title, url, category, content, namespace) "
            f"values ('{chunk.chunk_id}', '{title_sql}', {url_value}, "
            f"'{category_sql}', '{content_sql}', 'pmoves.consciousness') "
            "on conflict (id) do update set title=excluded.title, url=excluded.url, category=excluded.category, "
            "content=excluded.content, namespace=excluded.namespace;"
        )
    dest.write_text("\n".join(statements) + "\n", encoding="utf-8")


def write_geometry_sample(chunks: List[Chunk], dest: Path) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    sample = chunks[0]
    geometry = {
        "type": "geometry.cgp.v1",
        "data": {
            "id": f"cgp-{sample.chunk_id}",
            "namespace": "pmoves.consciousness",
            "label": sample.title[:96],
            "meta": {
                "category": sample.category,
                "source_url": sample.url,
                "generator": "pmoves.consciousness.harvest",
            },
            "summary": sample.content[:512],
            "anchors": [
                {
                    "label": "primary",
                    "vector": [0.0, 1.0, 0.0],
                }
            ],
        },
    }
    dest.write_text(json.dumps(geometry, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")


def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--root",
        default=HARVEST_SUFFIX,
        help="Harvest directory (default: %(default)s relative to repo root)",
    )
    args = parser.parse_args(argv)

    repo_root = Path(__file__).resolve().parents[2]
    base = (repo_root / args.root).resolve()
    if not base.exists():
        print(f"[error] Harvest directory not found: {base}", file=sys.stderr)
        return 1

    chunks = collect_chunks(base)
    chunks_sorted = sorted(chunks, key=lambda c: c.chunk_id)

    write_jsonl(
        chunks_sorted,
        base / "processed-for-rag" / "embeddings-ready" / "consciousness-chunks.jsonl",
    )
    write_seed_sql(
        chunks_sorted,
        base / "processed-for-rag" / "supabase-import" / "consciousness-seed.sql",
    )
    write_schema(base / "processed-for-rag" / "supabase-import" / "consciousness-schema.sql")
    write_geometry_sample(
        chunks_sorted,
        base / "processed-for-rag" / "supabase-import" / "consciousness-geometry-sample.json",
    )

    print(f"[ok] Generated artifacts from {len(chunks_sorted)} chunks at {base}")
    return 0


if __name__ == "__main__":
    sys.exit(main())
