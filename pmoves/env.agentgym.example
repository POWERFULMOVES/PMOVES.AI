# AgentGym-RL Integration Environment Variables
# Add these to your env.shared or .env.local file

# ============================================================================
# AgentGym-RL Core Configuration
# ============================================================================

# Enable/disable AgentGym-RL integration
AGENTGYM_ENABLE=true

# Coordinator service URL (internal Docker network)
AGENTGYM_COORDINATOR_URL=http://agentgym-rl-coordinator:8114

# Base model for agent training (HuggingFace model ID or local path)
AGENTGYM_BASE_MODEL=Qwen2.5-7B-Instruct

# Model storage path (Docker volume mount)
AGENTGYM_MODEL_PATH=/models

# ============================================================================
# Training Defaults
# ============================================================================

# Default RL algorithm: ppo|grpo|rloo|reinforce++
AGENTGYM_DEFAULT_ALGORITHM=ppo

# Default interaction horizon (turns per episode)
AGENTGYM_DEFAULT_HORIZON=10

# Default number of training epochs
AGENTGYM_DEFAULT_EPOCHS=25

# Default batch size
AGENTGYM_DEFAULT_BATCH_SIZE=32

# Default learning rate
AGENTGYM_DEFAULT_LR=1e-6

# Default KL divergence coefficient (for PPO/GRPO)
AGENTGYM_DEFAULT_KL_COEF=0.001

# ============================================================================
# Geometry-Aware Reward Weights
# ============================================================================

# Weight for task success (did agent answer correctly?)
# Range: 0.0 to 1.0
AGENTGYM_TASK_SUCCESS_WEIGHT=0.4

# Weight for retrieval quality (relevance of retrieved info)
# Range: 0.0 to 1.0
AGENTGYM_RETRIEVAL_QUALITY_WEIGHT=0.3

# Weight for CGP fitness alignment (geometry coherence)
# Range: 0.0 to 1.0
AGENTGYM_CGP_FITNESS_WEIGHT=0.2

# Weight for efficiency (penalty for extra steps)
# Range: 0.0 to 1.0
AGENTGYM_EFFICIENCY_WEIGHT=0.1

# Note: Weights should sum to ~1.0 for interpretability

# ============================================================================
# Training Triggers (EvoSwarm Integration)
# ============================================================================

# Trigger training when fitness plateaus
AGENTGYM_TRIGGER_ON_PLATEAU=true

# Number of generations to check for plateau
AGENTGYM_PLATEAU_WINDOW=5

# Trigger training when new constellation detected
AGENTGYM_TRIGGER_ON_NEW_CONSTELLATION=true

# Periodic training interval (in EvoSwarm epochs)
AGENTGYM_PERIODIC_TRAINING_INTERVAL=100

# ============================================================================
# ScalingInter-RL Progressive Horizon Scaling
# ============================================================================

# Horizon schedule (comma-separated list)
# Example: 5,10,15 means start with 5 turns, then 10, then 15
AGENTGYM_HORIZON_SCHEDULE=5,10,15

# Epoch thresholds for horizon changes (comma-separated)
# Example: 0,10,20 means use first horizon at epoch 0, second at 10, third at 20
AGENTGYM_HORIZON_EPOCH_THRESHOLDS=0,10,20

# ============================================================================
# Environment Configuration
# ============================================================================

# PMOVES-HiRAG environment URL
AGENTGYM_ENV_URL=http://agentgym-env-pmoves:36000

# Maximum turns per episode
AGENTGYM_ENV_MAX_TURNS=15

# Episode timeout (seconds)
AGENTGYM_ENV_TIMEOUT=600

# Namespace for constellation tasks
AGENTGYM_ENV_NAMESPACE=pmoves.consciousness

# Task generator mode: constellation|random|curriculum
AGENTGYM_TASK_GENERATOR_MODE=constellation

# Task difficulty distribution (format: difficulty:weight,...)
AGENTGYM_TASK_DIFFICULTY_DISTRIBUTION=medium:0.5,easy:0.3,hard:0.2

# ============================================================================
# Monitoring & Observability
# ============================================================================

# Weights & Biases integration
AGENTGYM_WANDB_PROJECT=pmoves-agentgym-rl
AGENTGYM_WANDB_ENTITY=pmoves-ai
AGENTGYM_WANDB_API_KEY=  # Add your W&B API key here

# Enable trajectory logging to Supabase
AGENTGYM_LOG_TRAJECTORIES=true

# Checkpoint save frequency (epochs)
AGENTGYM_SAVE_FREQ=5

# Log level: DEBUG|INFO|WARNING|ERROR
AGENTGYM_LOG_LEVEL=INFO

# ============================================================================
# GPU Configuration
# ============================================================================

# GPU memory utilization (0.0 to 1.0)
AGENTGYM_GPU_MEMORY_UTILIZATION=0.7

# Tensor parallel size (number of GPUs for model parallelism)
AGENTGYM_TENSOR_PARALLEL_SIZE=1

# Enable CUDA
USE_CUDA=true

# Visible GPU devices (comma-separated IDs, or "all")
NVIDIA_VISIBLE_DEVICES=all

# ============================================================================
# Advanced Training Options
# ============================================================================

# PPO-specific parameters
AGENTGYM_PPO_MINI_BATCH_SIZE=4
AGENTGYM_PPO_MICRO_BATCH_SIZE_PER_GPU=1
AGENTGYM_PPO_INNER_EPOCHS=2
AGENTGYM_PPO_CLIP_RANGE=0.2

# GRPO-specific parameters
AGENTGYM_GRPO_GROUP_SIZE=8
AGENTGYM_GRPO_BETA=0.1

# RLOO-specific parameters
AGENTGYM_RLOO_BASELINE_TYPE=mean  # mean|ema

# REINFORCE++ specific parameters
AGENTGYM_REINFORCE_USE_BASELINE=true
AGENTGYM_REINFORCE_GAE_LAMBDA=0.95

# ============================================================================
# Curriculum Learning (Optional)
# ============================================================================

# Enable curriculum learning
AGENTGYM_CURRICULUM_ENABLE=false

# Start difficulty level: easy|medium|hard
AGENTGYM_CURRICULUM_START_DIFFICULTY=easy

# Success rate threshold to increase difficulty (0.0 to 1.0)
AGENTGYM_CURRICULUM_THRESHOLD=0.7

# Number of episodes to average for threshold check
AGENTGYM_CURRICULUM_WINDOW=100

# ============================================================================
# Model Versioning & Deployment
# ============================================================================

# Enable automatic model registration in TensorZero
AGENTGYM_REGISTER_IN_TENSORZERO=true

# Model naming convention: {base_model}-{run_id}-epoch-{epoch}
AGENTGYM_MODEL_NAME_TEMPLATE={base_model}-{run_id}-epoch-{epoch}

# Enable A/B testing of checkpoints
AGENTGYM_ENABLE_AB_TESTING=false

# A/B test traffic split (new_model:old_model)
AGENTGYM_AB_TEST_SPLIT=0.1:0.9

# ============================================================================
# Storage Configuration
# ============================================================================

# MinIO bucket for models
AGENTGYM_MINIO_MODEL_BUCKET=agentgym-models

# MinIO bucket for trajectories
AGENTGYM_MINIO_TRAJECTORY_BUCKET=agentgym-trajectories

# MinIO bucket for datasets
AGENTGYM_MINIO_DATASET_BUCKET=agentgym-datasets

# Trajectory retention period (days, 0 = keep forever)
AGENTGYM_TRAJECTORY_RETENTION_DAYS=90

# ============================================================================
# Multi-Environment Training (Future)
# ============================================================================

# Enable multi-environment training
AGENTGYM_MULTI_ENV_ENABLE=false

# Comma-separated list of environments
AGENTGYM_MULTI_ENV_LIST=pmoves-hirag,webarena,textcraft

# Environment sampling strategy: uniform|weighted|curriculum
AGENTGYM_MULTI_ENV_SAMPLING=uniform

# ============================================================================
# Population-Based Training (Future)
# ============================================================================

# Enable PBT
AGENTGYM_PBT_ENABLE=false

# Population size
AGENTGYM_PBT_POPULATION_SIZE=8

# Exploit strategy: truncate|tournament
AGENTGYM_PBT_EXPLOIT_STRATEGY=truncate

# Explore strategy: resample|perturb
AGENTGYM_PBT_EXPLORE_STRATEGY=perturb

# PBT evaluation interval (epochs)
AGENTGYM_PBT_EVAL_INTERVAL=10

# ============================================================================
# Development & Debugging
# ============================================================================

# Enable debug mode (verbose logging, small batches)
AGENTGYM_DEBUG_MODE=false

# Dry run (don't actually train, just log)
AGENTGYM_DRY_RUN=false

# Profile training (save profiling data)
AGENTGYM_PROFILE=false

# Seed for reproducibility
AGENTGYM_SEED=42

# ============================================================================
# Example Configurations
# ============================================================================

# QUICK TEST (fast iteration for development):
# AGENTGYM_DEFAULT_EPOCHS=2
# AGENTGYM_DEFAULT_BATCH_SIZE=8
# AGENTGYM_DEFAULT_HORIZON=5
# AGENTGYM_ENV_MAX_TURNS=5

# PRODUCTION (full training):
# AGENTGYM_DEFAULT_EPOCHS=50
# AGENTGYM_DEFAULT_BATCH_SIZE=64
# AGENTGYM_DEFAULT_HORIZON=15
# AGENTGYM_ENV_MAX_TURNS=20

# RESEARCH (exploration-focused):
# AGENTGYM_DEFAULT_ALGORITHM=grpo
# AGENTGYM_DEFAULT_EPOCHS=100
# AGENTGYM_CGP_FITNESS_WEIGHT=0.4
# AGENTGYM_TRIGGER_ON_PLATEAU=true
